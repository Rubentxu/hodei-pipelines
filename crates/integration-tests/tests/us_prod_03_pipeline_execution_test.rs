#![cfg(feature = "container_tests")]
//! US-PROD-03: Pipeline Execution Integration Tests with Singleton Container
//!
//! This test suite validates real pipeline execution using the ConcreteOrchestrator
//! with actual step execution, dependency management, and state tracking.
//!
//! ğŸš€ SINGLETON: Uses single shared PostgreSQL container
//! ğŸ’¾ MEMORY: Reduced from ~4GB to ~37MB with singleton container

use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;
use tracing::info;

use hodei_pipelines_adapters::{
    InMemoryBus, PostgreSqlJobRepository, PostgreSqlPipelineExecutionRepository,
    PostgreSqlPipelineRepository,
};
use hodei_pipelines_core::{
    DomainError, Result,
    job::ResourceQuota,
    pipeline::{PipelineId, PipelineStatus},
};
use hodei_pipelines_modules::{
    ConcreteOrchestrator, PipelineExecutionConfig, PipelineExecutionService, PipelineService,
    pipeline_crud::{CreatePipelineRequest, CreatePipelineStepRequest, ExecutePipelineRequest},
};
use sqlx::pool::PoolOptions;

mod helpers;
use helpers::get_shared_postgres;

#[tokio::test]
async fn test_pipeline_execution_orchestrator() {
    info!("ğŸ§ª Starting Pipeline Execution Orchestrator integration tests with singleton container");

    // âœ… USE SINGLETON CONTAINER - shared PostgreSQL for all tests
    let shared_pg = get_shared_postgres().await;

    info!("âœ… Acquired shared PostgreSQL (port {})", shared_pg.port());
    info!("   ğŸ’¾ Single container shared across all tests");
    info!("   ğŸš€ Maximum performance with minimum memory");

    // Get connection pool with retry logic
    let pool = retry_connection(&shared_pg.database_url(), 5)
        .await
        .expect("Failed to connect to shared PostgreSQL");

    info!("âœ… Connected to shared PostgreSQL successfully");

    // Initialize repositories
    let pipeline_repo = PostgreSqlPipelineRepository::new(pool.clone());
    pipeline_repo
        .init_schema()
        .await
        .expect("Failed to init pipeline schema");
    info!("âœ… Pipeline schema initialized");

    let job_repo = PostgreSqlJobRepository::new(pool.clone());
    job_repo
        .init_schema()
        .await
        .expect("Failed to init job schema");
    info!("âœ… Job schema initialized");

    let execution_repo = PostgreSqlPipelineExecutionRepository::new(pool.clone());
    execution_repo
        .init_schema()
        .await
        .expect("Failed to init execution schema");
    info!("âœ… Execution schema initialized");

    // Initialize orchestrator with real implementations
    let event_bus = Arc::new(InMemoryBus::new(1_000_000_000));
    let config = PipelineExecutionConfig {
        max_concurrent_steps: 10,
        max_retry_attempts: 3,
        step_timeout_secs: 3600,
        cleanup_interval_secs: 300,
    };

    let orchestrator = ConcreteOrchestrator::new(
        Arc::new(execution_repo),
        Arc::new(job_repo),
        Arc::new(pipeline_repo),
        event_bus,
        config,
    )
    .expect("Failed to create orchestrator");

    info!("âœ… ConcreteOrchestrator initialized successfully");

    // Test 1: Create a pipeline with multiple steps
    info!("\nğŸ“ Test 1: Create pipeline with multiple steps");
    let create_request = CreatePipelineRequest {
        name: "test-execution-pipeline".to_string(),
        description: Some("Pipeline for execution testing".to_string()),
        steps: vec![
            CreatePipelineStepRequest {
                name: "step-1".to_string(),
                image: "rust:1.75".to_string(),
                command: vec!["echo".to_string(), "Step 1 executing".to_string()],
                resources: Some(ResourceQuota {
                    cpu_m: 1000,
                    memory_mb: 512,
                    gpu: None,
                }),
                timeout_ms: Some(60000),
                retries: Some(0),
                env: Some(HashMap::new()),
                secret_refs: Some(Vec::new()),
                depends_on: None,
            },
            CreatePipelineStepRequest {
                name: "step-2".to_string(),
                image: "rust:1.75".to_string(),
                command: vec!["echo".to_string(), "Step 2 executing".to_string()],
                resources: Some(ResourceQuota {
                    cpu_m: 1000,
                    memory_mb: 512,
                    gpu: None,
                }),
                timeout_ms: Some(60000),
                retries: Some(0),
                env: Some(HashMap::new()),
                secret_refs: Some(Vec::new()),
                depends_on: None,
            },
            CreatePipelineStepRequest {
                name: "step-3".to_string(),
                image: "rust:1.75".to_string(),
                command: vec!["echo".to_string(), "Step 3 executing".to_string()],
                resources: Some(ResourceQuota {
                    cpu_m: 1000,
                    memory_mb: 512,
                    gpu: None,
                }),
                timeout_ms: Some(60000),
                retries: Some(0),
                env: Some(HashMap::new()),
                secret_refs: Some(Vec::new()),
                depends_on: None,
            },
        ],
        variables: Some(HashMap::from([
            ("ENV".to_string(), "test".to_string()),
            ("EXECUTION_ID".to_string(), "test-123".to_string()),
        ])),
    };

    let pipeline = orchestrator
        .create_pipeline(create_request)
        .await
        .expect("Operation failed");
    assert_eq!(pipeline.name, "test-execution-pipeline");
    assert_eq!(pipeline.status, PipelineStatus::PENDING);
    assert_eq!(pipeline.steps.len(), 3);
    info!("âœ… Pipeline created with {} steps", pipeline.steps.len());

    // Test 2: Get pipeline by ID
    info!("\nğŸ“ Test 2: Get pipeline by ID");
    let retrieved = orchestrator
        .get_pipeline(&pipeline.id)
        .await
        .expect("Operation failed");
    assert!(retrieved.is_some(), "Pipeline should be found");
    assert_eq!(retrieved.as_ref().unwrap().id, pipeline.id);
    info!("âœ… Pipeline retrieved successfully");

    // Test 3: Start pipeline execution
    info!("\nğŸ“ Test 3: Start pipeline execution");
    let execution_request = hodei_pipelines_modules::pipeline_crud::ExecutePipelineRequest {
        pipeline_id: pipeline.id.clone(),
        variables: Some(HashMap::from([(
            "RUNTIME_VAR".to_string(),
            "value".to_string(),
        )])),
        tenant_id: None,
        correlation_id: None,
    };
    let execution = orchestrator
        .execute_pipeline(execution_request)
        .await
        .expect("Operation failed");
    assert_eq!(execution.pipeline_id, pipeline.id);
    assert_eq!(execution.steps.len(), 3);
    info!("âœ… Pipeline execution started with ID: {}", execution.id);

    // Test 4: Get execution by ID
    info!("\nğŸ“ Test 4: Get execution by ID");
    let retrieved_execution = orchestrator
        .get_execution(&execution.id)
        .await
        .expect("Operation failed");
    assert!(retrieved_execution.is_some(), "Execution should be found");
    assert_eq!(retrieved_execution.as_ref().unwrap().id, execution.id);
    info!("âœ… Execution retrieved successfully");

    // Test 5: Update pipeline
    info!("\nğŸ“ Test 5: Update pipeline");
    let update_request = hodei_pipelines_modules::pipeline_crud::UpdatePipelineRequest {
        name: Some("updated-execution-pipeline".to_string()),
        description: Some("Updated pipeline for execution testing".to_string()),
        steps: None,
        variables: Some(HashMap::from([(
            "UPDATED_VAR".to_string(),
            "updated_value".to_string(),
        )])),
    };

    let updated_pipeline = orchestrator
        .update_pipeline(&pipeline.id, update_request)
        .await
        .expect("Operation failed");
    assert_eq!(updated_pipeline.name, "updated-execution-pipeline");
    assert_eq!(
        updated_pipeline.description,
        Some("Updated pipeline for execution testing".to_string())
    );
    info!("âœ… Pipeline updated successfully");

    // Test 6: List all pipelines
    info!("\nğŸ“ Test 6: List all pipelines");
    let pipelines = orchestrator
        .list_pipelines(None)
        .await
        .expect("Operation failed");
    assert!(!pipelines.is_empty(), "Should have pipelines");
    assert!(
        pipelines.iter().any(|p| p.id == pipeline.id),
        "Created pipeline should be in list"
    );
    info!("âœ… Found {} pipelines", pipelines.len());

    // Test 7: Create pipeline with step dependencies
    info!("\nğŸ“ Test 7: Create pipeline with dependencies");
    let step1_id = pipeline.steps[0].id.to_string();
    let dependent_request = CreatePipelineRequest {
        name: "dependent-execution-pipeline".to_string(),
        description: Some("Pipeline with step dependencies".to_string()),
        steps: vec![CreatePipelineStepRequest {
            name: "dependent-step-1".to_string(),
            image: "rust:1.75".to_string(),
            command: vec!["echo".to_string(), "Dependent step".to_string()],
            resources: Some(ResourceQuota {
                cpu_m: 1000,
                memory_mb: 512,
                gpu: None,
            }),
            timeout_ms: Some(60000),
            retries: Some(0),
            env: Some(HashMap::new()),
            secret_refs: Some(Vec::new()),
            depends_on: Some(vec![step1_id]),
        }],
        variables: None,
    };

    let dependent_pipeline = orchestrator
        .create_pipeline(dependent_request)
        .await
        .expect("Operation failed");
    assert_eq!(dependent_pipeline.steps.len(), 1);
    assert_eq!(dependent_pipeline.steps[0].depends_on.len(), 1);
    info!("âœ… Pipeline with dependencies created successfully");

    // Test 8: Cancel execution
    info!("\nğŸ“ Test 8: Cancel pipeline execution");
    let cancel_result = orchestrator.cancel_execution(&execution.id).await;
    // Cancellation might fail if execution already completed, which is fine
    info!("âœ… Cancel execution attempted");

    // Test 9: Execute another pipeline to verify orchestrator functionality
    info!("\nğŸ“ Test 9: Verify orchestrator handles multiple executions");
    let execution_request2 = hodei_pipelines_modules::pipeline_crud::ExecutePipelineRequest {
        pipeline_id: pipeline.id.clone(),
        variables: Some(HashMap::from([(
            "TEST_VAR".to_string(),
            "test_value".to_string(),
        )])),
        tenant_id: None,
        correlation_id: None,
    };
    let execution2 = orchestrator
        .execute_pipeline(execution_request2)
        .await
        .expect("Operation failed");
    assert_eq!(execution2.pipeline_id, pipeline.id);
    info!("âœ… Multiple executions handled successfully");

    info!("\nğŸ‰ All Pipeline Execution Orchestrator tests passed successfully!");
    info!("âœ… US-PROD-03: Real Pipeline Execution (Orchestrator Integration) - COMPLETED");
}

#[tokio::test]
async fn test_pipeline_execution_orchestrator_edge_cases() {
    info!("ğŸ§ª Testing Pipeline Execution Orchestrator edge cases with singleton container");

    // âœ… USE SINGLETON CONTAINER
    let shared_pg = get_shared_postgres().await;
    let pool = retry_connection(&shared_pg.database_url(), 5)
        .await
        .expect("Failed to connect to shared PostgreSQL");

    let pipeline_repo = PostgreSqlPipelineRepository::new(pool.clone());
    pipeline_repo.init_schema().await.expect("Operation failed");

    let job_repo = PostgreSqlJobRepository::new(pool.clone());
    job_repo.init_schema().await.expect("Operation failed");

    let execution_repo = PostgreSqlPipelineExecutionRepository::new(pool.clone());
    execution_repo
        .init_schema()
        .await
        .expect("Operation failed");

    let event_bus = Arc::new(InMemoryBus::new(1_000_000_000));
    let config = PipelineExecutionConfig::default();

    let orchestrator = ConcreteOrchestrator::new(
        Arc::new(execution_repo),
        Arc::new(job_repo),
        Arc::new(pipeline_repo),
        event_bus,
        config,
    )
    .expect("Failed to create orchestrator");

    // Test 1: Get non-existent pipeline
    info!("ğŸ“ Test 1: Get non-existent pipeline");
    let fake_id = PipelineId::new();
    let result = orchestrator
        .get_pipeline(&fake_id)
        .await
        .expect("Operation failed");
    assert!(
        result.is_none(),
        "Should return None for non-existent pipeline"
    );
    info!("âœ… Correctly returned None for non-existent pipeline");

    // Test 2: Create pipeline with empty name (should fail)
    info!("ğŸ“ Test 2: Create pipeline with invalid data");
    let invalid_request = CreatePipelineRequest {
        name: "".to_string(),
        description: None,
        steps: vec![],
        variables: None,
    };

    let result = orchestrator.create_pipeline(invalid_request).await;
    assert!(result.is_err(), "Should fail with invalid pipeline name");
    info!("âœ… Correctly rejected invalid pipeline");

    // Test 3: Execute non-existent pipeline
    info!("ğŸ“ Test 3: Execute non-existent pipeline");
    let fake_id = PipelineId::new();
    let invalid_execution_request = ExecutePipelineRequest {
        pipeline_id: fake_id,
        variables: Some(HashMap::new()),
        tenant_id: None,
        correlation_id: None,
    };
    let result = orchestrator
        .execute_pipeline(invalid_execution_request)
        .await;
    assert!(
        result.is_err(),
        "Should fail when executing non-existent pipeline"
    );
    info!("âœ… Correctly rejected execution of non-existent pipeline");

    // Test 4: Get non-existent execution
    info!("ğŸ“ Test 4: Get non-existent execution");
    let fake_execution_id = hodei_pipelines_core::pipeline_execution::ExecutionId::new();
    let result = orchestrator
        .get_execution(&fake_execution_id)
        .await
        .expect("Operation failed");
    assert!(
        result.is_none(),
        "Should return None for non-existent execution"
    );
    info!("âœ… Correctly returned None for non-existent execution");

    info!("\nğŸ‰ All Pipeline Execution Orchestrator edge case tests passed successfully!");
}

#[tokio::test]
async fn test_pipeline_execution_concurrent_steps() {
    info!("ğŸ§ª Testing Pipeline Execution with concurrent steps using singleton container");

    // âœ… USE SINGLETON CONTAINER
    let shared_pg = get_shared_postgres().await;
    let pool = retry_connection(&shared_pg.database_url(), 5)
        .await
        .expect("Failed to connect to shared PostgreSQL");

    let pipeline_repo = PostgreSqlPipelineRepository::new(pool.clone());
    pipeline_repo.init_schema().await.expect("Operation failed");

    let job_repo = PostgreSqlJobRepository::new(pool.clone());
    job_repo.init_schema().await.expect("Operation failed");

    let execution_repo = PostgreSqlPipelineExecutionRepository::new(pool.clone());
    execution_repo
        .init_schema()
        .await
        .expect("Operation failed");

    let event_bus = Arc::new(InMemoryBus::new(1_000_000_000));
    let config = PipelineExecutionConfig {
        max_concurrent_steps: 5,
        max_retry_attempts: 3,
        step_timeout_secs: 3600,
        cleanup_interval_secs: 300,
    };

    let orchestrator = ConcreteOrchestrator::new(
        Arc::new(execution_repo),
        Arc::new(job_repo),
        Arc::new(pipeline_repo),
        event_bus,
        config,
    )
    .expect("Failed to create orchestrator");

    // Test: Create and execute pipeline with 5 parallel steps
    info!("ğŸ“ Test: Create pipeline with 5 concurrent steps");
    let request = CreatePipelineRequest {
        name: "concurrent-test-pipeline".to_string(),
        description: Some("Pipeline with concurrent steps".to_string()),
        steps: vec![
            CreatePipelineStepRequest {
                name: "step-1".to_string(),
                image: "rust:1.75".to_string(),
                command: vec![
                    "sh".to_string(),
                    "-c".to_string(),
                    "echo 'Step 1'; sleep 1".to_string(),
                ],
                resources: Some(ResourceQuota {
                    cpu_m: 100,
                    memory_mb: 128,
                    gpu: None,
                }),
                timeout_ms: Some(10000),
                retries: Some(0),
                env: None,
                secret_refs: None,
                depends_on: None,
            },
            CreatePipelineStepRequest {
                name: "step-2".to_string(),
                image: "rust:1.75".to_string(),
                command: vec![
                    "sh".to_string(),
                    "-c".to_string(),
                    "echo 'Step 2'; sleep 1".to_string(),
                ],
                resources: Some(ResourceQuota {
                    cpu_m: 100,
                    memory_mb: 128,
                    gpu: None,
                }),
                timeout_ms: Some(10000),
                retries: Some(0),
                env: None,
                secret_refs: None,
                depends_on: None,
            },
            CreatePipelineStepRequest {
                name: "step-3".to_string(),
                image: "rust:1.75".to_string(),
                command: vec![
                    "sh".to_string(),
                    "-c".to_string(),
                    "echo 'Step 3'; sleep 1".to_string(),
                ],
                resources: Some(ResourceQuota {
                    cpu_m: 100,
                    memory_mb: 128,
                    gpu: None,
                }),
                timeout_ms: Some(10000),
                retries: Some(0),
                env: None,
                secret_refs: None,
                depends_on: None,
            },
            CreatePipelineStepRequest {
                name: "step-4".to_string(),
                image: "rust:1.75".to_string(),
                command: vec![
                    "sh".to_string(),
                    "-c".to_string(),
                    "echo 'Step 4'; sleep 1".to_string(),
                ],
                resources: Some(ResourceQuota {
                    cpu_m: 100,
                    memory_mb: 128,
                    gpu: None,
                }),
                timeout_ms: Some(10000),
                retries: Some(0),
                env: None,
                secret_refs: None,
                depends_on: None,
            },
            CreatePipelineStepRequest {
                name: "step-5".to_string(),
                image: "rust:1.75".to_string(),
                command: vec![
                    "sh".to_string(),
                    "-c".to_string(),
                    "echo 'Step 5'; sleep 1".to_string(),
                ],
                resources: Some(ResourceQuota {
                    cpu_m: 100,
                    memory_mb: 128,
                    gpu: None,
                }),
                timeout_ms: Some(10000),
                retries: Some(0),
                env: None,
                secret_refs: None,
                depends_on: None,
            },
        ],
        variables: Some(HashMap::from([
            ("ENV".to_string(), "test".to_string()),
            ("TIMEOUT".to_string(), "10000".to_string()),
        ])),
    };

    let pipeline = orchestrator
        .create_pipeline(request)
        .await
        .expect("Operation failed");
    info!("âœ… Pipeline created with ID: {}", pipeline.id);

    let execution_request = hodei_pipelines_modules::pipeline_crud::ExecutePipelineRequest {
        pipeline_id: pipeline.id.clone(),
        variables: Some(HashMap::new()),
        tenant_id: Some("test-tenant".to_string()),
        correlation_id: Some("test-correlation".to_string()),
    };

    let execution = orchestrator
        .execute_pipeline(execution_request)
        .await
        .expect("Operation failed");
    assert_eq!(execution.pipeline_id, pipeline.id);
    assert_eq!(execution.steps.len(), 5);
    info!(
        "âœ… Concurrent pipeline execution created successfully with {} steps",
        execution.steps.len()
    );

    info!("\nğŸ‰ All concurrent step execution tests passed successfully!");
}

async fn retry_connection(
    database_url: &str,
    max_retries: usize,
) -> std::result::Result<sqlx::PgPool, sqlx::Error> {
    for i in 0..max_retries {
        match PoolOptions::new()
            .max_connections(10)
            .connect(database_url)
            .await
        {
            Ok(pool) => return Ok(pool),
            Err(e) => {
                if i < max_retries - 1 {
                    tokio::time::sleep(Duration::from_millis(500)).await;
                } else {
                    return Err(e);
                }
            }
        }
    }
    unreachable!()
}
