<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <style>:root {
  --color: black;
  --bg: white;
  --head-bg: white;
  --link: #338;

  --blue: #ccf;
  --red: #fcc;
  --yellow: #ffc;
  --green: #cfc;
}

[data-theme='dark'] {
  --color: white;
  --bg: black;
  --head-bg: #333;
  --link: #aaf;

  --blue: #225;
  --red: #522;
  --yellow: #552;
  --green: #252;
}

html,
body {
  margin: 0;
  padding: 0;
  color: var(--color);
  background: var(--bg);
}

.app {
  margin: 10px;
  padding: 0;
}

.files-list {
  margin: 10px 0 0;
  width: 100%;
  border-collapse: collapse;
}
.files-list__head {
  border: 1px solid #999;
}
.files-list__head > tr > th {
  padding: 10px;
  border: 1px solid #999;
  text-align: left;
  font-weight: normal;
  background: var(--head-bg);
}
.files-list__body {
}
.files-list__file {
  cursor: pointer;
}
.files-list__file:hover {
  background: var(--blue);
}
.files-list__file > td {
  padding: 10px;
  border: 1px solid #999;
}
.files-list__file > td:first-child::before {
  content: '\01F4C4';
  margin-right: 1em;
}
.files-list__file_low {
  background: var(--red);
}
.files-list__file_medium {
  background: var(--yellow);
}
.files-list__file_high {
  background: var(--green);
}
.files-list__file_folder > td:first-child::before {
  content: '\01F4C1';
  margin-right: 1em;
}

.file-header {
  border: 1px solid #999;
  display: flex;
  justify-content: space-between;
  align-items: center;
  position: sticky;
  top: 0;
  background: var(--bg);
}

.file-header__back {
  margin: 10px;
  cursor: pointer;
  flex-shrink: 0;
  flex-grow: 0;
  text-decoration: underline;
  color: var(--link);
}

.file-header__name {
  margin: 10px;
  flex-shrink: 2;
  flex-grow: 2;
}

.file-header__stat {
  margin: 10px;
  flex-shrink: 0;
  flex-grow: 0;
}

.file-content {
  margin: 10px 0 0;
  border: 1px solid #999;
  padding: 10px;
  counter-reset: line;
  display: flex;
  flex-direction: column;
}

.code-line::before {
  content: counter(line);
  margin-right: 10px;
}
.code-line {
  margin: 0;
  padding: 0.3em;
  height: 1em;
  counter-increment: line;
}
.code-line_covered {
  background: var(--green);
}
.code-line_uncovered {
  background: var(--red);
}

#theme-toggle-label {
  margin-left: 1ch;
}
</style>
</head>
<body>
    <div id="root"></div>
    <script>
        var data = {"files":[{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","concurrency-patterns","src","lib.rs"],"content":"//! Concurrency patterns module\n\npub mod worker_pools;\n\npub use worker_pools::{BackpressureController, CircuitBreaker, DynamicWorkerPool};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","concurrency-patterns","src","worker_pools.rs"],"content":"//! Optimized Tokio worker pools for high concurrency\n\nuse parking_lot::Mutex;\nuse std::collections::HashMap;\nuse std::sync::{\n    Arc,\n    atomic::{AtomicUsize, Ordering},\n};\n\n/// Worker handle (optimized)\ntype WorkerHandle = tokio::task::JoinHandle\u003c()\u003e;\n\n/// Dynamic worker pool that auto-scales based on load\npub struct DynamicWorkerPool {\n    workers: Arc\u003cMutex\u003cHashMap\u003cString, WorkerHandle\u003e\u003e\u003e,\n    max_workers: usize,\n    min_workers: usize,\n    active_tasks: Arc\u003cAtomicUsize\u003e,\n    queue: Arc\u003ccrossbeam::queue::SegQueue\u003ctokio::task::JoinHandle\u003c()\u003e\u003e\u003e,\n}\n\nimpl DynamicWorkerPool {\n    pub fn new(min_workers: usize, max_workers: usize) -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n            min_workers,\n            max_workers,\n            active_tasks: Arc::new(AtomicUsize::new(0)),\n            queue: Arc::new(crossbeam::queue::SegQueue::new()),\n        }\n    }\n\n    /// Submit work to the pool (optimized)\n    pub async fn submit\u003cF, Fut\u003e(\u0026self, worker_id: \u0026str, work: F) -\u003e Result\u003c(), WorkerPoolError\u003e\n    where\n        F: FnOnce() -\u003e Fut + Send + 'static,\n        Fut: std::future::Future\u003cOutput = ()\u003e + Send,\n    {\n        let workers = self.workers.lock();\n        if workers.contains_key(worker_id) {\n            drop(workers);\n\n            // Increment active task counter\n            self.active_tasks.fetch_add(1, Ordering::SeqCst);\n\n            let active_tasks = Arc::clone(\u0026self.active_tasks);\n\n            // Spawn optimized task\n            let handle = tokio::spawn(async move {\n                work().await;\n                active_tasks.fetch_sub(1, Ordering::SeqCst);\n            });\n\n            self.queue.push(handle);\n            Ok(())\n        } else {\n            Err(WorkerPoolError::WorkerNotFound)\n        }\n    }\n\n    /// Get current load metric\n    pub fn current_load(\u0026self) -\u003e f64 {\n        let active = self.active_tasks.load(Ordering::SeqCst);\n        let total = self.workers.lock().len().max(1);\n        active as f64 / total as f64\n    }\n\n    /// Auto-scale workers based on load\n    pub fn auto_scale(\u0026self) {\n        let load = self.current_load();\n        let mut workers = self.workers.lock();\n\n        if load \u003e 0.8 \u0026\u0026 workers.len() \u003c self.max_workers {\n            let new_worker_id = format!(\"worker-{}\", workers.len());\n            // Spawn new worker (simplified)\n            let handle = tokio::spawn(async move {\n                tokio::time::sleep(std::time::Duration::from_secs(3600)).await;\n            });\n            workers.insert(new_worker_id, handle);\n        } else if load \u003c 0.3 \u0026\u0026 workers.len() \u003e self.min_workers {\n            // Remove excess worker (simplified)\n            if let Some(key) = workers.keys().next().cloned() {\n                if let Some(handle) = workers.remove(\u0026key) {\n                    handle.abort();\n                }\n            }\n        }\n    }\n}\n\nimpl Drop for DynamicWorkerPool {\n    fn drop(\u0026mut self) {\n        // Graceful shutdown of all workers\n        let workers = self.workers.lock();\n        for handle in workers.values() {\n            handle.abort();\n        }\n    }\n}\n\n/// Optimized Circuit breaker for resilience\npub struct CircuitBreaker {\n    state: Arc\u003cparking_lot::Mutex\u003cCircuitBreakerState\u003e\u003e,\n    threshold: u32,\n    timeout: std::time::Duration,\n    failures: Arc\u003cAtomicUsize\u003e,\n}\n\n#[derive(Debug, Clone, PartialEq)]\nenum CircuitBreakerState {\n    Closed,\n    Open,\n    HalfOpen,\n    LastFailure(std::time::Instant),\n}\n\nimpl CircuitBreaker {\n    pub fn new(threshold: u32, timeout: std::time::Duration) -\u003e Self {\n        Self {\n            state: Arc::new(parking_lot::Mutex::new(CircuitBreakerState::Closed)),\n            threshold,\n            timeout,\n            failures: Arc::new(AtomicUsize::new(0)),\n        }\n    }\n\n    pub async fn execute\u003cF, Fut, T\u003e(\u0026self, operation: F) -\u003e Result\u003cT, CircuitBreakerError\u003e\n    where\n        F: FnOnce() -\u003e Fut + Send,\n        Fut: std::future::Future\u003cOutput = Result\u003cT, Box\u003cdyn std::error::Error + Send + Sync\u003e\u003e\u003e\n            + Send,\n    {\n        let state = self.state.lock();\n\n        match *state {\n            CircuitBreakerState::Open =\u003e {\n                drop(state);\n                Err(CircuitBreakerError::Open)\n            }\n            CircuitBreakerState::HalfOpen =\u003e {\n                drop(state);\n                match operation().await {\n                    Ok(result) =\u003e {\n                        self.reset();\n                        Ok(result)\n                    }\n                    Err(_) =\u003e {\n                        self.record_failure();\n                        Err(CircuitBreakerError::ExecutionFailed)\n                    }\n                }\n            }\n            _ =\u003e {\n                drop(state);\n                match operation().await {\n                    Ok(result) =\u003e {\n                        self.reset();\n                        Ok(result)\n                    }\n                    Err(_) =\u003e {\n                        self.record_failure();\n                        Err(CircuitBreakerError::ExecutionFailed)\n                    }\n                }\n            }\n        }\n    }\n\n    fn record_failure(\u0026self) {\n        let failures = self.failures.fetch_add(1, Ordering::SeqCst) + 1;\n        if failures \u003e= self.threshold as usize {\n            let mut state = self.state.lock();\n            *state = CircuitBreakerState::Open;\n        }\n    }\n\n    fn reset(\u0026self) {\n        self.failures.store(0, Ordering::SeqCst);\n        let mut state = self.state.lock();\n        *state = CircuitBreakerState::Closed;\n    }\n\n    pub fn half_open(\u0026self) {\n        let mut state = self.state.lock();\n        *state = CircuitBreakerState::HalfOpen;\n    }\n}\n\n/// Optimized Backpressure controller\npub struct BackpressureController {\n    max_queue_size: usize,\n    current_queue_size: Arc\u003cAtomicUsize\u003e,\n    rejection_count: Arc\u003cAtomicUsize\u003e,\n}\n\nimpl BackpressureController {\n    pub fn new(max_queue_size: usize) -\u003e Self {\n        Self {\n            max_queue_size,\n            current_queue_size: Arc::new(AtomicUsize::new(0)),\n            rejection_count: Arc::new(AtomicUsize::new(0)),\n        }\n    }\n\n    pub fn can_accept(\u0026self) -\u003e bool {\n        let size = self.current_queue_size.load(Ordering::SeqCst);\n        size \u003c self.max_queue_size\n    }\n\n    pub fn record_enqueue(\u0026self) -\u003e Result\u003c(), BackpressureError\u003e {\n        let size = self.current_queue_size.fetch_add(1, Ordering::SeqCst) + 1;\n        if size \u003e self.max_queue_size {\n            self.current_queue_size.fetch_sub(1, Ordering::SeqCst);\n            self.rejection_count.fetch_add(1, Ordering::SeqCst);\n            return Err(BackpressureError::QueueFull);\n        }\n        Ok(())\n    }\n\n    pub fn record_dequeue(\u0026self) {\n        self.current_queue_size.fetch_sub(1, Ordering::SeqCst);\n    }\n\n    pub fn rejection_rate(\u0026self) -\u003e f64 {\n        let rejections = self.rejection_count.load(Ordering::SeqCst) as f64;\n        let total_attempts = rejections + self.current_queue_size.load(Ordering::SeqCst) as f64;\n        if total_attempts \u003e 0.0 {\n            rejections / total_attempts\n        } else {\n            0.0\n        }\n    }\n}\n\n/// Rate limiter with token bucket\npub struct RateLimiter {\n    capacity: usize,\n    tokens: Arc\u003cAtomicUsize\u003e,\n    refill_rate: std::time::Duration,\n    last_refill: Arc\u003cMutex\u003cstd::time::Instant\u003e\u003e,\n}\n\nimpl RateLimiter {\n    pub fn new(capacity: usize, refill_rate: std::time::Duration) -\u003e Self {\n        let tokens = Arc::new(AtomicUsize::new(capacity));\n        let last_refill = Arc::new(Mutex::new(std::time::Instant::now()));\n\n        // Start refill task\n        let tokens_ref = Arc::clone(\u0026tokens);\n        let last_refill_ref = Arc::clone(\u0026last_refill);\n        let refill_rate_clone = refill_rate;\n        tokio::spawn(async move {\n            let mut interval = tokio::time::interval(refill_rate_clone);\n            loop {\n                interval.tick().await;\n                let mut last = last_refill_ref.lock();\n                *last = std::time::Instant::now();\n                tokens_ref.store(capacity, Ordering::SeqCst);\n            }\n        });\n\n        Self {\n            capacity,\n            tokens,\n            refill_rate,\n            last_refill,\n        }\n    }\n\n    pub async fn acquire(\u0026self) -\u003e Result\u003c(), RateLimitError\u003e {\n        let current = self.tokens.fetch_sub(1, Ordering::SeqCst);\n        if current \u003e 0 {\n            Ok(())\n        } else {\n            self.tokens.fetch_add(1, Ordering::SeqCst); // Restore token\n            Err(RateLimitError::RateExceeded)\n        }\n    }\n\n    pub fn remaining_tokens(\u0026self) -\u003e usize {\n        self.tokens.load(Ordering::SeqCst)\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum WorkerPoolError {\n    #[error(\"worker not found\")]\n    WorkerNotFound,\n\n    #[error(\"pool at capacity\")]\n    AtCapacity,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum CircuitBreakerError {\n    #[error(\"circuit breaker is open\")]\n    Open,\n\n    #[error(\"execution failed\")]\n    ExecutionFailed,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum BackpressureError {\n    #[error(\"queue is full\")]\n    QueueFull,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum RateLimitError {\n    #[error(\"rate limit exceeded\")]\n    RateExceeded,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_worker_pool_load() {\n        let pool = DynamicWorkerPool::new(1, 5);\n\n        // Check load calculation with no active tasks\n        let load = pool.current_load();\n        assert_eq!(load, 0.0);\n\n        // Test auto-scaling\n        pool.auto_scale();\n    }\n\n    #[tokio::test]\n    async fn test_circuit_breaker() {\n        let breaker = CircuitBreaker::new(2, std::time::Duration::from_secs(1));\n\n        // First failure\n        assert!(\n            breaker\n                .execute(|| async { Err::\u003c(), _\u003e(\"fail\".into()) })\n                .await\n                .is_err()\n        );\n\n        // Second failure - should open\n        assert!(\n            breaker\n                .execute(|| async { Err::\u003c(), _\u003e(\"fail\".into()) })\n                .await\n                .is_err()\n        );\n\n        // Should be open now\n        assert!(matches!(\n            breaker.execute(|| async { Ok::\u003c(), _\u003e(()) }).await,\n            Err(CircuitBreakerError::Open)\n        ));\n    }\n\n    #[tokio::test]\n    async fn test_backpressure() {\n        let controller = BackpressureController::new(2);\n\n        assert!(controller.record_enqueue().is_ok());\n        assert!(controller.record_enqueue().is_ok());\n\n        // Should fail on third\n        assert!(matches!(\n            controller.record_enqueue(),\n            Err(BackpressureError::QueueFull)\n        ));\n    }\n}\n","traces":[{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":34},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","coordinator","src","lib.rs"],"content":"//! Distributed Coordinator implementation (US-003)\n//!\n//! This module provides job coordination with load balancing and failure detection.\n\nuse async_trait::async_trait;\nuse hodei_distributed_comm::EventBus;\nuse hodei_shared_types::job_definitions::{JobId, JobSpec};\nuse hodei_shared_types::{CorrelationId, DomainError, TenantId, Uuid};\nuse parking_lot::Mutex;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n/// Worker information for load balancing\n#[derive(Debug, Clone)]\npub struct WorkerInfo {\n    pub id: Uuid,\n    pub load: f64,\n    pub capabilities: Vec\u003cString\u003e,\n    pub is_healthy: bool,\n}\n\n/// Load balancer for selecting optimal workers\npub struct LoadBalancer {\n    workers: Arc\u003cMutex\u003cHashMap\u003cUuid, WorkerInfo\u003e\u003e\u003e,\n}\n\nimpl LoadBalancer {\n    pub fn new() -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    pub fn add_worker(\u0026self, worker: WorkerInfo) {\n        let mut workers = self.workers.lock();\n        workers.insert(worker.id, worker);\n    }\n\n    pub fn select_worker(\u0026self, required_capabilities: \u0026[String]) -\u003e Option\u003cUuid\u003e {\n        let workers = self.workers.lock();\n\n        let mut candidates: Vec\u003c_\u003e = workers\n            .values()\n            .filter(|w| {\n                w.is_healthy\n                    \u0026\u0026 required_capabilities\n                        .iter()\n                        .all(|cap| w.capabilities.contains(cap))\n            })\n            .collect();\n\n        if candidates.is_empty() {\n            return None;\n        }\n\n        // Select worker with minimum load\n        candidates.sort_by(|a, b| {\n            a.load\n                .partial_cmp(\u0026b.load)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n        Some(candidates[0].id)\n    }\n\n    pub fn update_load(\u0026self, worker_id: Uuid, new_load: f64) {\n        let mut workers = self.workers.lock();\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.load = new_load;\n        }\n    }\n\n    pub fn mark_unhealthy(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.is_healthy = false;\n        }\n    }\n}\n\n/// Failure detector based on heartbeat\npub struct FailureDetector {\n    workers: Arc\u003cMutex\u003cHashMap\u003cUuid, std::time::Instant\u003e\u003e\u003e,\n    timeout: std::time::Duration,\n}\n\nimpl FailureDetector {\n    pub fn new(timeout: std::time::Duration) -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n            timeout,\n        }\n    }\n\n    pub fn record_heartbeat(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        workers.insert(worker_id, std::time::Instant::now());\n    }\n\n    pub fn detect_failures(\u0026self) -\u003e Vec\u003cUuid\u003e {\n        let workers = self.workers.lock();\n        let now = std::time::Instant::now();\n\n        workers\n            .iter()\n            .filter(|(_, last_seen)| now.duration_since(**last_seen) \u003e self.timeout)\n            .map(|(id, _)| *id)\n            .collect()\n    }\n\n    pub fn remove_worker(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        workers.remove(\u0026worker_id);\n    }\n}\n\n/// Job Coordinator\npub struct JobCoordinator {\n    load_balancer: Arc\u003cLoadBalancer\u003e,\n    failure_detector: Arc\u003cFailureDetector\u003e,\n    event_bus: Option\u003cArc\u003chodei_distributed_comm::NatsEventBus\u003e\u003e,\n}\n\nimpl JobCoordinator {\n    pub fn new() -\u003e Self {\n        Self {\n            load_balancer: Arc::new(LoadBalancer::new()),\n            failure_detector: Arc::new(FailureDetector::new(std::time::Duration::from_secs(30))),\n            event_bus: None,\n        }\n    }\n\n    pub fn with_event_bus(mut self, event_bus: Arc\u003chodei_distributed_comm::NatsEventBus\u003e) -\u003e Self {\n        self.event_bus = Some(event_bus);\n        self\n    }\n\n    /// Register a new worker\n    pub fn register_worker(\u0026self, worker: WorkerInfo) {\n        self.load_balancer.add_worker(worker.clone());\n        self.failure_detector.record_heartbeat(worker.id);\n    }\n\n    /// Schedule a job to an available worker\n    pub async fn schedule_job(\n        \u0026self,\n        job_id: JobId,\n        job_spec: \u0026JobSpec,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003cUuid, DomainError\u003e {\n        // Extract required capabilities from job spec (simplified)\n        let required_capabilities = vec![\"default\".to_string()];\n\n        // Select optimal worker\n        let worker_id = self\n            .load_balancer\n            .select_worker(\u0026required_capabilities)\n            .ok_or_else(|| DomainError::Infrastructure(\"no suitable worker found\".to_string()))?;\n\n        // Record heartbeat\n        self.failure_detector.record_heartbeat(worker_id);\n\n        // Update worker load (simulate job assignment)\n        self.load_balancer.update_load(worker_id, 1.0);\n\n        // Publish job scheduled event if event bus is available\n        if let Some(event_bus) = \u0026self.event_bus {\n            let event = JobScheduledEvent {\n                job_id: job_id.clone(),\n                worker_id,\n                correlation_id: correlation_id.clone(),\n                tenant_id: tenant_id.clone(),\n            };\n\n            if let Err(e) = event_bus\n                .publish(\n                    \"jobs.scheduled\",\n                    \u0026event,\n                    correlation_id.clone(),\n                    tenant_id.clone(),\n                )\n                .await\n            {\n                tracing::warn!(\"Failed to publish job scheduled event: {}\", e);\n            }\n        }\n\n        tracing::info!(\"Scheduled job {} to worker {}\", job_id.as_uuid(), worker_id);\n\n        Ok(worker_id)\n    }\n\n    /// Handle worker heartbeat\n    pub fn handle_heartbeat(\u0026self, worker_id: Uuid) {\n        self.failure_detector.record_heartbeat(worker_id);\n        self.load_balancer.update_load(worker_id, 0.5); // Active but not at full capacity\n    }\n\n    /// Handle worker failure\n    pub fn handle_worker_failure(\u0026self, worker_id: Uuid) {\n        self.load_balancer.mark_unhealthy(worker_id);\n        self.failure_detector.remove_worker(worker_id);\n        tracing::warn!(\"Worker {} marked as unhealthy\", worker_id);\n    }\n\n    /// Check for failed workers\n    pub fn check_failures(\u0026self) -\u003e Vec\u003cUuid\u003e {\n        let failed_workers = self.failure_detector.detect_failures();\n        for worker_id in \u0026failed_workers {\n            self.handle_worker_failure(*worker_id);\n        }\n        failed_workers\n    }\n}\n\n/// Event for job scheduled\n#[derive(Debug, serde::Serialize, serde::Deserialize)]\npub struct JobScheduledEvent {\n    pub job_id: JobId,\n    pub worker_id: Uuid,\n    pub correlation_id: CorrelationId,\n    pub tenant_id: TenantId,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use hodei_shared_types::job_definitions::ResourceQuota;\n    use std::collections::HashMap;\n\n    #[test]\n    fn test_load_balancer_worker_selection() {\n        let balancer = LoadBalancer::new();\n\n        // Add workers with different loads\n        balancer.add_worker(WorkerInfo {\n            id: Uuid::new_v4(),\n            load: 0.5,\n            capabilities: vec![\"default\".to_string()],\n            is_healthy: true,\n        });\n\n        let worker_id = balancer.select_worker(\u0026[\"default\".to_string()]);\n        assert!(worker_id.is_some());\n    }\n\n    #[test]\n    fn test_failure_detection() {\n        let detector = FailureDetector::new(std::time::Duration::from_millis(100));\n\n        let worker_id = Uuid::new_v4();\n        detector.record_heartbeat(worker_id);\n\n        // Should not detect failure immediately\n        assert_eq!(detector.detect_failures().len(), 0);\n\n        std::thread::sleep(std::time::Duration::from_millis(150));\n\n        // Should detect failure after timeout\n        let failures = detector.detect_failures();\n        assert_eq!(failures.len(), 1);\n        assert_eq!(failures[0], worker_id);\n    }\n\n    #[test]\n    fn test_coordinator_job_scheduling() {\n        let coordinator = JobCoordinator::new();\n\n        // Register a worker\n        coordinator.register_worker(WorkerInfo {\n            id: Uuid::new_v4(),\n            load: 0.0,\n            capabilities: vec![\"default\".to_string()],\n            is_healthy: true,\n        });\n\n        let spec = JobSpec {\n            name: \"test-job\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        };\n\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test\".to_string());\n\n        // This would be async in real usage\n        // but we're just testing the coordination logic here\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","error_handling.rs"],"content":"//! Error handling for distributed communication\n\n#[derive(Debug, thiserror::Error)]\npub enum DistributedError {\n    #[error(\"connection error: {0}\")]\n    Connection(String),\n\n    #[error(\"publish error: {0}\")]\n    Publish(String),\n\n    #[error(\"subscribe error: {0}\")]\n    Subscribe(String),\n\n    #[error(\"timeout: {0}\")]\n    Timeout(String),\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","lib.rs"],"content":"//! Distributed communication module\n\npub mod error_handling;\npub mod nats_adapter;\npub mod tests;\n\npub use error_handling::DistributedError;\npub use nats_adapter::{EventBus, NatsClient, NatsError, NatsEventBus, TopicManager};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","nats_adapter.rs"],"content":"//! NATS JetStream adapter for distributed communication (Mock Implementation)\n\nuse async_trait::async_trait;\nuse hodei_shared_types::{CorrelationId, TenantId};\nuse serde::{Deserialize, Serialize};\nuse std::sync::Arc;\n\n/// Mock NATS connection wrapper\npub struct NatsClient {\n    _phantom: std::marker::PhantomData\u003c()\u003e,\n}\n\nimpl NatsClient {\n    pub fn new() -\u003e Self {\n        Self {\n            _phantom: std::marker::PhantomData,\n        }\n    }\n\n    /// Publish a message (mock)\n    pub async fn publish(\u0026self, _subject: \u0026str, _data: \u0026[u8]) -\u003e Result\u003c(), NatsError\u003e {\n        tracing::debug!(\"Mock publish to subject\");\n        Ok(())\n    }\n\n    /// Subscribe to a subject (mock)\n    pub fn subscribe(\n        \u0026self,\n        _subject: \u0026str,\n    ) -\u003e Result\u003cstd::sync::mpsc::Receiver\u003cVec\u003cu8\u003e\u003e, NatsError\u003e {\n        let (_tx, rx) = std::sync::mpsc::channel();\n        tracing::debug!(\"Mock subscribe to subject\");\n        // Return a mock receiver\n        Ok(rx)\n    }\n}\n\n/// NATS error types\n#[derive(Debug, thiserror::Error)]\npub enum NatsError {\n    #[error(\"publish error: {0}\")]\n    PublishError(String),\n\n    #[error(\"subscribe error: {0}\")]\n    SubscribeError(String),\n\n    #[error(\"connection error: {0}\")]\n    ConnectionError(String),\n}\n\n/// Message types for distributed communication\n#[derive(Debug, Serialize, Deserialize)]\npub struct Message {\n    pub subject: String,\n    pub payload: Vec\u003cu8\u003e,\n    pub correlation_id: CorrelationId,\n    pub tenant_id: TenantId,\n}\n\n/// NATS topics manager\npub struct TopicManager {\n    nats_client: Arc\u003cNatsClient\u003e,\n}\n\nimpl TopicManager {\n    pub fn new(nats_client: Arc\u003cNatsClient\u003e) -\u003e Self {\n        Self { nats_client }\n    }\n\n    /// Create a JetStream stream (mock)\n    pub async fn create_stream(\u0026self, name: \u0026str, subjects: \u0026[\u0026str]) -\u003e Result\u003c(), NatsError\u003e {\n        tracing::info!(\n            \"Mock: Creating stream {} with subjects {:?}\",\n            name,\n            subjects\n        );\n        Ok(())\n    }\n\n    /// Publish event to topic\n    pub async fn publish_event\u003cT: Serialize\u003e(\n        \u0026self,\n        topic: \u0026str,\n        event: \u0026T,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003c(), NatsError\u003e {\n        let payload =\n            bincode::serialize(event).map_err(|e| NatsError::PublishError(e.to_string()))?;\n\n        let message = Message {\n            subject: topic.to_string(),\n            payload,\n            correlation_id,\n            tenant_id,\n        };\n\n        let message_bytes =\n            bincode::serialize(\u0026message).map_err(|e| NatsError::PublishError(e.to_string()))?;\n\n        self.nats_client.publish(topic, \u0026message_bytes).await?;\n        Ok(())\n    }\n}\n\n/// EventBus trait for publishing events\n#[async_trait]\npub trait EventBus: Send + Sync {\n    async fn publish\u003cT: Serialize + Sync\u003e(\n        \u0026self,\n        topic: \u0026str,\n        event: \u0026T,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003c(), NatsError\u003e;\n}\n\n/// NATS EventBus implementation\npub struct NatsEventBus {\n    topic_manager: Arc\u003cTopicManager\u003e,\n}\n\nimpl NatsEventBus {\n    pub fn new(topic_manager: Arc\u003cTopicManager\u003e) -\u003e Self {\n        Self { topic_manager }\n    }\n}\n\n#[async_trait]\nimpl EventBus for NatsEventBus {\n    async fn publish\u003cT: Serialize + Sync\u003e(\n        \u0026self,\n        topic: \u0026str,\n        event: \u0026T,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003c(), NatsError\u003e {\n        self.topic_manager\n            .publish_event(topic, event, correlation_id, tenant_id)\n            .await\n    }\n}\n","traces":[{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":10},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","tests.rs"],"content":"#[cfg(test)]\nmod tests {\n    use crate::{EventBus, NatsClient, NatsEventBus, TopicManager};\n    use hodei_shared_types::{CorrelationId, TenantId};\n    use std::sync::Arc;\n\n    #[tokio::test]\n    async fn test_nats_client_creation() {\n        let client = NatsClient::new();\n        assert!(client.publish(\"test.subject\", b\"test data\").await.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_event_publishing() {\n        let client = Arc::new(NatsClient::new());\n        let topic_manager = Arc::new(TopicManager::new(client));\n        let event_bus = Arc::new(NatsEventBus::new(topic_manager));\n\n        let test_event = \"test event\";\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n\n        let result = event_bus\n            .publish(\"test.topic\", \u0026test_event, correlation_id, tenant_id)\n            .await;\n\n        assert!(result.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_stream_creation() {\n        let client = Arc::new(NatsClient::new());\n        let topic_manager = Arc::new(TopicManager::new(client));\n\n        let result = topic_manager\n            .create_stream(\"test-stream\", \u0026[\"test.subject.*\"])\n            .await;\n\n        assert!(result.is_ok());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","monitoring","src","lib.rs"],"content":"//! Performance benchmarking and monitoring (US-006)\n//!\n//! This module provides comprehensive performance monitoring including:\n//! - Metrics collection (counters, gauges, histograms)\n//! - Benchmark runner with automated metrics\n//! - Prometheus metrics exporter\n//! - SLA monitoring and alerting\n\nuse hodei_shared_types::{CorrelationId, JobId, TenantId, Uuid};\nuse parking_lot::Mutex;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\n\n/// Performance metrics collector\n#[derive(Debug, Clone)]\npub struct MetricsCollector {\n    pub metrics: HashMap\u003cString, MetricValue\u003e,\n    start_time: Instant,\n}\n\n#[derive(Debug, Clone)]\npub struct MetricValue {\n    pub name: String,\n    pub value: f64,\n    pub metric_type: MetricType,\n    pub labels: HashMap\u003cString, String\u003e,\n    pub timestamp: Instant,\n}\n\n#[derive(Debug, Clone)]\npub enum MetricType {\n    Counter,\n    Gauge,\n    Histogram,\n}\n\nimpl MetricsCollector {\n    pub fn new() -\u003e Self {\n        Self {\n            metrics: HashMap::new(),\n            start_time: Instant::now(),\n        }\n    }\n\n    /// Record a counter metric\n    pub fn increment_counter(\n        \u0026mut self,\n        name: \u0026str,\n        value: f64,\n        labels: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    ) {\n        let labels = labels.unwrap_or_default();\n        let key = format!(\"{}_{:?}\", name, labels);\n\n        if let Some(existing) = self.metrics.get_mut(\u0026key) {\n            existing.value += value;\n            existing.timestamp = Instant::now();\n        } else {\n            self.metrics.insert(\n                key,\n                MetricValue {\n                    name: name.to_string(),\n                    value,\n                    metric_type: MetricType::Counter,\n                    labels,\n                    timestamp: Instant::now(),\n                },\n            );\n        }\n    }\n\n    /// Record a gauge metric\n    pub fn set_gauge(\u0026mut self, name: \u0026str, value: f64, labels: Option\u003cHashMap\u003cString, String\u003e\u003e) {\n        let labels = labels.unwrap_or_default();\n        let key = format!(\"{}_{:?}\", name, labels);\n\n        self.metrics.insert(\n            key,\n            MetricValue {\n                name: name.to_string(),\n                value,\n                metric_type: MetricType::Gauge,\n                labels,\n                timestamp: Instant::now(),\n            },\n        );\n    }\n\n    /// Record a histogram metric\n    pub fn record_histogram(\n        \u0026mut self,\n        name: \u0026str,\n        value: f64,\n        labels: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    ) {\n        let labels = labels.unwrap_or_default();\n        let key = format!(\"{}_{:?}\", name, labels);\n\n        self.metrics.insert(\n            key,\n            MetricValue {\n                name: name.to_string(),\n                value,\n                metric_type: MetricType::Histogram,\n                labels,\n                timestamp: Instant::now(),\n            },\n        );\n    }\n\n    /// Get all metrics\n    pub fn get_metrics(\u0026self) -\u003e Vec\u003c\u0026MetricValue\u003e {\n        self.metrics.values().collect()\n    }\n\n    /// Get uptime\n    pub fn uptime(\u0026self) -\u003e Duration {\n        self.start_time.elapsed()\n    }\n\n    /// Clear all metrics\n    pub fn clear(\u0026mut self) {\n        self.metrics.clear();\n    }\n}\n\n/// Benchmark runner\npub struct BenchmarkRunner {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n}\n\nimpl BenchmarkRunner {\n    pub fn new() -\u003e Self {\n        Self {\n            metrics: Arc::new(Mutex::new(MetricsCollector::new())),\n        }\n    }\n\n    /// Run a benchmark and record results\n    pub async fn run_benchmark\u003cF, Fut\u003e(\u0026self, name: \u0026str, operation: F) -\u003e BenchmarkResult\n    where\n        F: FnOnce() -\u003e Fut,\n        Fut: std::future::Future,\n    {\n        let start = Instant::now();\n\n        operation().await;\n\n        let duration = start.elapsed();\n        let mut metrics = self.metrics.lock();\n\n        metrics.record_histogram(\n            \u0026format!(\"{}_duration_ms\", name),\n            duration.as_millis() as f64,\n            None,\n        );\n\n        BenchmarkResult {\n            name: name.to_string(),\n            duration,\n            success: true,\n        }\n    }\n\n    /// Get reference to metrics collector\n    pub fn metrics(\u0026self) -\u003e \u0026Arc\u003cMutex\u003cMetricsCollector\u003e\u003e {\n        \u0026self.metrics\n    }\n}\n\n/// Prometheus metrics exporter\npub struct PrometheusExporter {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n}\n\nimpl PrometheusExporter {\n    pub fn new(metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e) -\u003e Self {\n        Self { metrics }\n    }\n\n    /// Export metrics in Prometheus format\n    pub fn export(\u0026self) -\u003e String {\n        let metrics = self.metrics.lock();\n        let mut output = String::new();\n\n        for metric in metrics.get_metrics() {\n            let labels: Vec\u003cString\u003e = metric\n                .labels\n                .iter()\n                .map(|(k, v)| format!(\"{}=\\\"{}\\\"\", k, v))\n                .collect();\n\n            let labels_str = if !labels.is_empty() {\n                format!(\"{{{}}}\", labels.join(\",\"))\n            } else {\n                String::new()\n            };\n\n            output.push_str(\u0026format!(\n                \"# TYPE {} {}\\n{}{} {}\\n\\n\",\n                metric.name,\n                format!(\"{:?}\", metric.metric_type).to_lowercase(),\n                metric.name,\n                labels_str,\n                metric.value\n            ));\n        }\n\n        output\n    }\n}\n\n/// SLA monitor for tracking service level objectives\npub struct SLAMonitor {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n    sla_thresholds: HashMap\u003cString, Threshold\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct Threshold {\n    pub target_value: f64,\n    pub comparison: Comparison,\n}\n\n#[derive(Debug, Clone)]\npub enum Comparison {\n    LessThan,\n    GreaterThan,\n    Equal,\n}\n\nimpl SLAMonitor {\n    pub fn new() -\u003e Self {\n        let mut sla_thresholds = HashMap::new();\n\n        // Define default SLAs\n        sla_thresholds.insert(\n            \"job_scheduling_latency_ms\".to_string(),\n            Threshold {\n                target_value: 50.0, // \u003c 50ms\n                comparison: Comparison::LessThan,\n            },\n        );\n\n        sla_thresholds.insert(\n            \"system_uptime_percent\".to_string(),\n            Threshold {\n                target_value: 99.9, // \u003e 99.9%\n                comparison: Comparison::GreaterThan,\n            },\n        );\n\n        Self {\n            metrics: Arc::new(Mutex::new(MetricsCollector::new())),\n            sla_thresholds,\n        }\n    }\n\n    /// Check if SLA is being met\n    pub fn check_sla(\u0026self, metric_name: \u0026str) -\u003e SLACheckResult {\n        let metrics = self.metrics.lock();\n\n        if let Some(threshold) = self.sla_thresholds.get(metric_name) {\n            // Find metric by name (keys may have labels suffix)\n            for metric in metrics.metrics.values() {\n                if metric.name == metric_name {\n                    let meets_sla = match threshold.comparison {\n                        Comparison::LessThan =\u003e metric.value \u003c= threshold.target_value,\n                        Comparison::GreaterThan =\u003e metric.value \u003e= threshold.target_value,\n                        Comparison::Equal =\u003e metric.value == threshold.target_value,\n                    };\n\n                    return SLACheckResult {\n                        metric_name: metric_name.to_string(),\n                        current_value: metric.value,\n                        threshold: threshold.target_value,\n                        meets_sla,\n                    };\n                }\n            }\n        }\n\n        SLACheckResult {\n            metric_name: metric_name.to_string(),\n            current_value: 0.0,\n            threshold: 0.0,\n            meets_sla: false,\n        }\n    }\n\n    /// Record job scheduling latency\n    pub fn record_job_scheduling_latency(\u0026self, latency_ms: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.record_histogram(\"job_scheduling_latency_ms\", latency_ms, None);\n    }\n\n    /// Record worker utilization\n    pub fn record_worker_utilization(\u0026self, worker_id: Uuid, utilization: f64) {\n        let mut metrics = self.metrics.lock();\n        let mut labels = HashMap::new();\n        labels.insert(\"worker_id\".to_string(), worker_id.to_string());\n        metrics.set_gauge(\"worker_utilization_percent\", utilization, Some(labels));\n    }\n\n    /// Record job throughput\n    pub fn record_job_throughput(\u0026self, jobs_per_minute: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.increment_counter(\"job_throughput_total\", jobs_per_minute, None);\n    }\n\n    /// Record system health\n    pub fn record_system_health(\u0026self, health_score: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.set_gauge(\"system_health_score\", health_score, None);\n    }\n\n    /// Get metrics collector reference\n    pub fn metrics(\u0026self) -\u003e \u0026Arc\u003cMutex\u003cMetricsCollector\u003e\u003e {\n        \u0026self.metrics\n    }\n}\n\n#[derive(Debug)]\npub struct SLACheckResult {\n    pub metric_name: String,\n    pub current_value: f64,\n    pub threshold: f64,\n    pub meets_sla: bool,\n}\n\n#[derive(Debug)]\npub struct BenchmarkResult {\n    pub name: String,\n    pub duration: Duration,\n    pub success: bool,\n}\n\n/// Performance profiler for detailed analysis\npub struct PerformanceProfiler {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n    profiling_enabled: bool,\n}\n\nimpl PerformanceProfiler {\n    pub fn new() -\u003e Self {\n        Self {\n            metrics: Arc::new(Mutex::new(MetricsCollector::new())),\n            profiling_enabled: true,\n        }\n    }\n\n    /// Profile a function call\n    pub async fn profile\u003cF, Fut, T\u003e(\u0026self, name: \u0026str, operation: F) -\u003e T\n    where\n        F: FnOnce() -\u003e Fut,\n        Fut: std::future::Future\u003cOutput = T\u003e,\n    {\n        if !self.profiling_enabled {\n            return operation().await;\n        }\n\n        let start = Instant::now();\n        let result = operation().await;\n        let duration = start.elapsed();\n\n        let mut metrics = self.metrics.lock();\n        metrics.record_histogram(\n            \u0026format!(\"{}_profiled_duration_ms\", name),\n            duration.as_millis() as f64,\n            None,\n        );\n\n        result\n    }\n\n    /// Record memory usage\n    pub fn record_memory_usage(\u0026self, bytes: usize) {\n        let mut metrics = self.metrics.lock();\n        metrics.set_gauge(\"memory_usage_bytes\", bytes as f64, None);\n    }\n\n    /// Record CPU utilization\n    pub fn record_cpu_utilization(\u0026self, percentage: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.set_gauge(\"cpu_utilization_percent\", percentage, None);\n    }\n\n    /// Enable/disable profiling\n    pub fn set_profiling_enabled(\u0026self, _enabled: bool) {\n        // This would need interior mutability in a real implementation\n        // Simplified for example purposes\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_metrics_collection() {\n        let mut collector = MetricsCollector::new();\n\n        collector.increment_counter(\"requests_total\", 1.0, None);\n        collector.set_gauge(\"memory_usage_mb\", 512.0, None);\n        collector.record_histogram(\"response_time_ms\", 45.0, None);\n\n        let metrics = collector.get_metrics();\n        assert_eq!(metrics.len(), 3);\n\n        // Small delay to ensure uptime is measurable\n        std::thread::sleep(Duration::from_millis(10));\n\n        // Check uptime is set\n        assert!(collector.uptime().as_millis() \u003e 0);\n    }\n\n    #[test]\n    fn test_sla_monitoring() {\n        let monitor = SLAMonitor::new();\n\n        // Record good latency\n        monitor.record_job_scheduling_latency(30.0);\n\n        let result = monitor.check_sla(\"job_scheduling_latency_ms\");\n        assert!(result.meets_sla);\n        assert_eq!(result.current_value, 30.0);\n\n        // Record bad latency\n        monitor.record_job_scheduling_latency(100.0);\n\n        let result = monitor.check_sla(\"job_scheduling_latency_ms\");\n        assert!(!result.meets_sla);\n    }\n\n    #[test]\n    fn test_prometheus_export() {\n        let metrics = Arc::new(Mutex::new(MetricsCollector::new()));\n        {\n            let mut collector = metrics.lock();\n            collector.increment_counter(\"test_counter\", 5.0, None);\n            collector.set_gauge(\"test_gauge\", 42.0, None);\n        }\n\n        let exporter = PrometheusExporter::new(metrics);\n        let output = exporter.export();\n\n        assert!(output.contains(\"test_counter\"));\n        assert!(output.contains(\"test_gauge\"));\n        assert!(output.contains(\"counter\"));\n        assert!(output.contains(\"gauge\"));\n    }\n\n    #[tokio::test]\n    async fn test_benchmark_runner() {\n        let runner = BenchmarkRunner::new();\n\n        let result = runner\n            .run_benchmark(\"test_operation\", || async {\n                tokio::time::sleep(Duration::from_millis(10)).await;\n            })\n            .await;\n\n        assert!(result.success);\n        assert!(result.duration.as_millis() \u003e= 10);\n\n        let metrics = runner.metrics().lock();\n        assert!(metrics.get_metrics().len() \u003e 0);\n    }\n}\n","traces":[{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":360,"address":[],"length":0,"stats":{"Line":0}},{"line":361,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":0}},{"line":370,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":20},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","application","job_use_cases.rs"],"content":"//! Application layer - Use cases and Application Services\n//!\n//! This layer orchestrates domain entities and provides application-specific logic.\n\nuse crate::domain::{Job, JobEvent};\nuse async_trait::async_trait;\nuse hodei_shared_types::{CorrelationId, DomainError, JobId, JobSpec, TenantId};\n\n/// Data Transfer Objects\npub mod dtos {\n    use serde::{Deserialize, Serialize};\n\n    #[derive(Debug, Serialize, Deserialize)]\n    pub struct CreateJobRequest {\n        pub spec: hodei_shared_types::JobSpec,\n        pub correlation_id: Option\u003chodei_shared_types::CorrelationId\u003e,\n        pub tenant_id: hodei_shared_types::TenantId,\n    }\n\n    #[derive(Debug, Serialize, Deserialize)]\n    pub struct CreateJobResponse {\n        pub job_id: hodei_shared_types::JobId,\n    }\n\n    #[derive(Debug, Serialize, Deserialize)]\n    pub struct JobStatusResponse {\n        pub job_id: hodei_shared_types::JobId,\n        pub state: String,\n        pub attempts: u8,\n        pub created_at: Option\u003cString\u003e,\n        pub started_at: Option\u003cString\u003e,\n        pub completed_at: Option\u003cString\u003e,\n    }\n}\n\n/// Job Use Cases\n#[async_trait]\npub trait JobUseCases {\n    async fn create_job(\n        \u0026self,\n        request: dtos::CreateJobRequest,\n    ) -\u003e Result\u003cdtos::CreateJobResponse, DomainError\u003e;\n\n    async fn get_job_status(\u0026self, job_id: JobId) -\u003e Result\u003cdtos::JobStatusResponse, DomainError\u003e;\n\n    async fn cancel_job(\u0026self, job_id: JobId) -\u003e Result\u003c(), DomainError\u003e;\n}\n\n/// In-memory Job Repository (for testing)\npub struct InMemoryJobRepository {\n    jobs: std::sync::Arc\u003cstd::sync::Mutex\u003cstd::collections::HashMap\u003cJobId, Job\u003e\u003e\u003e,\n}\n\nimpl InMemoryJobRepository {\n    pub fn new() -\u003e Self {\n        Self {\n            jobs: std::sync::Arc::new(std::sync::Mutex::new(std::collections::HashMap::new())),\n        }\n    }\n\n    pub async fn save(\u0026self, job: Job) -\u003e Result\u003c(), DomainError\u003e {\n        let mut jobs = self.jobs.lock().unwrap();\n        jobs.insert(job.id.clone(), job);\n        Ok(())\n    }\n\n    pub async fn find(\u0026self, job_id: \u0026JobId) -\u003e Result\u003cOption\u003cJob\u003e, DomainError\u003e {\n        let jobs = self.jobs.lock().unwrap();\n        Ok(jobs.get(job_id).cloned())\n    }\n}\n\n/// Job Application Service\npub struct JobApplicationService {\n    repository: InMemoryJobRepository,\n}\n\nimpl JobApplicationService {\n    pub fn new(repository: InMemoryJobRepository) -\u003e Self {\n        Self { repository }\n    }\n}\n\n#[async_trait]\nimpl JobUseCases for JobApplicationService {\n    async fn create_job(\n        \u0026self,\n        request: dtos::CreateJobRequest,\n    ) -\u003e Result\u003cdtos::CreateJobResponse, DomainError\u003e {\n        let correlation_id = request.correlation_id.unwrap_or_default();\n\n        // Use domain service to create job\n        let job = Job::create(request.spec, correlation_id, request.tenant_id)?;\n\n        // Save to repository\n        self.repository.save(job.clone()).await?;\n\n        Ok(dtos::CreateJobResponse { job_id: job.id })\n    }\n\n    async fn get_job_status(\u0026self, job_id: JobId) -\u003e Result\u003cdtos::JobStatusResponse, DomainError\u003e {\n        let job =\n            self.repository.find(\u0026job_id).await?.ok_or_else(|| {\n                DomainError::NotFound(format!(\"job {} not found\", job_id.as_uuid()))\n            })?;\n\n        Ok(dtos::JobStatusResponse {\n            job_id: job.id,\n            state: job.state.as_str().to_string(),\n            attempts: job.attempts,\n            created_at: job.created_at.map(|t| t.to_rfc3339()),\n            started_at: job.started_at.map(|t| t.to_rfc3339()),\n            completed_at: job.completed_at.map(|t| t.to_rfc3339()),\n        })\n    }\n\n    async fn cancel_job(\u0026self, job_id: JobId) -\u003e Result\u003c(), DomainError\u003e {\n        let mut job =\n            self.repository.find(\u0026job_id).await?.ok_or_else(|| {\n                DomainError::NotFound(format!(\"job {} not found\", job_id.as_uuid()))\n            })?;\n\n        job.cancel()?;\n\n        self.repository.save(job).await?;\n        Ok(())\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","application","mod.rs"],"content":"//! Application layer module\n\npub mod job_use_cases;\n\npub use job_use_cases::{dtos, JobApplicationService, JobUseCases};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","domain","job_entity.rs"],"content":"//! Job Entity implementation\n//!\n//! This module contains the Job aggregate root with state management\n//! and business logic according to DDD principles.\n\nuse hodei_shared_types::job_definitions::{ExecResult, JobId, JobSpec, JobState};\nuse hodei_shared_types::{CorrelationId, DateTime, DomainError, TenantId, Utc};\n\n/// Job aggregate root\n#[derive(Debug, Clone)]\npub struct Job {\n    pub id: JobId,\n    pub spec: JobSpec,\n    pub state: JobState,\n    pub attempts: u8,\n    pub created_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub started_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub completed_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub correlation_id: CorrelationId,\n    pub tenant_id: TenantId,\n}\n\n#[derive(Debug)]\npub enum JobEvent {\n    JobRequested {\n        job_id: JobId,\n        spec: JobSpec,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    },\n    JobScheduled {\n        job_id: JobId,\n    },\n    JobStarted {\n        job_id: JobId,\n    },\n    JobCompleted {\n        job_id: JobId,\n        result: ExecResult,\n    },\n    JobFailed {\n        job_id: JobId,\n        error: String,\n        retryable: bool,\n    },\n    JobCancelled {\n        job_id: JobId,\n    },\n}\n\nimpl Job {\n    /// Create a new Job with PENDING state\n    pub fn create(\n        spec: JobSpec,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003cSelf, DomainError\u003e {\n        // Validate spec before creation\n        spec.validate()?;\n\n        Ok(Self {\n            id: JobId::new(),\n            spec,\n            state: JobState::from(JobState::PENDING.to_string()),\n            attempts: 1,\n            created_at: Some(Utc::now()),\n            started_at: None,\n            completed_at: None,\n            correlation_id,\n            tenant_id,\n        })\n    }\n\n    /// Transition job to SCHEDULED state\n    pub fn schedule(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::SCHEDULED.to_string()))?;\n        self.state = JobState::from(JobState::SCHEDULED.to_string());\n\n        Ok(vec![JobEvent::JobScheduled {\n            job_id: self.id.clone(),\n        }])\n    }\n\n    /// Transition job to RUNNING state\n    pub fn start(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::RUNNING.to_string()))?;\n        self.state = JobState::from(JobState::RUNNING.to_string());\n        self.started_at = Some(Utc::now());\n\n        Ok(vec![JobEvent::JobStarted {\n            job_id: self.id.clone(),\n        }])\n    }\n\n    /// Transition job to SUCCESS state\n    pub fn complete(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::SUCCESS.to_string()))?;\n        self.state = JobState::from(JobState::SUCCESS.to_string());\n        self.completed_at = Some(Utc::now());\n\n        let result = ExecResult {\n            exit_code: 0,\n            stdout: None,\n            stderr: None,\n        };\n\n        Ok(vec![JobEvent::JobCompleted {\n            job_id: self.id.clone(),\n            result,\n        }])\n    }\n\n    /// Transition job to FAILED state\n    pub fn fail(\u0026mut self, error: String, retryable: bool) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::FAILED.to_string()))?;\n        self.state = JobState::from(JobState::FAILED.to_string());\n        self.completed_at = Some(Utc::now());\n\n        Ok(vec![JobEvent::JobFailed {\n            job_id: self.id.clone(),\n            error,\n            retryable,\n        }])\n    }\n\n    /// Transition job to CANCELLED state\n    pub fn cancel(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::CANCELLED.to_string()))?;\n        self.state = JobState::from(JobState::CANCELLED.to_string());\n\n        Ok(vec![JobEvent::JobCancelled {\n            job_id: self.id.clone(),\n        }])\n    }\n\n    /// Retry a failed job (transition to PENDING)\n    pub fn retry(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        // Check max retries\n        if self.attempts \u003e= self.spec.retries {\n            return Err(DomainError::Validation(format!(\n                \"max retries ({}) exceeded\",\n                self.spec.retries\n            )));\n        }\n\n        self.ensure_transition_valid(\u0026JobState::from(JobState::PENDING.to_string()))?;\n        self.state = JobState::from(JobState::PENDING.to_string());\n        self.attempts += 1;\n\n        Ok(vec![JobEvent::JobRequested {\n            job_id: self.id.clone(),\n            spec: self.spec.clone(),\n            correlation_id: self.correlation_id.clone(),\n            tenant_id: self.tenant_id.clone(),\n        }])\n    }\n\n    /// Manual state transition with validation\n    pub fn transition_to(\u0026mut self, target_state: \u0026JobState) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(target_state)?;\n        self.state = target_state.clone();\n\n        match target_state.as_str() {\n            JobState::RUNNING =\u003e self.started_at = Some(Utc::now()),\n            JobState::SUCCESS | JobState::FAILED | JobState::CANCELLED =\u003e {\n                self.completed_at = Some(Utc::now())\n            }\n            _ =\u003e {}\n        }\n\n        Ok(vec![])\n    }\n\n    /// Validate state transition\n    fn ensure_transition_valid(\u0026self, target: \u0026JobState) -\u003e Result\u003c(), DomainError\u003e {\n        if !self.state.can_transition_to(target) {\n            return Err(DomainError::invalid_state_transition(\n                self.state.as_str(),\n                target.as_str(),\n            ));\n        }\n        Ok(())\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","domain","mod.rs"],"content":"//! Domain layer - Core business logic\n//!\n//! This layer contains entities, value objects, and domain services.\n//! It is independent of infrastructure concerns.\n\npub mod job_entity;\n\npub use job_entity::{Job, JobEvent};\n\n/// Domain services\npub mod services {\n    use hodei_shared_types::DomainError;\n\n    /// Job validation service\n    pub struct JobValidator;\n\n    impl JobValidator {\n        pub fn validate_name(name: \u0026str) -\u003e Result\u003c(), DomainError\u003e {\n            if name.trim().is_empty() {\n                return Err(DomainError::Validation(\n                    \"job name cannot be empty\".to_string(),\n                ));\n            }\n            Ok(())\n        }\n\n        pub fn validate_timeout(timeout_ms: u64) -\u003e Result\u003c(), DomainError\u003e {\n            if timeout_ms == 0 {\n                return Err(DomainError::Validation(\n                    \"timeout must be greater than 0\".to_string(),\n                ));\n            }\n            Ok(())\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","lib.rs"],"content":"//! Orchestrator crate\n//!\n//! This crate provides orchestration services for the Hodei CI/CD system.\n\npub mod application;\npub mod domain;\n\npub use application::job_use_cases::{JobApplicationService, JobUseCases, dtos};\npub use domain::{Job, JobEvent};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","tests","unit","domain","test_job_entity.rs"],"content":"#[cfg(test)]\nmod tests {\n    use hodei_shared_types::job_definitions::{JobId, JobSpec, ResourceQuota};\n    use hodei_shared_types::{CorrelationId, DomainError, TenantId};\n    use std::collections::HashMap;\n\n    #[test]\n    fn test_job_creation_with_valid_spec() {\n        // Arrange\n        let spec = JobSpec {\n            name: \"test-job\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string(), \"hello\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        };\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n\n        // Act\n        let result = Job::create(spec, correlation_id, tenant_id);\n\n        // Assert\n        assert!(result.is_ok());\n        let job = result.unwrap();\n        assert_eq!(job.state.as_str(), JobState::PENDING);\n        assert!(job.created_at.is_some());\n    }\n\n    #[test]\n    fn test_job_creation_with_empty_name() {\n        // Arrange\n        let mut spec = JobSpec {\n            name: \"\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string(), \"hello\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        };\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n\n        // Act\n        let result = Job::create(spec, correlation_id, tenant_id);\n\n        // Assert\n        assert!(result.is_err());\n        if let Err(e) = result {\n            assert!(matches!(e, DomainError::Validation(_)));\n        }\n    }\n\n    #[test]\n    fn test_job_state_transition_from_pending_to_scheduled() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n\n        // Act\n        let events = job.schedule().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::SCHEDULED);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_state_transition_from_scheduled_to_running() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n\n        // Act\n        let events = job.start().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::RUNNING);\n        assert!(job.started_at.is_some());\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_state_transition_from_running_to_success() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n\n        // Act\n        let events = job.complete().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::SUCCESS);\n        assert!(job.completed_at.is_some());\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_state_transition_from_running_to_failed() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n\n        // Act\n        let events = job.fail(\"Command failed\".to_string(), true).unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::FAILED);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_invalid_state_transition_from_running_to_pending() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n\n        // Act\n        let result = job.transition_to(\u0026JobState::from(JobState::PENDING.to_string()));\n\n        // Assert\n        assert!(result.is_err());\n        if let Err(e) = result {\n            assert!(matches!(e, DomainError::InvalidStateTransition { .. }));\n        }\n    }\n\n    #[test]\n    fn test_job_cancellation_from_pending() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n\n        // Act\n        let events = job.cancel().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::CANCELLED);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_retry_from_failed() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n        job.fail(\"Command failed\".to_string(), true).unwrap();\n\n        // Act\n        let events = job.retry().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::PENDING);\n        assert!(job.attempts \u003e 1);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_with_max_retries() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec.clone(), correlation_id.clone(), tenant_id.clone()).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n        job.fail(\"Command failed\".to_string(), true).unwrap();\n        job.retry().unwrap();\n\n        let mut second_job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        second_job.schedule().unwrap();\n        second_job.start().unwrap();\n        second_job.fail(\"Command failed\".to_string(), true).unwrap();\n\n        // Act\n        let events = second_job.retry();\n\n        // Assert - should fail due to max retries\n        assert!(events.is_err());\n    }\n\n    fn create_test_spec() -\u003e JobSpec {\n        JobSpec {\n            name: \"test-job\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string(), \"hello\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","worker-lifecycle","src","lib.rs"],"content":"//! Worker Lifecycle Management (US-005)\n//!\n//! This module provides comprehensive worker lifecycle management including:\n//! - Worker state transitions (AVAILABLE, RUNNING, UNHEALTHY, DRAINING)\n//! - Health monitoring with heartbeat-based detection\n//! - Auto-recovery mechanisms\n//! - Capability matching for optimal job scheduling\n\nuse hodei_shared_types::{CorrelationId, TenantId, Uuid};\nuse parking_lot::Mutex;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::Duration;\nuse tokio::time::{Instant, interval};\n\n/// Worker state representing lifecycle stage\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum WorkerState {\n    /// Worker is available for job assignment\n    AVAILABLE,\n    /// Worker is currently executing jobs\n    RUNNING,\n    /// Worker is unhealthy and not accepting new jobs\n    UNHEALTHY,\n    /// Worker is draining and completing existing jobs\n    DRAINING,\n}\n\nimpl WorkerState {\n    pub fn as_str(\u0026self) -\u003e \u0026'static str {\n        match self {\n            WorkerState::AVAILABLE =\u003e \"AVAILABLE\",\n            WorkerState::RUNNING =\u003e \"RUNNING\",\n            WorkerState::UNHEALTHY =\u003e \"UNHEALTHY\",\n            WorkerState::DRAINING =\u003e \"DRAINING\",\n        }\n    }\n\n    pub fn can_accept_jobs(\u0026self) -\u003e bool {\n        matches!(self, WorkerState::AVAILABLE | WorkerState::RUNNING)\n    }\n\n    pub fn is_healthy(\u0026self) -\u003e bool {\n        matches!(self, WorkerState::AVAILABLE | WorkerState::RUNNING)\n    }\n}\n\n/// Worker capabilities (e.g., CPU type, GPU, special hardware)\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct WorkerCapabilities {\n    pub cpu_cores: u32,\n    pub memory_gb: u64,\n    pub has_gpu: bool,\n    pub gpu_count: Option\u003cu8\u003e,\n    pub specialized_hardware: Vec\u003cString\u003e,\n    pub container_runtime: String,\n}\n\nimpl Default for WorkerCapabilities {\n    fn default() -\u003e Self {\n        Self {\n            cpu_cores: 4,\n            memory_gb: 8,\n            has_gpu: false,\n            gpu_count: None,\n            specialized_hardware: vec![],\n            container_runtime: \"docker\".to_string(),\n        }\n    }\n}\n\n/// Worker information and state\n#[derive(Debug, Clone)]\npub struct Worker {\n    pub id: Uuid,\n    pub state: WorkerState,\n    pub capabilities: WorkerCapabilities,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub max_jobs: usize,\n    pub last_heartbeat: Instant,\n    pub failure_count: u32,\n    pub tenant_id: Option\u003cTenantId\u003e,\n}\n\nimpl Worker {\n    pub fn new(\n        id: Uuid,\n        capabilities: WorkerCapabilities,\n        max_jobs: usize,\n        tenant_id: Option\u003cTenantId\u003e,\n    ) -\u003e Self {\n        Self {\n            id,\n            state: WorkerState::AVAILABLE,\n            capabilities,\n            load: 0.0,\n            jobs_running: 0,\n            max_jobs,\n            last_heartbeat: Instant::now(),\n            failure_count: 0,\n            tenant_id,\n        }\n    }\n\n    pub fn can_accept_job(\u0026self) -\u003e bool {\n        self.state.can_accept_jobs() \u0026\u0026 self.jobs_running \u003c self.max_jobs \u0026\u0026 self.state.is_healthy()\n    }\n\n    pub fn update_heartbeat(\u0026mut self) {\n        self.last_heartbeat = Instant::now();\n        self.failure_count = 0;\n    }\n\n    pub fn record_failure(\u0026mut self) {\n        self.failure_count += 1;\n        if self.failure_count \u003e= 3 {\n            self.state = WorkerState::UNHEALTHY;\n        }\n    }\n}\n\n/// Worker Manager for lifecycle operations\npub struct WorkerManager {\n    workers: Arc\u003cMutex\u003cHashMap\u003cUuid, Worker\u003e\u003e\u003e,\n    coordinator: Option\u003cArc\u003chodei_coordinator::JobCoordinator\u003e\u003e,\n    health_check_interval: Duration,\n    heartbeat_timeout: Duration,\n}\n\nimpl WorkerManager {\n    pub fn new(\n        coordinator: Option\u003cArc\u003chodei_coordinator::JobCoordinator\u003e\u003e,\n        health_check_interval: Duration,\n        heartbeat_timeout: Duration,\n    ) -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n            coordinator,\n            health_check_interval,\n            heartbeat_timeout,\n        }\n    }\n\n    /// Register a new worker\n    pub fn register_worker(\u0026self, worker: Worker) -\u003e Result\u003c(), WorkerLifecycleError\u003e {\n        let worker_id = worker.id;\n        let mut workers = self.workers.lock();\n\n        if workers.contains_key(\u0026worker_id) {\n            return Err(WorkerLifecycleError::WorkerAlreadyExists);\n        }\n\n        workers.insert(worker_id, worker);\n        tracing::info!(\"Worker registered: {}\", worker_id);\n\n        Ok(())\n    }\n\n    /// Deregister a worker (graceful shutdown)\n    pub async fn deregister_worker(\u0026self, worker_id: Uuid) -\u003e Result\u003c(), WorkerLifecycleError\u003e {\n        let mut workers = self.workers.lock();\n\n        if let Some(mut worker) = workers.remove(\u0026worker_id) {\n            if worker.jobs_running \u003e 0 {\n                worker.state = WorkerState::DRAINING;\n                workers.insert(worker_id, worker);\n                tracing::info!(\"Worker {} entered draining state\", worker_id);\n\n                // Wait for jobs to complete (simplified - in real impl would track completion)\n                tokio::time::sleep(Duration::from_secs(30)).await;\n                workers.remove(\u0026worker_id);\n            }\n\n            tracing::info!(\"Worker deregistered: {}\", worker_id);\n        }\n\n        Ok(())\n    }\n\n    /// Process worker heartbeat\n    pub fn handle_heartbeat(\n        \u0026self,\n        worker_id: Uuid,\n        load: f64,\n        jobs_running: usize,\n    ) -\u003e Result\u003c(), WorkerLifecycleError\u003e {\n        let mut workers = self.workers.lock();\n\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.update_heartbeat();\n            worker.load = load;\n            worker.jobs_running = jobs_running;\n\n            if worker.state == WorkerState::UNHEALTHY \u0026\u0026 worker.failure_count \u003c 3 {\n                worker.state = WorkerState::AVAILABLE;\n                tracing::info!(\"Worker {} recovered from unhealthy state\", worker_id);\n            }\n\n            return Ok(());\n        }\n\n        Err(WorkerLifecycleError::WorkerNotFound)\n    }\n\n    /// Find available workers with matching capabilities\n    pub fn find_suitable_workers(\n        \u0026self,\n        required_cpus: u32,\n        required_memory_gb: u64,\n        needs_gpu: bool,\n        max_jobs_per_worker: usize,\n    ) -\u003e Vec\u003cUuid\u003e {\n        let workers = self.workers.lock();\n\n        let mut suitable_workers: Vec\u003c_\u003e = workers\n            .values()\n            .filter(|w| {\n                w.can_accept_job()\n                    \u0026\u0026 w.capabilities.cpu_cores \u003e= required_cpus\n                    \u0026\u0026 w.capabilities.memory_gb \u003e= required_memory_gb\n                    \u0026\u0026 (!needs_gpu || w.capabilities.has_gpu)\n                    \u0026\u0026 w.jobs_running \u003c max_jobs_per_worker\n            })\n            .cloned()\n            .collect();\n\n        // Sort by load (ascending) and job count (ascending)\n        suitable_workers.sort_by(|a, b| {\n            a.load\n                .partial_cmp(\u0026b.load)\n                .unwrap_or(std::cmp::Ordering::Equal)\n                .then_with(|| a.jobs_running.cmp(\u0026b.jobs_running))\n        });\n\n        suitable_workers.into_iter().map(|w| w.id).collect()\n    }\n\n    /// Mark worker as unhealthy\n    pub fn mark_unhealthy(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.state = WorkerState::UNHEALTHY;\n            worker.record_failure();\n            tracing::warn!(\n                \"Worker {} marked as unhealthy (failures: {})\",\n                worker_id,\n                worker.failure_count\n            );\n        }\n    }\n\n    /// Get worker status\n    pub fn get_worker_status(\n        \u0026self,\n        worker_id: Uuid,\n    ) -\u003e Result\u003cWorkerStatusResponse, WorkerLifecycleError\u003e {\n        let workers = self.workers.lock();\n\n        if let Some(worker) = workers.get(\u0026worker_id) {\n            Ok(WorkerStatusResponse {\n                worker_id: worker.id,\n                state: worker.state.as_str().to_string(),\n                load: worker.load,\n                jobs_running: worker.jobs_running,\n                max_jobs: worker.max_jobs,\n                last_heartbeat: Some(worker.last_heartbeat.elapsed().as_secs()),\n                capabilities: worker.capabilities.clone(),\n                failure_count: worker.failure_count,\n            })\n        } else {\n            Err(WorkerLifecycleError::WorkerNotFound)\n        }\n    }\n\n    /// Start health monitoring background task\n    pub fn start_health_monitoring(\u0026self) {\n        let workers = Arc::clone(\u0026self.workers);\n        let heartbeat_timeout = self.heartbeat_timeout;\n        let coordinator = self.coordinator.clone();\n\n        tokio::spawn(async move {\n            let mut interval = interval(Duration::from_secs(5));\n\n            loop {\n                interval.tick().await;\n\n                let mut failed_workers = vec![];\n                {\n                    let workers_guard = workers.lock();\n                    let now = Instant::now();\n\n                    for (worker_id, worker) in workers_guard.iter() {\n                        if now.duration_since(worker.last_heartbeat) \u003e heartbeat_timeout {\n                            failed_workers.push(*worker_id);\n                        }\n                    }\n                }\n\n                // Process failures outside the lock\n                for worker_id in failed_workers {\n                    tracing::warn!(\"Worker {} heartbeat timeout detected\", worker_id);\n\n                    // Mark as unhealthy\n                    {\n                        let mut workers_guard = workers.lock();\n                        if let Some(worker) = workers_guard.get_mut(\u0026worker_id) {\n                            worker.state = WorkerState::UNHEALTHY;\n                            worker.record_failure();\n                        }\n                    }\n\n                    // Notify coordinator if available\n                    if let Some(coordinator) = \u0026coordinator {\n                        coordinator.handle_worker_failure(worker_id);\n                    }\n                }\n            }\n        });\n    }\n\n    /// Get all workers summary\n    pub fn get_workers_summary(\u0026self) -\u003e Vec\u003cWorkerSummary\u003e {\n        let workers = self.workers.lock();\n\n        workers\n            .values()\n            .map(|w| WorkerSummary {\n                worker_id: w.id,\n                state: w.state.as_str().to_string(),\n                load: w.load,\n                jobs_running: w.jobs_running,\n                has_gpu: w.capabilities.has_gpu,\n            })\n            .collect()\n    }\n}\n\n/// Worker status response\n#[derive(Debug, Serialize, Deserialize)]\npub struct WorkerStatusResponse {\n    pub worker_id: Uuid,\n    pub state: String,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub max_jobs: usize,\n    pub last_heartbeat: Option\u003cu64\u003e,\n    pub capabilities: WorkerCapabilities,\n    pub failure_count: u32,\n}\n\n/// Worker summary for listing\n#[derive(Debug, Serialize, Deserialize)]\npub struct WorkerSummary {\n    pub worker_id: Uuid,\n    pub state: String,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub has_gpu: bool,\n}\n\n/// Error types\n#[derive(Debug, thiserror::Error)]\npub enum WorkerLifecycleError {\n    #[error(\"worker not found\")]\n    WorkerNotFound,\n\n    #[error(\"worker already exists\")]\n    WorkerAlreadyExists,\n\n    #[error(\"invalid worker state transition\")]\n    InvalidStateTransition,\n\n    #[error(\"worker is unhealthy\")]\n    WorkerUnhealthy,\n\n    #[error(\"worker at capacity\")]\n    WorkerAtCapacity,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::time::Duration;\n\n    #[test]\n    fn test_worker_creation() {\n        let worker = Worker::new(Uuid::new_v4(), WorkerCapabilities::default(), 10, None);\n\n        assert_eq!(worker.state, WorkerState::AVAILABLE);\n        assert_eq!(worker.jobs_running, 0);\n        assert_eq!(worker.max_jobs, 10);\n        assert!(worker.can_accept_job());\n    }\n\n    #[test]\n    fn test_worker_state_transitions() {\n        let mut worker = Worker::new(Uuid::new_v4(), WorkerCapabilities::default(), 10, None);\n\n        // Start job\n        worker.jobs_running = 5;\n        assert!(worker.can_accept_job());\n\n        // Too many jobs\n        worker.jobs_running = 10;\n        assert!(!worker.can_accept_job());\n\n        // Unhealthy\n        worker.state = WorkerState::UNHEALTHY;\n        assert!(!worker.can_accept_job());\n    }\n\n    #[test]\n    fn test_capability_matching() {\n        let manager = WorkerManager::new(None, Duration::from_secs(30), Duration::from_secs(10));\n\n        // Register workers with different capabilities\n        manager\n            .register_worker(Worker::new(\n                Uuid::new_v4(),\n                WorkerCapabilities {\n                    cpu_cores: 8,\n                    memory_gb: 16,\n                    has_gpu: true,\n                    gpu_count: Some(1),\n                    specialized_hardware: vec![\"nvme\".to_string()],\n                    container_runtime: \"docker\".to_string(),\n                },\n                5,\n                None,\n            ))\n            .unwrap();\n\n        // Find suitable workers\n        let suitable = manager.find_suitable_workers(4, 8, true, 5);\n        assert_eq!(suitable.len(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_heartbeat_handling() {\n        let manager = WorkerManager::new(None, Duration::from_secs(30), Duration::from_secs(10));\n\n        let worker_id = Uuid::new_v4();\n        manager\n            .register_worker(Worker::new(\n                worker_id,\n                WorkerCapabilities::default(),\n                10,\n                None,\n            ))\n            .unwrap();\n\n        // Send heartbeat\n        assert!(manager.handle_heartbeat(worker_id, 0.5, 2).is_ok());\n\n        // Verify status\n        let status = manager.get_worker_status(worker_id).unwrap();\n        assert_eq!(status.state, \"AVAILABLE\");\n        assert_eq!(status.load, 0.5);\n        assert_eq!(status.jobs_running, 2);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","factory","mod.rs"],"content":"//! Provider Factory Module\n//!\n//! This module provides factory patterns for creating and managing worker providers.\n\nuse crate::models::{ProviderConfig, ProviderCredentials, ProviderType};\nuse crate::traits::WorkerProvider;\nuse crate::{MockWorkerProvider, ProviderError};\nuse std::collections::HashMap;\n\n/// Provider factory for creating and managing provider instances\npub struct ProviderFactory {\n    // For simplicity, we'll use an enum-based approach\n    // In production, this could be extended with a plugin system\n}\n\nimpl ProviderFactory {\n    pub fn new() -\u003e Result\u003cSelf, ProviderError\u003e {\n        Ok(Self {})\n    }\n\n    /// Create a provider instance\n    pub async fn create_provider(\n        \u0026self,\n        provider_type: ProviderType,\n        config: Option\u003cProviderConfig\u003e,\n    ) -\u003e Result\u003cBox\u003cdyn WorkerProvider\u003e, ProviderError\u003e {\n        let provider_config = config.ok_or_else(|| {\n            ProviderError::ConfigurationError(\"Provider configuration is required\".to_string())\n        })?;\n\n        match provider_type {\n            ProviderType::Kubernetes =\u003e {\n                Ok(Box::new(MockWorkerProvider::new(ProviderType::Kubernetes))\n                    as Box\u003cdyn WorkerProvider\u003e)\n            }\n            ProviderType::Docker =\u003e {\n                Ok(Box::new(MockWorkerProvider::new(ProviderType::Docker))\n                    as Box\u003cdyn WorkerProvider\u003e)\n            }\n            ProviderType::AwsEcs =\u003e Err(ProviderError::ConfigurationError(\n                \"AWS ECS provider not yet implemented\".to_string(),\n            )),\n            ProviderType::AzureContainerInstances =\u003e Err(ProviderError::ConfigurationError(\n                \"Azure provider not yet implemented\".to_string(),\n            )),\n            ProviderType::Custom(_) =\u003e Err(ProviderError::ConfigurationError(\n                \"Custom providers not yet implemented\".to_string(),\n            )),\n        }\n    }\n\n    /// Detect the best provider type based on environment\n    pub async fn detect_best_provider(\u0026self) -\u003e Result\u003cProviderType, ProviderError\u003e {\n        // For now, default to Kubernetes\n        // In production, this would detect available providers\n        Ok(ProviderType::Kubernetes)\n    }\n}\n","traces":[{"line":17,"address":[2295664],"length":1,"stats":{"Line":2}},{"line":18,"address":[2295667],"length":1,"stats":{"Line":5}},{"line":22,"address":[2295680],"length":1,"stats":{"Line":1}},{"line":27,"address":[2355696,2354508,2354426,2354584],"length":1,"stats":{"Line":5}},{"line":28,"address":[2355710],"length":1,"stats":{"Line":1}},{"line":31,"address":[2354687],"length":1,"stats":{"Line":2}},{"line":33,"address":[2354756,2354984],"length":1,"stats":{"Line":2}},{"line":37,"address":[2355062,2354800],"length":1,"stats":{"Line":0}},{"line":38,"address":[2355114],"length":1,"stats":{"Line":0}},{"line":40,"address":[2355116],"length":1,"stats":{"Line":1}},{"line":41,"address":[2354834],"length":1,"stats":{"Line":1}},{"line":43,"address":[2355245],"length":1,"stats":{"Line":1}},{"line":44,"address":[2354868],"length":1,"stats":{"Line":1}},{"line":46,"address":[2355374],"length":1,"stats":{"Line":0}},{"line":47,"address":[2354902],"length":1,"stats":{"Line":0}},{"line":53,"address":[2295800,2295792],"length":1,"stats":{"Line":4}},{"line":56,"address":[2355861],"length":1,"stats":{"Line":1}}],"covered":13,"coverable":17},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","lib.rs"],"content":"//! Worker Provider Abstraction Layer (US-018)\n//!\n//! This module provides a unified abstraction for worker providers\n//! supporting multiple execution backends (Kubernetes, Docker, AWS ECS, etc.).\n//!\n//! Features:\n//! - Provider abstraction with trait-based interfaces\n//! - Provider factory for automatic instantiation\n//! - Capability detection and feature matrix\n//! - Multi-provider coordination\n//! - Error handling with retry and circuit breaker patterns\n//! - Performance optimization with connection pooling\n\npub mod factory;\npub mod models;\npub mod traits;\n\npub use factory::ProviderFactory;\npub use models::*;\npub use traits::WorkerProvider;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::collections::HashMap;\n\n    #[tokio::test]\n    async fn test_provider_factory_creation() {\n        // RED: Test that we can create a provider factory\n        let factory = ProviderFactory::new();\n\n        // GREEN: Just check the factory was created\n        assert!(factory.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_provider_creation_without_config() {\n        // RED: Test that provider creation fails without configuration\n        let factory = ProviderFactory::new().unwrap();\n        let result = factory\n            .create_provider(ProviderType::Kubernetes, None)\n            .await;\n\n        // GREEN: Check it returns the expected error type\n        assert!(matches!(result, Err(ProviderError::ConfigurationError(_))));\n    }\n\n    #[tokio::test]\n    async fn test_provider_creation_with_config() {\n        // RED: Test that we can create a provider with configuration\n        let factory = ProviderFactory::new().unwrap();\n        let config = ProviderConfig {\n            provider_type: ProviderType::Kubernetes,\n            connection_string: \"test\".to_string(),\n            credentials: None,\n            options: HashMap::new(),\n        };\n\n        let result = factory\n            .create_provider(ProviderType::Kubernetes, Some(config))\n            .await;\n\n        // GREEN: Check provider was created successfully\n        assert!(result.is_ok());\n        let provider = result.unwrap();\n        assert_eq!(provider.get_provider_type(), ProviderType::Kubernetes);\n    }\n\n    #[tokio::test]\n    async fn test_mock_worker_provider_operations() {\n        // RED: Test basic worker operations on mock provider\n        let provider = MockWorkerProvider::new(ProviderType::Kubernetes);\n\n        // Create a test worker config\n        use hodei_shared_types::worker_messages::WorkerId;\n\n        let worker_id = WorkerId::new();\n        let worker_config = WorkerConfig {\n            worker_id: worker_id.clone(),\n            image: \"test-image:latest\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 1.0,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n\n        // Create worker\n        let handle = provider.create_worker(\u0026worker_config).await.unwrap();\n        assert_eq!(handle.worker_id, worker_id);\n\n        // Get status\n        let status = provider.get_worker_status(\u0026worker_id).await.unwrap();\n        assert_eq!(status.state, WorkerState::Creating);\n\n        // Stop worker\n        let result = provider.stop_worker(\u0026worker_id, true).await;\n        assert!(result.is_ok());\n\n        // Delete worker\n        let result = provider.delete_worker(\u0026worker_id).await;\n        assert!(result.is_ok());\n\n        // Verify worker is deleted\n        let status_result = provider.get_worker_status(\u0026worker_id).await;\n        assert!(status_result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_provider_capabilities() {\n        // RED: Test that provider capabilities can be retrieved\n        let provider = MockWorkerProvider::new(ProviderType::Kubernetes);\n\n        // GREEN: Check capabilities are returned\n        let capabilities = provider.get_capabilities().await.unwrap();\n        assert_eq!(capabilities.auto_scaling, false);\n        assert_eq!(capabilities.health_checks, false);\n    }\n\n    #[tokio::test]\n    async fn test_worker_config_validation() {\n        // RED: Test worker config validation\n        use hodei_shared_types::worker_messages::WorkerId;\n\n        let worker_id = WorkerId::new();\n\n        // Valid config should pass\n        let valid_config = WorkerConfig {\n            worker_id: worker_id.clone(),\n            image: \"test-image:latest\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 1.0,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n        assert!(valid_config.validate().is_ok());\n\n        // Invalid config (empty image) should fail\n        let invalid_config = WorkerConfig {\n            worker_id,\n            image: \"\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 1.0,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n        assert!(invalid_config.validate().is_err());\n\n        // Invalid config (too low CPU) should fail\n        let invalid_config2 = WorkerConfig {\n            worker_id: WorkerId::new(),\n            image: \"test-image:latest\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 0.01,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n        assert!(invalid_config2.validate().is_err());\n    }\n\n    #[tokio::test]\n    async fn test_provider_display_implementations() {\n        // RED: Test Display implementations for enums\n        assert_eq!(format!(\"{}\", ProviderType::Kubernetes), \"kubernetes\");\n        assert_eq!(format!(\"{}\", ProviderType::Docker), \"docker\");\n        assert_eq!(format!(\"{}\", ProviderType::AwsEcs), \"aws-ecs\");\n        assert_eq!(\n            format!(\"{}\", ProviderType::AzureContainerInstances),\n            \"azure-container-instances\"\n        );\n        assert_eq!(\n            format!(\"{}\", ProviderType::Custom(\"custom-provider\".to_string())),\n            \"custom-custom-provider\"\n        );\n\n        assert_eq!(format!(\"{}\", WorkerState::Creating), \"Creating\");\n        assert_eq!(format!(\"{}\", WorkerState::Running), \"Running\");\n        assert_eq!(format!(\"{}\", WorkerState::Failed), \"Failed\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_error_types() {\n        // RED: Test all provider error types\n        let error1 = ProviderError::ConfigurationError(\"test error\".to_string());\n        assert!(matches!(error1, ProviderError::ConfigurationError(_)));\n\n        let error2 = ProviderError::WorkerOperationFailed(\"op failed\".to_string());\n        assert!(matches!(error2, ProviderError::WorkerOperationFailed(_)));\n\n        let error3 = ProviderError::CapabilityNotSupported(\"cap not supported\".to_string());\n        assert!(matches!(error3, ProviderError::CapabilityNotSupported(_)));\n\n        let error4 = ProviderError::Timeout;\n        assert!(matches!(error4, ProviderError::Timeout));\n    }\n\n    #[tokio::test]\n    async fn test_worker_metadata() {\n        // RED: Test WorkerMetadata functionality\n        let metadata = WorkerMetadata::new()\n            .with_label(\"env\".to_string(), \"test\".to_string())\n            .with_label(\"version\".to_string(), \"1.0\".to_string());\n\n        assert_eq!(metadata.labels.get(\"env\").unwrap(), \u0026\"test\".to_string());\n        assert_eq!(metadata.labels.get(\"version\").unwrap(), \u0026\"1.0\".to_string());\n    }\n\n    #[tokio::test]\n    async fn test_worker_handle() {\n        // RED: Test WorkerHandle creation\n        use hodei_shared_types::worker_messages::WorkerId;\n\n        let worker_id = WorkerId::new();\n        let handle = WorkerHandle::new(worker_id.clone(), ProviderType::Kubernetes);\n\n        assert_eq!(handle.worker_id, worker_id);\n        assert_eq!(handle.provider_type, ProviderType::Kubernetes);\n        assert_eq!(handle.metadata.labels.len(), 0);\n    }\n\n    #[tokio::test]\n    async fn test_provider_capabilities_creation() {\n        // RED: Test ProviderCapabilities creation\n        let caps = ProviderCapabilities::new();\n        assert_eq!(caps.auto_scaling, false);\n        assert_eq!(caps.health_checks, false);\n\n        let caps2 = ProviderCapabilities {\n            auto_scaling: true,\n            health_checks: true,\n            volumes: true,\n            config_maps: true,\n            secrets: true,\n            network_policies: false,\n            multi_cluster: false,\n        };\n\n        assert_eq!(caps2.auto_scaling, true);\n        assert_eq!(caps2.health_checks, true);\n        assert_eq!(caps2.network_policies, false);\n    }\n\n    #[tokio::test]\n    async fn test_scale_workers() {\n        // RED: Test scale_workers operation\n        let provider = MockWorkerProvider::new(ProviderType::Kubernetes);\n\n        // Scale operation should succeed but return empty vector (no workers)\n        let result = provider.scale_workers(\"test-worker\", 5).await.unwrap();\n        assert!(result.is_empty());\n    }\n\n    #[tokio::test]\n    async fn test_provider_factory_detect_best_provider() {\n        // RED: Test best provider detection\n        let factory = ProviderFactory::new().unwrap();\n        let best_provider = factory.detect_best_provider().await.unwrap();\n\n        assert_eq!(best_provider, ProviderType::Kubernetes);\n    }\n\n    #[tokio::test]\n    async fn test_unsupported_provider_types() {\n        // RED: Test that unsupported providers return errors\n        let factory = ProviderFactory::new().unwrap();\n\n        let aws_result = factory\n            .create_provider(\n                ProviderType::AwsEcs,\n                Some(ProviderConfig {\n                    provider_type: ProviderType::AwsEcs,\n                    connection_string: \"test\".to_string(),\n                    credentials: None,\n                    options: HashMap::new(),\n                }),\n            )\n            .await;\n\n        assert!(matches!(\n            aws_result,\n            Err(ProviderError::ConfigurationError(_))\n        ));\n\n        let azure_result = factory\n            .create_provider(\n                ProviderType::AzureContainerInstances,\n                Some(ProviderConfig {\n                    provider_type: ProviderType::AzureContainerInstances,\n                    connection_string: \"test\".to_string(),\n                    credentials: None,\n                    options: HashMap::new(),\n                }),\n            )\n            .await;\n\n        assert!(matches!(\n            azure_result,\n            Err(ProviderError::ConfigurationError(_))\n        ));\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","models","mod.rs"],"content":"//! Models module\n//!\n//! This module contains all data types for worker provider abstraction.\n\nuse async_trait::async_trait;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::time::{Duration, SystemTime};\n\nuse crate::traits::WorkerProvider;\npub use hodei_shared_types::worker_messages::WorkerId;\n\n/// Provider types supported by the abstraction layer\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub enum ProviderType {\n    Kubernetes,\n    Docker,\n    AwsEcs,\n    AzureContainerInstances,\n    Custom(String),\n}\n\nimpl std::fmt::Display for ProviderType {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            ProviderType::Kubernetes =\u003e write!(f, \"kubernetes\"),\n            ProviderType::Docker =\u003e write!(f, \"docker\"),\n            ProviderType::AwsEcs =\u003e write!(f, \"aws-ecs\"),\n            ProviderType::AzureContainerInstances =\u003e write!(f, \"azure-container-instances\"),\n            ProviderType::Custom(name) =\u003e write!(f, \"custom-{}\", name),\n        }\n    }\n}\n\n/// Provider configuration\n#[derive(Debug, Clone)]\npub struct ProviderConfig {\n    pub provider_type: ProviderType,\n    pub connection_string: String,\n    pub credentials: Option\u003cProviderCredentials\u003e,\n    pub options: HashMap\u003cString, String\u003e,\n}\n\n/// Provider credentials\n#[derive(Debug, Clone)]\npub struct ProviderCredentials {\n    pub username: Option\u003cString\u003e,\n    pub password: Option\u003cString\u003e,\n    pub token: Option\u003cString\u003e,\n    pub certificate_path: Option\u003cString\u003e,\n}\n\n/// Worker configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct WorkerConfig {\n    pub worker_id: WorkerId,\n    pub image: String,\n    pub resources: ResourceRequirements,\n    pub environment: HashMap\u003cString, String\u003e,\n    pub secrets: Vec\u003cSecretReference\u003e,\n    pub health_checks: Vec\u003cHealthCheckConfig\u003e,\n    pub scaling_config: ScalingConfiguration,\n}\n\nimpl WorkerConfig {\n    pub fn validate(\u0026self) -\u003e Result\u003c(), ProviderError\u003e {\n        if self.image.is_empty() {\n            return Err(ProviderError::ConfigurationError(\n                \"image cannot be empty\".to_string(),\n            ));\n        }\n\n        if self.resources.cpu_cores \u003c 0.1 {\n            return Err(ProviderError::ConfigurationError(\n                \"cpu_cores must be at least 0.1\".to_string(),\n            ));\n        }\n\n        Ok(())\n    }\n}\n\n/// Resource requirements for workers\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ResourceRequirements {\n    pub cpu_cores: f64,\n    pub memory_bytes: u64,\n    pub ephemeral_storage_bytes: Option\u003cu64\u003e,\n}\n\n/// Reference to a secret\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SecretReference {\n    pub name: String,\n    pub mount_path: String,\n}\n\n/// Health check configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct HealthCheckConfig {\n    pub path: String,\n    pub port: u16,\n    pub interval: Duration,\n    pub timeout: Duration,\n    pub initial_delay: Duration,\n}\n\n/// Scaling configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ScalingConfiguration {\n    pub min_replicas: usize,\n    pub max_replicas: usize,\n    pub target_cpu_utilization: Option\u003cu32\u003e,\n}\n\n/// Worker status\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct WorkerStatus {\n    pub worker_id: WorkerId,\n    pub state: WorkerState,\n    pub started_at: Option\u003cSystemTime\u003e,\n    pub last_health_check: Option\u003cSystemTime\u003e,\n    pub resource_usage: ResourceUsage,\n}\n\n/// Worker state enum\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub enum WorkerState {\n    Creating,\n    Starting,\n    Running,\n    Degraded,\n    Recovering,\n    Stopping,\n    Stopped,\n    Failed,\n}\n\nimpl std::fmt::Display for WorkerState {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            WorkerState::Creating =\u003e write!(f, \"Creating\"),\n            WorkerState::Starting =\u003e write!(f, \"Starting\"),\n            WorkerState::Running =\u003e write!(f, \"Running\"),\n            WorkerState::Degraded =\u003e write!(f, \"Degraded\"),\n            WorkerState::Recovering =\u003e write!(f, \"Recovering\"),\n            WorkerState::Stopping =\u003e write!(f, \"Stopping\"),\n            WorkerState::Stopped =\u003e write!(f, \"Stopped\"),\n            WorkerState::Failed =\u003e write!(f, \"Failed\"),\n        }\n    }\n}\n\n/// Resource usage metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ResourceUsage {\n    pub cpu_cores: f64,\n    pub memory_bytes: u64,\n    pub disk_bytes: u64,\n}\n\n/// Provider capabilities\n#[derive(Debug, Clone, Default)]\npub struct ProviderCapabilities {\n    pub auto_scaling: bool,\n    pub health_checks: bool,\n    pub volumes: bool,\n    pub config_maps: bool,\n    pub secrets: bool,\n    pub network_policies: bool,\n    pub multi_cluster: bool,\n}\n\nimpl ProviderCapabilities {\n    pub fn new() -\u003e Self {\n        Self::default()\n    }\n}\n\n/// Worker metadata for tracking\n#[derive(Debug, Clone, Default)]\npub struct WorkerMetadata {\n    pub created_at: Option\u003cSystemTime\u003e,\n    pub labels: HashMap\u003cString, String\u003e,\n}\n\nimpl WorkerMetadata {\n    pub fn new() -\u003e Self {\n        Self::default()\n    }\n\n    pub fn with_label(mut self, key: String, value: String) -\u003e Self {\n        self.labels.insert(key, value);\n        self\n    }\n}\n\n/// Handle to an existing worker\n#[derive(Debug, Clone)]\npub struct WorkerHandle {\n    pub worker_id: WorkerId,\n    pub provider_type: ProviderType,\n    pub metadata: WorkerMetadata,\n}\n\nimpl WorkerHandle {\n    pub fn new(worker_id: WorkerId, provider_type: ProviderType) -\u003e Self {\n        Self {\n            worker_id,\n            provider_type,\n            metadata: WorkerMetadata::new(),\n        }\n    }\n}\n\n/// Core error types for worker provider abstraction\n#[derive(thiserror::Error, Debug)]\npub enum ProviderError {\n    #[error(\"provider not initialized: {0}\")]\n    NotInitialized(String),\n\n    #[error(\"provider initialization failed: {0}\")]\n    InitializationFailed(String),\n\n    #[error(\"worker operation failed: {0}\")]\n    WorkerOperationFailed(String),\n\n    #[error(\"capability not supported: {0}\")]\n    CapabilityNotSupported(String),\n\n    #[error(\"configuration error: {0}\")]\n    ConfigurationError(String),\n\n    #[error(\"provider unavailable: {0}\")]\n    ProviderUnavailable(String),\n\n    #[error(\"provider operation timeout\")]\n    Timeout,\n\n    #[error(\"internal error: {0}\")]\n    Internal(#[from] anyhow::Error),\n}\n\n/// Mock provider implementation for testing\npub struct MockWorkerProvider {\n    provider_type: ProviderType,\n    workers: dashmap::DashMap\u003cWorkerId, WorkerStatus\u003e,\n}\n\nimpl MockWorkerProvider {\n    pub fn new(provider_type: ProviderType) -\u003e Self {\n        Self {\n            provider_type,\n            workers: dashmap::DashMap::new(),\n        }\n    }\n}\n\n#[async_trait]\nimpl WorkerProvider for MockWorkerProvider {\n    async fn create_worker(\u0026self, config: \u0026WorkerConfig) -\u003e Result\u003cWorkerHandle, ProviderError\u003e {\n        let status = WorkerStatus {\n            worker_id: config.worker_id.clone(),\n            state: WorkerState::Creating,\n            started_at: None,\n            last_health_check: None,\n            resource_usage: ResourceUsage {\n                cpu_cores: 0.0,\n                memory_bytes: 0,\n                disk_bytes: 0,\n            },\n        };\n\n        self.workers.insert(config.worker_id.clone(), status);\n\n        Ok(WorkerHandle::new(\n            config.worker_id.clone(),\n            self.provider_type.clone(),\n        ))\n    }\n\n    async fn start_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        if let Some(mut status) = self.workers.get_mut(worker_id) {\n            status.state = WorkerState::Starting;\n        }\n        Ok(())\n    }\n\n    async fn stop_worker(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        _graceful: bool,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        if let Some(mut status) = self.workers.get_mut(worker_id) {\n            status.state = WorkerState::Stopping;\n        }\n        Ok(())\n    }\n\n    async fn delete_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        self.workers.remove(worker_id);\n        Ok(())\n    }\n\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, ProviderError\u003e {\n        self.workers\n            .get(worker_id)\n            .map(|status| status.clone())\n            .ok_or_else(|| {\n                ProviderError::WorkerOperationFailed(format!(\"Worker {:?} not found\", worker_id))\n            })\n    }\n\n    async fn get_capabilities(\u0026self) -\u003e Result\u003cProviderCapabilities, ProviderError\u003e {\n        Ok(ProviderCapabilities::new())\n    }\n\n    async fn scale_workers(\n        \u0026self,\n        _worker_type: \u0026str,\n        _target_count: usize,\n    ) -\u003e Result\u003cVec\u003cWorkerId\u003e, ProviderError\u003e {\n        Ok(Vec::new())\n    }\n\n    fn get_provider_type(\u0026self) -\u003e ProviderType {\n        self.provider_type.clone()\n    }\n}\n","traces":[{"line":24,"address":[2298720],"length":1,"stats":{"Line":1}},{"line":25,"address":[2298752],"length":1,"stats":{"Line":1}},{"line":26,"address":[2298811],"length":1,"stats":{"Line":1}},{"line":27,"address":[2298855],"length":1,"stats":{"Line":1}},{"line":28,"address":[2298899],"length":1,"stats":{"Line":1}},{"line":29,"address":[2298949],"length":1,"stats":{"Line":1}},{"line":30,"address":[2299000],"length":1,"stats":{"Line":1}},{"line":66,"address":[2299136],"length":1,"stats":{"Line":1}},{"line":67,"address":[2299174],"length":1,"stats":{"Line":1}},{"line":68,"address":[2299237],"length":1,"stats":{"Line":1}},{"line":69,"address":[2299209],"length":1,"stats":{"Line":1}},{"line":73,"address":[2299192],"length":1,"stats":{"Line":1}},{"line":74,"address":[2299355],"length":1,"stats":{"Line":1}},{"line":75,"address":[2299327],"length":1,"stats":{"Line":1}},{"line":79,"address":[2299318],"length":1,"stats":{"Line":1}},{"line":140,"address":[2299456],"length":1,"stats":{"Line":1}},{"line":141,"address":[2299483],"length":1,"stats":{"Line":1}},{"line":142,"address":[2299514],"length":1,"stats":{"Line":1}},{"line":143,"address":[2299557],"length":1,"stats":{"Line":0}},{"line":144,"address":[2299600],"length":1,"stats":{"Line":1}},{"line":145,"address":[2299643],"length":1,"stats":{"Line":0}},{"line":146,"address":[2299692],"length":1,"stats":{"Line":0}},{"line":147,"address":[2299741],"length":1,"stats":{"Line":0}},{"line":148,"address":[2299787],"length":1,"stats":{"Line":0}},{"line":149,"address":[2299833],"length":1,"stats":{"Line":1}},{"line":175,"address":[2299904],"length":1,"stats":{"Line":1}},{"line":176,"address":[2299908],"length":1,"stats":{"Line":1}},{"line":188,"address":[2300000],"length":1,"stats":{"Line":1}},{"line":189,"address":[2300008],"length":1,"stats":{"Line":1}},{"line":192,"address":[2300137,2300032],"length":1,"stats":{"Line":1}},{"line":193,"address":[2300056,2300091],"length":1,"stats":{"Line":2}},{"line":194,"address":[2300117],"length":1,"stats":{"Line":1}},{"line":207,"address":[2300160,2300325],"length":1,"stats":{"Line":1}},{"line":211,"address":[2300204],"length":1,"stats":{"Line":1}},{"line":251,"address":[2300352,2300481],"length":1,"stats":{"Line":2}},{"line":254,"address":[2300387],"length":1,"stats":{"Line":3}},{"line":261,"address":[2361360,2360900,2360893,2360704,2360742,2360822,2360947,2360992],"length":1,"stats":{"Line":6}},{"line":263,"address":[2360916],"length":1,"stats":{"Line":1}},{"line":267,"address":[2361041],"length":1,"stats":{"Line":1}},{"line":274,"address":[2361155],"length":1,"stats":{"Line":1}},{"line":276,"address":[2361301],"length":1,"stats":{"Line":1}},{"line":277,"address":[2361231],"length":1,"stats":{"Line":1}},{"line":278,"address":[2361270],"length":1,"stats":{"Line":1}},{"line":282,"address":[2361495,2361609,2361665,2361729,2361914,2361392,2361616,2361430,2361985],"length":1,"stats":{"Line":0}},{"line":283,"address":[2361627,2361768],"length":1,"stats":{"Line":0}},{"line":284,"address":[2361839,2361963],"length":1,"stats":{"Line":0}},{"line":286,"address":[2361850],"length":1,"stats":{"Line":0}},{"line":289,"address":[2311014],"length":1,"stats":{"Line":5}},{"line":294,"address":[2362277,2362418],"length":1,"stats":{"Line":2}},{"line":295,"address":[2362489,2362613],"length":1,"stats":{"Line":2}},{"line":297,"address":[2362500],"length":1,"stats":{"Line":1}},{"line":300,"address":[2311070],"length":1,"stats":{"Line":6}},{"line":301,"address":[2362905],"length":1,"stats":{"Line":1}},{"line":302,"address":[2363044],"length":1,"stats":{"Line":1}},{"line":305,"address":[2311118],"length":1,"stats":{"Line":6}},{"line":306,"address":[2363323],"length":1,"stats":{"Line":1}},{"line":307,"address":[2363327],"length":1,"stats":{"Line":1}},{"line":308,"address":[2363451,2363577,2363552],"length":1,"stats":{"Line":3}},{"line":309,"address":[2363680,2363463],"length":1,"stats":{"Line":2}},{"line":310,"address":[2363704],"length":1,"stats":{"Line":1}},{"line":314,"address":[2311161],"length":1,"stats":{"Line":6}},{"line":315,"address":[2364091,2364226],"length":1,"stats":{"Line":2}},{"line":318,"address":[2311224],"length":1,"stats":{"Line":5}},{"line":323,"address":[2364701,2364824],"length":1,"stats":{"Line":2}},{"line":326,"address":[2311264],"length":1,"stats":{"Line":1}},{"line":327,"address":[2311281],"length":1,"stats":{"Line":1}}],"covered":57,"coverable":66},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","traits","mod.rs"],"content":"//! Provider traits module\n//!\n//! This module contains all the trait definitions for worker providers.\n\nuse crate::ProviderError;\nuse crate::models::{ProviderCapabilities, ProviderType, WorkerConfig, WorkerHandle, WorkerStatus};\nuse async_trait::async_trait;\nuse hodei_shared_types::worker_messages::WorkerId;\n\n/// Core worker provider trait\n#[async_trait]\npub trait WorkerProvider: Send + Sync {\n    /// Create a new worker\n    async fn create_worker(\u0026self, config: \u0026WorkerConfig) -\u003e Result\u003cWorkerHandle, ProviderError\u003e;\n\n    /// Start an existing worker\n    async fn start_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e;\n\n    /// Stop a running worker gracefully\n    async fn stop_worker(\u0026self, worker_id: \u0026WorkerId, graceful: bool) -\u003e Result\u003c(), ProviderError\u003e;\n\n    /// Delete a worker completely\n    async fn delete_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e;\n\n    /// Get worker status\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, ProviderError\u003e;\n\n    /// Get provider capabilities\n    async fn get_capabilities(\u0026self) -\u003e Result\u003cProviderCapabilities, ProviderError\u003e;\n\n    /// Scale workers up or down\n    async fn scale_workers(\n        \u0026self,\n        worker_type: \u0026str,\n        target_count: usize,\n    ) -\u003e Result\u003cVec\u003cWorkerId\u003e, ProviderError\u003e;\n\n    /// Get provider type\n    fn get_provider_type(\u0026self) -\u003e ProviderType;\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","affinity","mod.rs"],"content":"//! Affinity Rules and Taints/Tolerations Module\n//!\n//! This module implements Kubernetes-style affinity rules and taints/tolerations\n//! for fine-grained control over job placement decisions.\n\nuse crate::SchedulerError;\nuse crate::types::*;\nuse std::collections::HashMap;\n\n/// Affinity matcher for evaluating affinity rules\npub struct AffinityMatcher;\n\nimpl AffinityMatcher {\n    /// Create new affinity matcher\n    pub fn new() -\u003e Self {\n        Self\n    }\n\n    /// Check if node satisfies required (hard) affinity constraints\n    pub fn check_required_affinity(\u0026self, node: \u0026WorkerNode, affinity: \u0026NodeAffinity) -\u003e bool {\n        // Check required during scheduling constraints\n        for selector in \u0026affinity.required_during_scheduling {\n            if !self.matches_label_selector(node, selector) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node satisfies preferred (soft) affinity constraints\n    pub fn check_preferred_affinity(\u0026self, node: \u0026WorkerNode, affinity: \u0026NodeAffinity) -\u003e f64 {\n        let mut total_weight = 0;\n        let mut matched_weight = 0;\n\n        for weighted_selector in \u0026affinity.preferred_during_scheduling {\n            total_weight += weighted_selector.weight.abs() as u32;\n\n            if self.matches_label_selector(node, \u0026weighted_selector.selector) {\n                matched_weight += weighted_selector.weight.abs() as u32;\n            }\n        }\n\n        if total_weight == 0 {\n            0.0\n        } else {\n            matched_weight as f64 / total_weight as f64\n        }\n    }\n\n    /// Check if node matches a label selector\n    fn matches_label_selector(\u0026self, node: \u0026WorkerNode, selector: \u0026LabelSelector) -\u003e bool {\n        let node_value = node.labels.get(\u0026selector.key);\n\n        match selector.operator {\n            LabelSelectorOperator::In =\u003e {\n                if let Some(values) = \u0026selector.values {\n                    if let Some(node_val) = node_value {\n                        values.contains(node_val)\n                    } else {\n                        false\n                    }\n                } else {\n                    false\n                }\n            }\n            LabelSelectorOperator::NotIn =\u003e {\n                if let Some(values) = \u0026selector.values {\n                    if let Some(node_val) = node_value {\n                        !values.contains(node_val)\n                    } else {\n                        true // Node doesn't have the key, so it's NOT in the values\n                    }\n                } else {\n                    false\n                }\n            }\n            LabelSelectorOperator::Exists =\u003e node_value.is_some(),\n            LabelSelectorOperator::DoesNotExist =\u003e node_value.is_none(),\n        }\n    }\n\n    /// Check if node selector is satisfied\n    pub fn check_node_selector(\u0026self, node: \u0026WorkerNode, selector: \u0026NodeSelector) -\u003e bool {\n        for (key, value) in \u0026selector.labels {\n            if node.labels.get(key) != Some(value) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node has sufficient tolerations for its taints\n    pub fn check_taints_tolerations(\u0026self, node: \u0026WorkerNode, tolerations: \u0026[Toleration]) -\u003e bool {\n        for taint in \u0026node.taints {\n            let mut has_matching_toleration = false;\n\n            for toleration in tolerations {\n                if self.matches_taint_toleration(taint, toleration) {\n                    has_matching_toleration = true;\n                    break;\n                }\n            }\n\n            // If no matching toleration and effect is NoSchedule, reject\n            if !has_matching_toleration \u0026\u0026 matches!(taint.effect, TaintEffect::NoSchedule) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if a taint matches a toleration\n    fn matches_taint_toleration(\u0026self, taint: \u0026Taint, toleration: \u0026Toleration) -\u003e bool {\n        // Check key\n        if taint.key != toleration.key {\n            return false;\n        }\n\n        // Check effect\n        if taint.effect != toleration.effect {\n            return false;\n        }\n\n        // Check operator and value\n        match (\u0026taint.operator, \u0026toleration.operator) {\n            (TaintOperator::Equal, TaintOperator::Equal) =\u003e taint.value == toleration.value,\n            (TaintOperator::NotEqual, TaintOperator::NotEqual) =\u003e taint.value != toleration.value,\n            (TaintOperator::Exists, TaintOperator::Exists) =\u003e {\n                // Just checking if the key exists\n                true\n            }\n            _ =\u003e false,\n        }\n    }\n\n    /// Evaluate all affinity rules for a job\n    pub fn evaluate_affinity(\n        \u0026self,\n        job: \u0026Job,\n        node: \u0026WorkerNode,\n    ) -\u003e Result\u003cAffinityResult, SchedulerError\u003e {\n        let mut can_schedule = true;\n        let mut affinity_score = 0.0;\n\n        // Check node selector\n        if let Some(selector) = \u0026job.spec.node_selector {\n            if !self.check_node_selector(node, selector) {\n                can_schedule = false;\n            }\n        }\n\n        // Check node affinity\n        if let Some(affinity) = \u0026job.spec.affinity {\n            if !self.check_required_affinity(node, affinity) {\n                can_schedule = false;\n            }\n\n            // Calculate preferred affinity score\n            affinity_score = self.check_preferred_affinity(node, affinity);\n        }\n\n        // Check taints and tolerations\n        if !self.check_taints_tolerations(node, \u0026job.spec.tolerations) {\n            can_schedule = false;\n        }\n\n        Ok(AffinityResult {\n            can_schedule,\n            affinity_score,\n        })\n    }\n}\n\n/// Result of affinity evaluation\n#[derive(Debug, Clone)]\npub struct AffinityResult {\n    pub can_schedule: bool,\n    pub affinity_score: f64, // 0.0 to 1.0, where 1.0 is perfect match\n}\n\n/// Helper to check if job can run on a specific node\npub fn can_schedule_on_node(job: \u0026Job, node: \u0026WorkerNode) -\u003e Result\u003cbool, SchedulerError\u003e {\n    let matcher = AffinityMatcher::new();\n    let result = matcher.evaluate_affinity(job, node)?;\n    Ok(result.can_schedule)\n}\n\n/// Helper to calculate affinity score for a node\npub fn calculate_affinity_score(job: \u0026Job, node: \u0026WorkerNode) -\u003e Result\u003cf64, SchedulerError\u003e {\n    let matcher = AffinityMatcher::new();\n    let result = matcher.evaluate_affinity(job, node)?;\n    Ok(result.affinity_score)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::ComputeResource;\n    use crate::types::KubernetesNodeSpecific;\n\n    fn create_test_node(labels: HashMap\u003cString, String\u003e) -\u003e WorkerNode {\n        WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: BackendType::Kubernetes,\n            status: WorkerStatus::Running,\n            resources: ComputeResource {\n                cpu_cores: 8.0,\n                memory_bytes: 8_000_000_000,\n                gpu_count: 0,\n            },\n            labels,\n            taints: vec![],\n            backend_specific: BackendSpecific::Kubernetes(KubernetesNodeSpecific {\n                node_name: \"test-node\".to_string(),\n                namespace: \"default\".to_string(),\n            }),\n            location: NodeLocation::default(),\n        }\n    }\n\n    fn create_test_job_with_selector(selector_labels: HashMap\u003cString, String\u003e) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(2_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: JobPriority::Medium,\n                node_selector: Some(NodeSelector {\n                    labels: selector_labels,\n                }),\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_node_selector_match() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n        node_labels.insert(\"arch\".to_string(), \"amd64\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let mut job_selector = HashMap::new();\n        job_selector.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n\n        let job = create_test_job_with_selector(job_selector);\n\n        let can_schedule = can_schedule_on_node(\u0026job, \u0026node).unwrap();\n\n        assert!(can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_node_selector_no_match() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"zone\".to_string(), \"us-west-2\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let mut job_selector = HashMap::new();\n        job_selector.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n\n        let job = create_test_job_with_selector(job_selector);\n\n        let can_schedule = can_schedule_on_node(\u0026job, \u0026node).unwrap();\n\n        assert!(!can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_required_affinity() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"gpu\".to_string(), \"true\".to_string());\n        node_labels.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let required_affinity = NodeAffinity {\n            required_during_scheduling: vec![LabelSelector {\n                key: \"gpu\".to_string(),\n                operator: LabelSelectorOperator::Exists,\n                values: None,\n            }],\n            preferred_during_scheduling: vec![],\n        };\n\n        let mut job_selector = HashMap::new();\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(2_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: JobPriority::Medium,\n                node_selector: Some(NodeSelector {\n                    labels: job_selector,\n                }),\n                affinity: Some(required_affinity),\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let can_schedule = can_schedule_on_node(\u0026job, \u0026node).unwrap();\n\n        assert!(can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_taints_tolerations() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node = create_test_node(HashMap::new());\n        node.taints = vec![Taint {\n            key: \"dedicated\".to_string(),\n            operator: TaintOperator::Equal,\n            value: \"gpu\".to_string(),\n            effect: TaintEffect::NoSchedule,\n        }];\n\n        let tolerations = vec![Toleration {\n            key: \"dedicated\".to_string(),\n            operator: TaintOperator::Equal,\n            value: \"gpu\".to_string(),\n            effect: TaintEffect::NoSchedule,\n            toleration_seconds: None,\n        }];\n\n        let can_schedule = matcher.check_taints_tolerations(\u0026node, \u0026tolerations);\n\n        assert!(can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_no_toleration_for_noschedule_taint() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node = create_test_node(HashMap::new());\n        node.taints = vec![Taint {\n            key: \"dedicated\".to_string(),\n            operator: TaintOperator::Equal,\n            value: \"gpu\".to_string(),\n            effect: TaintEffect::NoSchedule,\n        }];\n\n        // No tolerations provided\n        let tolerations = vec![];\n\n        let can_schedule = matcher.check_taints_tolerations(\u0026node, \u0026tolerations);\n\n        assert!(!can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_affinity_score_calculation() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n        node_labels.insert(\"type\".to_string(), \"compute\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let affinity = NodeAffinity {\n            required_during_scheduling: vec![],\n            preferred_during_scheduling: vec![\n                WeightedLabelSelector {\n                    selector: LabelSelector {\n                        key: \"zone\".to_string(),\n                        operator: LabelSelectorOperator::In,\n                        values: Some(vec![\"us-east-1\".to_string()]),\n                    },\n                    weight: 50,\n                },\n                WeightedLabelSelector {\n                    selector: LabelSelector {\n                        key: \"type\".to_string(),\n                        operator: LabelSelectorOperator::In,\n                        values: Some(vec![\"compute\".to_string()]),\n                    },\n                    weight: 50,\n                },\n            ],\n        };\n\n        let score = matcher.check_preferred_affinity(\u0026node, \u0026affinity);\n\n        assert_eq!(score, 1.0); // Perfect match\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","backend","mod.rs"],"content":"//! Scheduler Backend Abstraction Module\n//!\n//! This module provides the backend abstraction layer for supporting multiple\n//! execution backends (Kubernetes, Docker, Cloud VMs, etc.). Each backend\n//! implements the SchedulerBackend trait to provide uniform interface.\n\nuse crate::types::*;\nuse crate::{SchedulerError, WorkerId, JobId};\nuse std::sync::Arc;\nuse async_trait::async_trait;\n\n/// Scheduler backend trait for multi-backend support\n#[async_trait]\npub trait SchedulerBackend: Send + Sync {\n    /// Get backend type\n    fn backend_type(\u0026self) -\u003e BackendType;\n\n    /// List all available nodes\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e;\n\n    /// Get specific node by ID\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e;\n\n    /// Bind job to node\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e;\n\n    /// Unbind job from node\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e;\n\n    /// Get node status\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e;\n}\n\n/// Compute resources of a worker node\n#[derive(Debug, Clone, PartialEq)]\npub struct ComputeResource {\n    pub cpu_cores: f64,\n    pub memory_bytes: u64,\n    pub gpu_count: u32,\n}\n\nimpl ComputeResource {\n    /// Check if node has enough resources\n    pub fn has_resources(\u0026self, requirements: \u0026ResourceRequirements) -\u003e bool {\n        if let Some(cpu) = requirements.cpu_cores {\n            if self.cpu_cores \u003c cpu {\n                return false;\n            }\n        }\n\n        if let Some(memory) = requirements.memory_bytes {\n            if self.memory_bytes \u003c memory {\n                return false;\n            }\n        }\n\n        if let Some(gpu) = requirements.gpu_count {\n            if self.gpu_count \u003c gpu {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Calculate utilization percentage (0.0 to 1.0)\n    pub fn utilization(\u0026self, requirements: \u0026ResourceRequirements) -\u003e f64 {\n        let mut max_util: f64 = 0.0;\n\n        if let Some(cpu) = requirements.cpu_cores {\n            max_util = max_util.max(cpu / self.cpu_cores);\n        }\n\n        if let Some(memory) = requirements.memory_bytes {\n            let mem_util = memory as f64 / self.memory_bytes as f64;\n            max_util = max_util.max(mem_util);\n        }\n\n        if let Some(gpu) = requirements.gpu_count {\n            if self.gpu_count \u003e 0 {\n                let gpu_util = gpu as f64 / self.gpu_count as f64;\n                max_util = max_util.max(gpu_util);\n            }\n        }\n\n        max_util.min(1.0)\n    }\n}\n\n/// Kubernetes backend adapter\npub struct KubernetesBackend {\n    // In production: would have Kubernetes client/API connector\n}\n\nimpl KubernetesBackend {\n    pub fn new() -\u003e Self {\n        Self {}\n    }\n}\n\n#[async_trait]\nimpl SchedulerBackend for KubernetesBackend {\n    fn backend_type(\u0026self) -\u003e BackendType {\n        BackendType::Kubernetes\n    }\n\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        // TODO: Connect to Kubernetes API\n        // For now: return empty list\n        Ok(vec![])\n    }\n\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n        // TODO: Implement Kubernetes API call\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        // TODO: Implement Kubernetes Pod binding\n        tracing::info!(\"Binding job {} to Kubernetes node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Unbinding job {} from Kubernetes node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e {\n        // TODO: Get status from Kubernetes API\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n}\n\n/// Docker backend adapter\npub struct DockerBackend {\n    // In production: would have Docker client connector\n}\n\nimpl DockerBackend {\n    pub fn new() -\u003e Self {\n        Self {}\n    }\n}\n\n#[async_trait]\nimpl SchedulerBackend for DockerBackend {\n    fn backend_type(\u0026self) -\u003e BackendType {\n        BackendType::Docker\n    }\n\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        // TODO: Connect to Docker API\n        Ok(vec![])\n    }\n\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Binding job {} to Docker node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Unbinding job {} from Docker node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n}\n\n/// Cloud VM backend adapter\npub struct CloudVmBackend {\n    // In production: would have cloud provider client (AWS, Azure, GCP)\n}\n\nimpl CloudVmBackend {\n    pub fn new() -\u003e Self {\n        Self {}\n    }\n}\n\n#[async_trait]\nimpl SchedulerBackend for CloudVmBackend {\n    fn backend_type(\u0026self) -\u003e BackendType {\n        BackendType::CloudVm\n    }\n\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        // TODO: Connect to cloud provider API\n        Ok(vec![])\n    }\n\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Binding job {} to Cloud VM {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Unbinding job {} from Cloud VM {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n}\n\n/// Backend registry for managing multiple backends\npub struct BackendRegistry {\n    backends: dashmap::DashMap\u003cBackendType, Arc\u003cdyn SchedulerBackend\u003e\u003e,\n}\n\nimpl BackendRegistry {\n    pub fn new() -\u003e Self {\n        Self {\n            backends: dashmap::DashMap::new(),\n        }\n    }\n\n    /// Register backend\n    pub fn register(\u0026self, backend: Arc\u003cdyn SchedulerBackend\u003e) {\n        let backend_type = backend.backend_type();\n        self.backends.insert(backend_type, backend);\n    }\n\n    /// Get backend by type\n    pub fn get(\u0026self, backend_type: BackendType) -\u003e Option\u003cArc\u003cdyn SchedulerBackend\u003e\u003e {\n        self.backends.get(\u0026backend_type).map(|entry| entry.clone())\n    }\n\n    /// List all registered backend types\n    pub fn list_types(\u0026self) -\u003e Vec\u003cBackendType\u003e {\n        self.backends.iter().map(|entry| entry.key().clone()).collect()\n    }\n}\n\nimpl Default for BackendRegistry {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::*;\n\n    #[test]\n    fn test_compute_resource_has_resources() {\n        let resource = ComputeResource {\n            cpu_cores: 8.0,\n            memory_bytes: 16_000_000_000,\n            gpu_count: 2,\n        };\n\n        let requirements = ResourceRequirements {\n            cpu_cores: Some(4.0),\n            memory_bytes: Some(8_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(resource.has_resources(\u0026requirements));\n\n        // Insufficient resources\n        let insufficient_requirements = ResourceRequirements {\n            cpu_cores: Some(16.0),\n            memory_bytes: Some(8_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(!resource.has_resources(\u0026insufficient_requirements));\n    }\n\n    #[test]\n    fn test_compute_resource_utilization() {\n        let resource = ComputeResource {\n            cpu_cores: 8.0,\n            memory_bytes: 16_000_000_000,\n            gpu_count: 2,\n        };\n\n        let requirements = ResourceRequirements {\n            cpu_cores: Some(4.0),\n            memory_bytes: Some(8_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        let utilization = resource.utilization(\u0026requirements);\n        assert!(utilization \u003e 0.0 \u0026\u0026 utilization \u003c= 1.0);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","integration","mod.rs"],"content":"//! Scheduler and Worker Lifecycle Integration (US-012)\n//!\n//! This module provides integration between the scheduler framework and the worker\n//! lifecycle management system, enabling:\n//! - Real-time worker state synchronization with scheduler\n//! - Event-driven scheduling based on worker lifecycle events\n//! - Automatic job rescheduling on worker failures\n//! - Coordinated preemption and job cleanup\n//! - Health monitoring and recovery\n\nuse crate::backend::SchedulerBackend;\nuse crate::types::{Job, WorkerNode, WorkerStatus};\nuse crate::{JobId, WorkerId};\nuse hodei_worker_lifecycle::WorkerManager;\nuse parking_lot::Mutex;\nuse std::collections::{HashMap, HashSet};\nuse std::sync::Arc;\nuse tokio::sync::broadcast;\nuse tracing::{error, info, warn};\n\n/// Integration coordinator between scheduler and worker lifecycle\npub struct SchedulerWorkerIntegration {\n    scheduler_backend: Arc\u003cdyn SchedulerBackend\u003e,\n    worker_manager: Arc\u003cWorkerManager\u003e,\n    event_handlers: Arc\u003cMutex\u003cVec\u003cBox\u003cdyn EventHandler\u003e\u003e\u003e\u003e,\n    job_to_worker_map: Arc\u003cMutex\u003cHashMap\u003cJobId, WorkerId\u003e\u003e\u003e,\n    shutdown_tx: broadcast::Sender\u003c()\u003e,\n}\n\nimpl SchedulerWorkerIntegration {\n    /// Create new integration coordinator\n    pub fn new(\n        scheduler_backend: Arc\u003cdyn SchedulerBackend\u003e,\n        worker_manager: Arc\u003cWorkerManager\u003e,\n    ) -\u003e Self {\n        let (shutdown_tx, _) = broadcast::channel(1);\n\n        Self {\n            scheduler_backend,\n            worker_manager,\n            event_handlers: Arc::new(Mutex::new(Vec::new())),\n            job_to_worker_map: Arc::new(Mutex::new(HashMap::new())),\n            shutdown_tx,\n        }\n    }\n\n    /// Start the integration coordinator\n    pub async fn start(\u0026self) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Starting scheduler-worker integration\");\n\n        // Start monitoring worker lifecycle events\n        self.spawn_event_monitoring().await;\n\n        // Register event handlers\n        self.register_default_handlers().await;\n\n        info!(\"Scheduler-worker integration started successfully\");\n\n        Ok(())\n    }\n\n    /// Stop the integration coordinator\n    pub async fn stop(\u0026self) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Stopping scheduler-worker integration\");\n\n        // Notify all tasks to shutdown\n        let _ = self.shutdown_tx.send(());\n\n        info!(\"Scheduler-worker integration stopped\");\n\n        Ok(())\n    }\n\n    /// Handle worker registration event\n    pub async fn handle_worker_registered(\n        \u0026self,\n        worker_id: WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Worker registered event: {}\", worker_id);\n\n        // Update backend with worker info\n        let worker_info = self\n            .worker_manager\n            .get_worker_status(worker_id)\n            .map_err(|e| IntegrationError::WorkerLifecycleError(e.to_string()))?;\n\n        // Trigger rescheduling of pending jobs if we now have capacity\n        self.trigger_pending_job_retry().await;\n\n        Ok(())\n    }\n\n    /// Handle worker heartbeat event\n    pub async fn handle_worker_heartbeat(\n        \u0026self,\n        worker_id: WorkerId,\n        load: f64,\n        jobs_running: usize,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        // Update worker manager with heartbeat\n        self.worker_manager\n            .handle_heartbeat(worker_id, load, jobs_running)\n            .map_err(|e| IntegrationError::WorkerLifecycleError(e.to_string()))?;\n\n        // Check if we need to reschedule jobs based on load\n        if load \u003e 0.9 {\n            warn!(\"Worker {} has high load ({:.2}%)\", worker_id, load * 100.0);\n            // Could trigger load balancing here\n        }\n\n        Ok(())\n    }\n\n    /// Handle worker failure event\n    pub async fn handle_worker_failed(\u0026self, worker_id: WorkerId) -\u003e Result\u003c(), IntegrationError\u003e {\n        error!(\"Worker failed event: {}\", worker_id);\n\n        // Get jobs running on this worker\n        let jobs_to_reschedule = {\n            let map = self.job_to_worker_map.lock();\n            map.iter()\n                .filter_map(|(job_id, w_id)| {\n                    if *w_id == worker_id {\n                        Some(*job_id)\n                    } else {\n                        None\n                    }\n                })\n                .collect::\u003cVec\u003c_\u003e\u003e()\n        };\n\n        // Remove worker from job mapping\n        {\n            let mut map = self.job_to_worker_map.lock();\n            map.retain(|_, w_id| *w_id != worker_id);\n        }\n\n        // Reschedule jobs on different workers\n        for job_id in jobs_to_reschedule {\n            info!(\n                \"Rescheduling job {} from failed worker {}\",\n                job_id, worker_id\n            );\n            // In production: get job details from scheduler queue and reschedule\n            // For now: just log the event\n        }\n\n        Ok(())\n    }\n\n    /// Handle worker deregistration event\n    pub async fn handle_worker_deregistered(\n        \u0026self,\n        worker_id: WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Worker deregistered event: {}\", worker_id);\n\n        // Handle graceful shutdown\n        self.handle_worker_failed(worker_id).await?;\n\n        Ok(())\n    }\n\n    /// Notify scheduler that job was bound to worker\n    pub fn notify_job_bound(\u0026self, job_id: JobId, worker_id: WorkerId) {\n        let mut map = self.job_to_worker_map.lock();\n        map.insert(job_id, worker_id);\n        info!(\"Job {} bound to worker {}\", job_id, worker_id);\n    }\n\n    /// Notify scheduler that job completed on worker\n    pub fn notify_job_completed(\u0026self, job_id: JobId, worker_id: WorkerId) {\n        let mut map = self.job_to_worker_map.lock();\n        map.remove(\u0026job_id);\n        info!(\"Job {} completed on worker {}\", job_id, worker_id);\n    }\n\n    /// Get worker status from lifecycle manager\n    pub async fn get_worker_status(\n        \u0026self,\n        worker_id: WorkerId,\n    ) -\u003e Result\u003cWorkerStatusView, IntegrationError\u003e {\n        let status = self\n            .worker_manager\n            .get_worker_status(worker_id)\n            .map_err(|e| IntegrationError::WorkerLifecycleError(e.to_string()))?;\n\n        // Map to scheduler view\n        let scheduler_status = match status.state.as_str() {\n            \"AVAILABLE\" =\u003e WorkerStatus::Ready,\n            \"RUNNING\" =\u003e WorkerStatus::Running,\n            \"UNHEALTHY\" =\u003e WorkerStatus::Failed,\n            \"DRAINING\" =\u003e WorkerStatus::Draining,\n            _ =\u003e WorkerStatus::Offline,\n        };\n\n        Ok(WorkerStatusView {\n            worker_id,\n            state: scheduler_status,\n            load: status.load,\n            jobs_running: status.jobs_running,\n            max_jobs: status.max_jobs,\n            has_gpu: status.capabilities.has_gpu,\n            backend_type: self.scheduler_backend.backend_type(),\n        })\n    }\n\n    /// Get all workers summary\n    pub async fn get_workers_summary(\u0026self) -\u003e Vec\u003cWorkerSummary\u003e {\n        self.worker_manager\n            .get_workers_summary()\n            .into_iter()\n            .map(|w| WorkerSummary {\n                worker_id: w.worker_id,\n                state: w.state,\n                load: w.load,\n                jobs_running: w.jobs_running,\n                has_gpu: w.has_gpu,\n            })\n            .collect()\n    }\n\n    /// Start background monitoring tasks\n    async fn spawn_event_monitoring(\u0026self) {\n        let worker_manager = Arc::clone(\u0026self.worker_manager);\n        let scheduler_backend = Arc::clone(\u0026self.scheduler_backend);\n        let shutdown_rx = self.shutdown_tx.subscribe();\n\n        // Monitor worker health and sync with backend\n        tokio::spawn(async move {\n            let mut shutdown_rx = shutdown_rx;\n\n            loop {\n                tokio::select! {\n                    _ = tokio::time::sleep(tokio::time::Duration::from_secs(5)) =\u003e {\n                        // Periodic health check and sync\n                        let workers = worker_manager.get_workers_summary();\n\n                        // Sync with backend (simplified - in production would do incremental sync)\n                        for worker_summary in workers {\n                            let backend_status = match worker_summary.state.as_str() {\n                                \"AVAILABLE\" =\u003e WorkerStatus::Ready,\n                                \"RUNNING\" =\u003e WorkerStatus::Running,\n                                \"DRAINING\" =\u003e WorkerStatus::Draining,\n                                _ =\u003e WorkerStatus::Offline,\n                            };\n\n                            // Update backend if needed (in production: only if changed)\n                            // scheduler_backend.update_worker_status(worker_summary.worker_id, backend_status).await;\n                        }\n                    }\n                    _ = shutdown_rx.recv() =\u003e {\n                        break;\n                    }\n                }\n            }\n        });\n    }\n\n    /// Register default event handlers\n    async fn register_default_handlers(\u0026self) {\n        let mut handlers = self.event_handlers.lock();\n        handlers.push(Box::new(LoadBalancingHandler::new()));\n        handlers.push(Box::new(PreemptionHandler::new()));\n        handlers.push(Box::new(MetricsHandler::new()));\n    }\n\n    /// Trigger retry of pending jobs\n    async fn trigger_pending_job_retry(\u0026self) {\n        info!(\"Triggering pending job retry due to worker availability change\");\n        // In production: would notify scheduler to re-evaluate pending jobs\n    }\n\n    /// Find workers for job using both scheduler and lifecycle manager\n    pub async fn find_suitable_workers(\n        \u0026self,\n        job: \u0026Job,\n    ) -\u003e Result\u003cVec\u003cWorkerNode\u003e, IntegrationError\u003e {\n        // Get workers from backend\n        let all_workers = self.scheduler_backend.list_nodes().await?;\n\n        // Filter using lifecycle manager state\n        let lifecycle_workers = self.worker_manager.get_workers_summary();\n        let lifecycle_worker_ids: std::collections::HashSet\u003c_\u003e =\n            lifecycle_workers.iter().map(|w| w.worker_id).collect();\n\n        let suitable_workers: Vec\u003cWorkerNode\u003e = all_workers\n            .into_iter()\n            .filter(|worker| {\n                // Check if worker is in lifecycle manager\n                lifecycle_worker_ids.contains(\u0026worker.id)\n                    \u0026\u0026 worker.status == WorkerStatus::Ready\n                    \u0026\u0026 worker.matches_requirements(job)\n            })\n            .collect();\n\n        Ok(suitable_workers)\n    }\n\n    /// Handle job binding to worker\n    pub async fn bind_job_to_worker(\n        \u0026self,\n        job_id: \u0026JobId,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        // Record binding\n        self.notify_job_bound(*job_id, *worker_id);\n\n        // Notify backend\n        self.scheduler_backend.bind_job(job_id, worker_id).await?;\n\n        // Update worker state if needed\n        // Note: Worker state is managed by WorkerManager, not scheduler\n\n        Ok(())\n    }\n\n    /// Handle job unbinding from worker\n    pub async fn unbind_job_from_worker(\n        \u0026self,\n        job_id: \u0026JobId,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        // Remove binding\n        {\n            let mut map = self.job_to_worker_map.lock();\n            map.remove(job_id);\n        }\n\n        // Notify backend\n        self.scheduler_backend.unbind_job(job_id, worker_id).await?;\n\n        Ok(())\n    }\n\n    /// Get job to worker mapping\n    pub fn get_job_to_worker_map(\u0026self) -\u003e HashMap\u003cJobId, WorkerId\u003e {\n        let map = self.job_to_worker_map.lock();\n        map.clone()\n    }\n\n    /// Check if job is currently bound to a worker\n    pub fn is_job_bound(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        let map = self.job_to_worker_map.lock();\n        map.contains_key(job_id)\n    }\n\n    /// Get worker that job is bound to\n    pub fn get_job_worker(\u0026self, job_id: \u0026JobId) -\u003e Option\u003cWorkerId\u003e {\n        let map = self.job_to_worker_map.lock();\n        map.get(job_id).copied()\n    }\n}\n\n/// Event handler trait for integration events\npub trait EventHandler: Send + Sync {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId);\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId);\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId);\n}\n\n/// Load balancing event handler\nstruct LoadBalancingHandler;\n\nimpl LoadBalancingHandler {\n    fn new() -\u003e Self {\n        Self\n    }\n}\n\nimpl EventHandler for LoadBalancingHandler {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId) {\n        info!(\"[LoadBalancer] New worker registered: {}\", worker_id);\n    }\n\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId) {\n        error!(\"[LoadBalancer] Worker failed: {}\", worker_id);\n    }\n\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId) {\n        info!(\"[LoadBalancer] Worker deregistered: {}\", worker_id);\n    }\n}\n\n/// Preemption event handler\nstruct PreemptionHandler;\n\nimpl PreemptionHandler {\n    fn new() -\u003e Self {\n        Self\n    }\n}\n\nimpl EventHandler for PreemptionHandler {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Preemption] Worker registered: {}\", worker_id);\n    }\n\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId) {\n        warn!(\"[Preemption] Worker failed: {}\", worker_id);\n    }\n\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Preemption] Worker deregistered: {}\", worker_id);\n    }\n}\n\n/// Metrics event handler\nstruct MetricsHandler;\n\nimpl MetricsHandler {\n    fn new() -\u003e Self {\n        Self\n    }\n}\n\nimpl EventHandler for MetricsHandler {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Metrics] Worker registered: {}\", worker_id);\n    }\n\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId) {\n        warn!(\"[Metrics] Worker failed: {}\", worker_id);\n    }\n\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Metrics] Worker deregistered: {}\", worker_id);\n    }\n}\n\n/// Worker status view for integration\n#[derive(Debug, Clone)]\npub struct WorkerStatusView {\n    pub worker_id: WorkerId,\n    pub state: WorkerStatus,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub max_jobs: usize,\n    pub has_gpu: bool,\n    pub backend_type: crate::types::BackendType,\n}\n\n/// Worker summary for listing\n#[derive(Debug, Clone)]\npub struct WorkerSummary {\n    pub worker_id: WorkerId,\n    pub state: String,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub has_gpu: bool,\n}\n\n/// Integration error types\n#[derive(thiserror::Error, Debug)]\npub enum IntegrationError {\n    #[error(\"scheduler error: {0}\")]\n    SchedulerError(#[from] crate::SchedulerError),\n\n    #[error(\"worker lifecycle error: {0}\")]\n    WorkerLifecycleError(String),\n\n    #[error(\"worker not found: {0}\")]\n    WorkerNotFound(WorkerId),\n\n    #[error(\"job not bound\")]\n    JobNotBound,\n\n    #[error(\"integration error: {0}\")]\n    Integration(String),\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{ComputeResource, SchedulerBackend};\n    use crate::types::{BackendType, Job, JobMetadata, JobSpec, ResourceRequirements, WorkerNode};\n    use chrono::Utc;\n    use hodei_worker_lifecycle::{Worker, WorkerCapabilities, WorkerManager};\n    use std::collections::HashMap;\n    use tokio::time::Duration;\n\n    /// Mock backend for testing\n    struct MockIntegrationBackend {\n        workers: Vec\u003cWorkerNode\u003e,\n    }\n\n    impl MockIntegrationBackend {\n        fn new() -\u003e Self {\n            Self {\n                workers: vec![\n                    WorkerNode {\n                        id: uuid::Uuid::new_v4(),\n                        backend_type: BackendType::Kubernetes,\n                        status: WorkerStatus::Ready,\n                        resources: ComputeResource {\n                            cpu_cores: 8.0,\n                            memory_bytes: 16_000_000_000,\n                            gpu_count: 0,\n                        },\n                        labels: HashMap::new(),\n                        taints: vec![],\n                        backend_specific: crate::types::BackendSpecific::Kubernetes(\n                            crate::types::KubernetesNodeSpecific {\n                                node_name: \"node-1\".to_string(),\n                                namespace: \"default\".to_string(),\n                            },\n                        ),\n                        location: crate::types::NodeLocation::default(),\n                    },\n                    WorkerNode {\n                        id: uuid::Uuid::new_v4(),\n                        backend_type: BackendType::Kubernetes,\n                        status: WorkerStatus::Ready,\n                        resources: ComputeResource {\n                            cpu_cores: 4.0,\n                            memory_bytes: 8_000_000_000,\n                            gpu_count: 0,\n                        },\n                        labels: HashMap::new(),\n                        taints: vec![],\n                        backend_specific: crate::types::BackendSpecific::Kubernetes(\n                            crate::types::KubernetesNodeSpecific {\n                                node_name: \"node-2\".to_string(),\n                                namespace: \"default\".to_string(),\n                            },\n                        ),\n                        location: crate::types::NodeLocation::default(),\n                    },\n                ],\n            }\n        }\n    }\n\n    #[async_trait::async_trait]\n    impl SchedulerBackend for MockIntegrationBackend {\n        fn backend_type(\u0026self) -\u003e BackendType {\n            BackendType::Kubernetes\n        }\n\n        async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, crate::SchedulerError\u003e {\n            Ok(self.workers.clone())\n        }\n\n        async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, crate::SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .cloned()\n                .ok_or_else(|| crate::SchedulerError::WorkerNotFound(*id))\n        }\n\n        async fn bind_job(\n            \u0026self,\n            _job_id: \u0026JobId,\n            _node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), crate::SchedulerError\u003e {\n            Ok(())\n        }\n\n        async fn unbind_job(\n            \u0026self,\n            _job_id: \u0026JobId,\n            _node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), crate::SchedulerError\u003e {\n            Ok(())\n        }\n\n        async fn get_node_status(\n            \u0026self,\n            id: \u0026WorkerId,\n        ) -\u003e Result\u003cWorkerStatus, crate::SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .map(|w| w.status.clone())\n                .ok_or_else(|| crate::SchedulerError::WorkerNotFound(*id))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_integration_creation() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager);\n\n        assert!(integration.start().await.is_ok());\n        assert!(integration.stop().await.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_worker_registration() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager.clone());\n        integration.start().await.unwrap();\n\n        let worker_id = uuid::Uuid::new_v4();\n\n        // Register worker in lifecycle manager\n        worker_manager\n            .register_worker(Worker::new(\n                worker_id,\n                WorkerCapabilities::default(),\n                10,\n                None,\n            ))\n            .unwrap();\n\n        // Handle registration event\n        assert!(\n            integration\n                .handle_worker_registered(worker_id)\n                .await\n                .is_ok()\n        );\n\n        // Check status\n        let status = integration.get_worker_status(worker_id).await;\n        assert!(status.is_ok());\n\n        integration.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_job_binding_tracking() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager);\n        integration.start().await.unwrap();\n\n        let job_id = uuid::Uuid::new_v4();\n        let worker_id = uuid::Uuid::new_v4();\n\n        // Notify binding\n        integration.notify_job_bound(job_id, worker_id);\n\n        // Check if job is bound\n        assert!(integration.is_job_bound(\u0026job_id));\n        assert_eq!(integration.get_job_worker(\u0026job_id), Some(worker_id));\n\n        // Check map\n        let map = integration.get_job_to_worker_map();\n        assert_eq!(map.get(\u0026job_id), Some(\u0026worker_id));\n\n        // Notify completion\n        integration.notify_job_completed(job_id, worker_id);\n\n        // Check if job is no longer bound\n        assert!(!integration.is_job_bound(\u0026job_id));\n        assert_eq!(integration.get_job_worker(\u0026job_id), None);\n\n        integration.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_find_suitable_workers() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        // Get backend workers before moving backend\n        let backend_workers = backend.list_nodes().await.unwrap();\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager.clone());\n        integration.start().await.unwrap();\n\n        // Register backend workers in the lifecycle manager\n        for worker_node in backend_workers {\n            worker_manager\n                .register_worker(Worker::new(\n                    worker_node.id,\n                    WorkerCapabilities::default(),\n                    10,\n                    None,\n                ))\n                .unwrap();\n        }\n\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(4_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        // Find suitable workers\n        let suitable = integration.find_suitable_workers(\u0026job).await;\n        assert!(suitable.is_ok());\n\n        // Should return all workers as they meet requirements\n        let workers = suitable.unwrap();\n        assert!(!workers.is_empty());\n\n        integration.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_worker_heartbeat() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager.clone());\n        integration.start().await.unwrap();\n\n        let worker_id = uuid::Uuid::new_v4();\n\n        // Register worker\n        worker_manager\n            .register_worker(Worker::new(\n                worker_id,\n                WorkerCapabilities::default(),\n                10,\n                None,\n            ))\n            .unwrap();\n\n        // Send heartbeat\n        assert!(\n            integration\n                .handle_worker_heartbeat(worker_id, 0.5, 2)\n                .await\n                .is_ok()\n        );\n\n        integration.stop().await.unwrap();\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","lib.rs"],"content":"//! Kubernetes-Style Scheduler Framework (US-007)\n//!\n//! This module provides a complete scheduler framework inspired by Kubernetes for the\n//! Hodei Jobs distributed CI/CD system. The scheduler supports multiple execution\n//! backends and implements a 4-phase scheduling pipeline.\n//!\n//! Architecture:\n//! - Backend abstraction layer for multi-backend support\n//! - 4-phase scheduling pipeline: Informer  Filter  Score  Bind\n//! - Priority queue with preemption support\n//! - Node affinity rules and taints/tolerations\n//! - Deterministic scheduling algorithms\n//!\n//! Supported backends:\n//! - Kubernetes (K8s clusters)\n//! - Docker (standalone containers)\n//! - Cloud VMs (AWS, Azure, GCP)\n//! - Bare Metal (physical servers)\n//! - Serverless (Lambda, Azure Functions)\n//! - HPC (SLURM, PBS clusters)\n//!\n//! Performance targets:\n//! - Scheduling latency: \u003c100ms\n//! - Throughput: 1000+ jobs/minute\n//! - Queue operations: O(log n)\n//! - Filter operations: O(n)\n\npub mod affinity;\npub mod backend;\npub mod integration;\npub mod pipeline;\npub mod queue;\npub mod selection;\npub mod types;\n\npub use affinity::*;\npub use backend::*;\npub use integration::*;\npub use pipeline::*;\npub use queue::*;\npub use selection::*;\npub use types::*;\n\npub use backend::ComputeResource;\n\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\nuse tracing::{error, info};\n\n/// Core scheduler orchestrator\npub struct Scheduler {\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n    pipeline: SchedulingPipeline,\n    queue: PriorityQueue,\n    config: SchedulerConfig,\n    metrics: Arc\u003cRwLock\u003cSchedulerMetrics\u003e\u003e,\n}\n\nimpl Scheduler {\n    /// Create new scheduler with backend\n    pub fn new(\n        backend: Arc\u003cdyn SchedulerBackend\u003e,\n        config: SchedulerConfig,\n    ) -\u003e Result\u003cSelf, SchedulerError\u003e {\n        let pipeline = SchedulingPipeline::new(config.pipeline.clone());\n        let queue = PriorityQueue::new(config.queue.clone());\n        let metrics = Arc::new(RwLock::new(SchedulerMetrics::default()));\n\n        Ok(Self {\n            backend,\n            pipeline,\n            queue,\n            config,\n            metrics,\n        })\n    }\n\n    /// Schedule a job (main entry point)\n    pub async fn schedule_job(\u0026self, job: Job) -\u003e Result\u003cSchedulingResult, SchedulerError\u003e {\n        let start_time = std::time::Instant::now();\n        let job_id = job.metadata.id;\n\n        info!(\"Starting scheduling for job {}\", job_id);\n\n        // Validate job requirements\n        self.validate_job(\u0026job)?;\n\n        // Add to queue\n        self.queue.enqueue(job.clone()).await?;\n\n        // Phase 1: Inform - Get available workers\n        let available_workers = self.pipeline.inform().await?;\n\n        // Phase 2: Filter - Filter feasible workers\n        let feasible_workers = self.pipeline.filter(\u0026job, available_workers).await?;\n\n        if feasible_workers.is_empty() {\n            // No suitable workers found\n            let result = SchedulingResult {\n                job_id,\n                assigned_node: None,\n                status: SchedulingStatus::Pending,\n                scheduling_latency_ms: start_time.elapsed().as_millis() as u64,\n                reasoning: \"No feasible workers found\".to_string(),\n                queue_position: self.queue.position(\u0026job_id).await.unwrap_or(0),\n            };\n\n            // Update metrics\n            self.update_metrics_on_failure(\u0026result).await;\n\n            return Ok(result);\n        }\n\n        // Phase 3: Score - Rank workers\n        let scored_workers = self.pipeline.score(\u0026job, feasible_workers).await?;\n\n        // Phase 4: Bind - Assign to best worker\n        let best_worker = scored_workers\n            .first()\n            .ok_or_else(|| SchedulerError::NoEligibleWorker(job_id))?;\n\n        let binding_result = self.backend.bind_job(\u0026job_id, \u0026best_worker.node.id).await?;\n\n        // Create result\n        let result = SchedulingResult {\n            job_id,\n            assigned_node: Some(best_worker.node.id.clone()),\n            status: SchedulingStatus::Scheduled,\n            scheduling_latency_ms: start_time.elapsed().as_millis() as u64,\n            reasoning: format!(\n                \"Selected worker {} with score {:.2}\",\n                best_worker.node.id, best_worker.score\n            ),\n            queue_position: 0,\n        };\n\n        // Update metrics\n        self.update_metrics_on_success(\u0026result, \u0026best_worker.score)\n            .await;\n\n        info!(\n            \"Successfully scheduled job {} to worker {} in {}ms\",\n            job_id, best_worker.node.id, result.scheduling_latency_ms\n        );\n\n        Ok(result)\n    }\n\n    /// Cancel a scheduled job\n    pub async fn cancel_job(\u0026self, job_id: \u0026JobId) -\u003e Result\u003c(), SchedulerError\u003e {\n        info!(\"Cancelling job {}\", job_id);\n\n        // Remove from queue if pending\n        self.queue.cancel(job_id).await?;\n\n        // TODO: Unbind from worker if scheduled\n\n        Ok(())\n    }\n\n    /// Get scheduler status\n    pub async fn get_status(\u0026self) -\u003e SchedulerStatusView {\n        let metrics = self.metrics.read().await;\n\n        SchedulerStatusView {\n            pending_jobs: self.queue.pending_count().await as u64,\n            active_schedulers: 1,\n            backend_type: self.backend.backend_type(),\n            total_scheduled: metrics.total_scheduled,\n            total_failed: metrics.total_failed,\n            avg_scheduling_latency_ms: metrics.avg_scheduling_latency_ms,\n            uptime_seconds: metrics.uptime_seconds,\n        }\n    }\n\n    /// Check if job is in queue (for testing)\n    pub async fn is_job_in_queue(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        self.queue.contains(job_id).await\n    }\n\n    /// Validate job requirements\n    fn validate_job(\u0026self, job: \u0026Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        // Check if job has valid requirements\n        if job.spec.resource_requirements.is_none() {\n            return Err(SchedulerError::InvalidJobRequirements(\n                \"Resource requirements not specified\".to_string(),\n            ));\n        }\n\n        // Validate resource values\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if let Some(cpu) = req.cpu_cores {\n                if cpu \u003c= 0.0 {\n                    return Err(SchedulerError::InvalidJobRequirements(\n                        \"CPU cores must be positive\".to_string(),\n                    ));\n                }\n            }\n\n            if let Some(memory) = req.memory_bytes {\n                if memory == 0 {\n                    return Err(SchedulerError::InvalidJobRequirements(\n                        \"Memory must be positive\".to_string(),\n                    ));\n                }\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Update metrics on successful scheduling\n    async fn update_metrics_on_success(\u0026self, result: \u0026SchedulingResult, score: \u0026f64) {\n        let mut metrics = self.metrics.write().await;\n        metrics.total_scheduled += 1;\n        metrics.avg_scheduling_latency_ms =\n            (metrics.avg_scheduling_latency_ms + result.scheduling_latency_ms) / 2;\n        metrics.avg_worker_score = (metrics.avg_worker_score + score) / 2.0;\n    }\n\n    /// Update metrics on failed scheduling\n    async fn update_metrics_on_failure(\u0026self, result: \u0026SchedulingResult) {\n        let mut metrics = self.metrics.write().await;\n        metrics.total_failed += 1;\n        metrics.avg_scheduling_latency_ms =\n            (metrics.avg_scheduling_latency_ms + result.scheduling_latency_ms) / 2;\n    }\n}\n\n/// Scheduler configuration\n#[derive(Debug, Clone)]\npub struct SchedulerConfig {\n    pub pipeline: PipelineConfig,\n    pub queue: QueueConfig,\n    pub worker_selection: WorkerSelectionStrategy,\n    pub preemption_enabled: bool,\n    pub timeout_seconds: u64,\n}\n\nimpl Default for SchedulerConfig {\n    fn default() -\u003e Self {\n        Self {\n            pipeline: PipelineConfig::default(),\n            queue: QueueConfig::default(),\n            worker_selection: WorkerSelectionStrategy::LeastLoaded,\n            preemption_enabled: true,\n            timeout_seconds: 300, // 5 minutes\n        }\n    }\n}\n\n/// Scheduling result\n#[derive(Debug, Clone)]\npub struct SchedulingResult {\n    pub job_id: JobId,\n    pub assigned_node: Option\u003cWorkerId\u003e,\n    pub status: SchedulingStatus,\n    pub scheduling_latency_ms: u64,\n    pub reasoning: String,\n    pub queue_position: usize,\n}\n\n/// Scheduling status\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SchedulingStatus {\n    Scheduled, // Successfully scheduled\n    Pending,   // Waiting in queue\n    Failed,    // Scheduling failed\n    Cancelled, // Job was cancelled\n}\n\n/// Scheduler metrics\n#[derive(Debug, Default)]\npub struct SchedulerMetrics {\n    pub total_scheduled: u64,\n    pub total_failed: u64,\n    pub avg_scheduling_latency_ms: u64,\n    pub avg_worker_score: f64,\n    pub uptime_seconds: u64,\n}\n\n/// Scheduler status view\n#[derive(Debug, Clone)]\npub struct SchedulerStatusView {\n    pub pending_jobs: u64,\n    pub active_schedulers: u32,\n    pub backend_type: BackendType,\n    pub total_scheduled: u64,\n    pub total_failed: u64,\n    pub avg_scheduling_latency_ms: u64,\n    pub uptime_seconds: u64,\n}\n\n/// Job ID type\npub type JobId = uuid::Uuid;\n\n/// Worker ID type\npub type WorkerId = uuid::Uuid;\n\n/// Custom error types\n#[derive(thiserror::Error, Debug)]\npub enum SchedulerError {\n    #[error(\"Job validation failed: {0}\")]\n    InvalidJobRequirements(String),\n\n    #[error(\"No eligible worker found for job {0}\")]\n    NoEligibleWorker(JobId),\n\n    #[error(\"Worker not found: {0}\")]\n    WorkerNotFound(WorkerId),\n\n    #[error(\"Backend error: {0}\")]\n    BackendError(String),\n\n    #[error(\"Pipeline error: {0}\")]\n    PipelineError(String),\n\n    #[error(\"Queue error: {0}\")]\n    QueueError(String),\n\n    #[error(\"Selection error: {0}\")]\n    SelectionError(String),\n\n    #[error(\"Binding error: {0}\")]\n    BindingError(String),\n\n    #[error(\"Preemption error: {0}\")]\n    PreemptionError(String),\n\n    #[error(\"Configuration error: {0}\")]\n    ConfigurationError(String),\n\n    #[error(\"Timeout error\")]\n    Timeout,\n\n    #[error(\"Internal error: {0}\")]\n    Internal(#[from] anyhow::Error),\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{ComputeResource, SchedulerBackend};\n    use crate::types::{BackendType, Job, JobMetadata, JobSpec, ResourceRequirements, WorkerNode};\n    use chrono::Utc;\n    use std::collections::HashMap;\n\n    /// Mock backend for testing\n    struct MockBackend {\n        workers: Vec\u003cWorkerNode\u003e,\n    }\n\n    impl MockBackend {\n        fn new() -\u003e Self {\n            Self {\n                workers: vec![WorkerNode {\n                    id: uuid::Uuid::new_v4(),\n                    backend_type: BackendType::Kubernetes,\n                    status: crate::types::WorkerStatus::Ready,\n                    resources: ComputeResource {\n                        cpu_cores: 8.0,\n                        memory_bytes: 16_000_000_000,\n                        gpu_count: 0,\n                    },\n                    labels: HashMap::new(),\n                    taints: vec![],\n                    backend_specific: crate::types::BackendSpecific::Kubernetes(\n                        crate::types::KubernetesNodeSpecific {\n                            node_name: \"node-1\".to_string(),\n                            namespace: \"default\".to_string(),\n                        },\n                    ),\n                    location: crate::types::NodeLocation::default(),\n                }],\n            }\n        }\n    }\n\n    #[async_trait::async_trait]\n    impl SchedulerBackend for MockBackend {\n        fn backend_type(\u0026self) -\u003e BackendType {\n            BackendType::Kubernetes\n        }\n\n        async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n            Ok(self.workers.clone())\n        }\n\n        async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .cloned()\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n\n        async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n            info!(\"Mock binding job {} to node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn unbind_job(\n            \u0026self,\n            job_id: \u0026JobId,\n            node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), SchedulerError\u003e {\n            info!(\"Mock unbinding job {} from node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn get_node_status(\n            \u0026self,\n            id: \u0026WorkerId,\n        ) -\u003e Result\u003ccrate::types::WorkerStatus, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .map(|w| w.status.clone())\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_creation() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config);\n        assert!(scheduler.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_scheduling_job() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config).unwrap();\n\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(4_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let result = scheduler.schedule_job(job.clone()).await;\n        assert!(result.is_ok());\n\n        let scheduling_result = result.unwrap();\n        // Job se agrega a la queue para procesamiento posterior\n        // No se schedulea inmediatamente en esta implementacin\n        assert_eq!(scheduling_result.status, SchedulingStatus::Pending);\n        assert!(scheduling_result.assigned_node.is_none());\n\n        // Verificar que el job est en la cola\n        assert!(scheduler.is_job_in_queue(\u0026job.metadata.id).await);\n    }\n\n    #[tokio::test]\n    async fn test_job_validation() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config).unwrap();\n\n        // Job without resource requirements should fail\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: None,\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let result = scheduler.schedule_job(job).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_status() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config).unwrap();\n\n        let status = scheduler.get_status().await;\n        assert_eq!(status.pending_jobs, 0);\n        assert_eq!(status.active_schedulers, 1);\n        assert_eq!(status.backend_type, BackendType::Kubernetes);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","multi_sched","mod.rs"],"content":"//! Multiple Schedulers Support Module\n//!\n//! This module implements support for multiple scheduler instances that can run\n//! simultaneously with different configurations, scopes, and specializations.\n\nuse crate::SchedulerError;\nuse crate::backend::SchedulerBackend;\nuse crate::pipeline::SchedulingPipeline;\nuse crate::queue::{PriorityQueue, QueueConfig, QueueStrategy};\nuse crate::selection::{SelectionStrategy, WorkerSelector};\nuse crate::types::*;\nuse std::collections::HashMap;\nuse std::sync::Arc;\n\n/// Multiple scheduler configuration\n#[derive(Debug, Clone)]\npub struct MultiSchedulerConfig {\n    /// Default scheduler name\n    pub default_scheduler: String,\n    /// Whether to enable automatic fallback\n    pub enable_fallback: bool,\n    /// Maximum number of schedulers allowed\n    pub max_schedulers: usize,\n    /// Scheduler health check interval\n    pub health_check_interval_ms: u64,\n}\n\n/// Scheduler instance definition\n#[derive(Debug, Clone)]\npub struct SchedulerInstance {\n    /// Unique scheduler name\n    pub name: String,\n    /// Scheduler configuration\n    pub config: SchedulerInstanceConfig,\n    /// Backend to use\n    pub backend: Arc\u003cdyn SchedulerBackend\u003e,\n    /// Pipeline for job processing\n    pub pipeline: SchedulingPipeline,\n    /// Selection strategy\n    pub selector: WorkerSelector,\n    /// Queue for jobs\n    pub queue: PriorityQueue,\n    /// Whether this scheduler is enabled\n    pub enabled: bool,\n    /// Current status\n    pub status: SchedulerInstanceStatus,\n}\n\n/// Scheduler instance configuration\n#[derive(Debug, Clone)]\npub struct SchedulerInstanceConfig {\n    /// Queue strategy\n    pub queue_strategy: QueueStrategy,\n    /// Selection strategy\n    pub selection_strategy: SelectionStrategy,\n    /// Scheduler scope (cluster-wide, namespace-specific, etc.)\n    pub scope: SchedulerScope,\n    /// Specialization tags (e.g., \"gpu\", \"high-priority\", \"batch\")\n    pub specializations: Vec\u003cString\u003e,\n    /// Priority threshold (only handle jobs above this priority)\n    pub priority_threshold: Option\u003cJobPriority\u003e,\n    /// Namespace filter (if namespace-scoped)\n    pub namespace_filter: Option\u003cString\u003e,\n}\n\n/// Scheduler scope defines where this scheduler operates\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SchedulerScope {\n    /// Cluster-wide scheduler\n    ClusterWide,\n    /// Namespace-specific scheduler\n    Namespace(String),\n    /// Custom scope\n    Custom(String),\n}\n\n/// Scheduler instance status\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SchedulerInstanceStatus {\n    /// Scheduler is initializing\n    Initializing,\n    /// Scheduler is running normally\n    Running,\n    /// Scheduler is in degraded mode\n    Degraded,\n    /// Scheduler is offline\n    Offline,\n    /// Scheduler has failed\n    Failed(String),\n}\n\n/// Job routing result\n#[derive(Debug, Clone)]\npub struct RoutingResult {\n    /// Selected scheduler name\n    pub selected_scheduler: String,\n    /// Whether routing was successful\n    pub success: bool,\n    /// Reason for routing decision\n    pub reason: String,\n}\n\n/// Scheduler registry for managing multiple instances\npub struct SchedulerRegistry {\n    /// Registry configuration\n    config: MultiSchedulerConfig,\n    /// Active scheduler instances\n    schedulers: HashMap\u003cString, SchedulerInstance\u003e,\n    /// Default scheduler name\n    default_scheduler: String,\n}\n\nimpl SchedulerRegistry {\n    /// Create new scheduler registry\n    pub fn new(config: MultiSchedulerConfig) -\u003e Self {\n        Self {\n            schedulers: HashMap::new(),\n            default_scheduler: config.default_scheduler.clone(),\n            config,\n        }\n    }\n\n    /// Register a new scheduler instance\n    pub fn register_scheduler(\n        \u0026mut self,\n        instance: SchedulerInstance,\n    ) -\u003e Result\u003c(), SchedulerError\u003e {\n        if self.schedulers.len() \u003e= self.config.max_schedulers {\n            return Err(SchedulerError::ConfigurationError(\n                \"Maximum number of schedulers reached\".to_string(),\n            ));\n        }\n\n        if self.schedulers.contains_key(\u0026instance.name) {\n            return Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' already registered\",\n                instance.name\n            )));\n        }\n\n        self.schedulers.insert(instance.name.clone(), instance);\n\n        Ok(())\n    }\n\n    /// Unregister a scheduler instance\n    pub fn unregister_scheduler(\u0026mut self, name: \u0026str) -\u003e Result\u003c(), SchedulerError\u003e {\n        if !self.schedulers.contains_key(name) {\n            return Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' not found\",\n                name\n            )));\n        }\n\n        self.schedulers.remove(name);\n\n        // Update default if necessary\n        if name == \u0026self.default_scheduler \u0026\u0026 !self.schedulers.is_empty() {\n            if let Some(first_key) = self.schedulers.keys().next() {\n                self.default_scheduler = first_key.clone();\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Route a job to an appropriate scheduler\n    pub fn route_job(\u0026self, job: \u0026Job) -\u003e RoutingResult {\n        // First, try to find a specialized scheduler\n        if let Some(specialized) = self.find_specialized_scheduler(job) {\n            return RoutingResult {\n                selected_scheduler: specialized,\n                success: true,\n                reason: \"Specialized scheduler found\".to_string(),\n            };\n        }\n\n        // If namespace-scoped scheduler exists, use it\n        if let Some(namespace_scheduler) = self.find_namespace_scheduler(job) {\n            return RoutingResult {\n                selected_scheduler: namespace_scheduler,\n                success: true,\n                reason: \"Namespace-scoped scheduler found\".to_string(),\n            };\n        }\n\n        // Use default scheduler\n        if self.schedulers.contains_key(\u0026self.default_scheduler) {\n            RoutingResult {\n                selected_scheduler: self.default_scheduler.clone(),\n                success: true,\n                reason: \"Using default scheduler\".to_string(),\n            }\n        } else {\n            RoutingResult {\n                selected_scheduler: \"\".to_string(),\n                success: false,\n                reason: \"No schedulers available\".to_string(),\n            }\n        }\n    }\n\n    /// Find a specialized scheduler for the job\n    fn find_specialized_scheduler(\u0026self, job: \u0026Job) -\u003e Option\u003cString\u003e {\n        // Check if any scheduler specializes in the job's requirements\n        for (name, scheduler) in \u0026self.schedulers {\n            if !scheduler.enabled || scheduler.status != SchedulerInstanceStatus::Running {\n                continue;\n            }\n\n            // Check specialization tags\n            let has_matching_specialization =\n                scheduler\n                    .config\n                    .specializations\n                    .iter()\n                    .any(|tag| match tag.as_str() {\n                        \"gpu\" =\u003e job\n                            .spec\n                            .resource_requirements\n                            .as_ref()\n                            .and_then(|r| r.gpu_count)\n                            .map(|gpu| gpu \u003e 0)\n                            .unwrap_or(false),\n                        \"high-priority\" =\u003e {\n                            if let Some(threshold) = \u0026scheduler.config.priority_threshold {\n                                job.spec.priority \u003e= *threshold\n                            } else {\n                                matches!(\n                                    job.spec.priority,\n                                    JobPriority::High | JobPriority::Critical\n                                )\n                            }\n                        }\n                        _ =\u003e false,\n                    });\n\n            if has_matching_specialization {\n                return Some(name.clone());\n            }\n        }\n\n        None\n    }\n\n    /// Find a namespace-scoped scheduler for the job\n    fn find_namespace_scheduler(\u0026self, job: \u0026Job) -\u003e Option\u003cString\u003e {\n        for (name, scheduler) in \u0026self.schedulers {\n            if !scheduler.enabled || scheduler.status != SchedulerInstanceStatus::Running {\n                continue;\n            }\n\n            if let SchedulerScope::Namespace(ref namespace) = scheduler.config.scope {\n                if namespace == \u0026job.metadata.namespace {\n                    return Some(name.clone());\n                }\n            }\n        }\n\n        None\n    }\n\n    /// Get a scheduler instance by name\n    pub fn get_scheduler(\u0026self, name: \u0026str) -\u003e Option\u003c\u0026SchedulerInstance\u003e {\n        self.schedulers.get(name)\n    }\n\n    /// Get all scheduler names\n    pub fn list_schedulers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.schedulers.keys().cloned().collect()\n    }\n\n    /// Check scheduler health\n    pub fn check_health(\u0026self, name: \u0026str) -\u003e Result\u003cSchedulerInstanceStatus, SchedulerError\u003e {\n        self.schedulers\n            .get(name)\n            .map(|s| s.status.clone())\n            .ok_or_else(|| {\n                SchedulerError::ConfigurationError(format!(\"Scheduler '{}' not found\", name))\n            })\n    }\n\n    /// Update scheduler status\n    pub fn update_status(\n        \u0026mut self,\n        name: \u0026str,\n        status: SchedulerInstanceStatus,\n    ) -\u003e Result\u003c(), SchedulerError\u003e {\n        if let Some(scheduler) = self.schedulers.get_mut(name) {\n            scheduler.status = status;\n            Ok(())\n        } else {\n            Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' not found\",\n                name\n            )))\n        }\n    }\n\n    /// Get the default scheduler name\n    pub fn default_scheduler(\u0026self) -\u003e \u0026str {\n        \u0026self.default_scheduler\n    }\n\n    /// Enable or disable a scheduler\n    pub fn set_enabled(\u0026mut self, name: \u0026str, enabled: bool) -\u003e Result\u003c(), SchedulerError\u003e {\n        if let Some(scheduler) = self.schedulers.get_mut(name) {\n            scheduler.enabled = enabled;\n            Ok(())\n        } else {\n            Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' not found\",\n                name\n            )))\n        }\n    }\n}\n\n/// Helper to create a default scheduler instance\npub fn create_default_scheduler(\n    name: String,\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n) -\u003e SchedulerInstance {\n    let config = SchedulerInstanceConfig {\n        queue_strategy: QueueStrategy::Priority {\n            with_preemption: true,\n            max_queue_time: None,\n        },\n        selection_strategy: SelectionStrategy::ResourceBalance,\n        scope: SchedulerScope::ClusterWide,\n        specializations: vec![],\n        priority_threshold: None,\n        namespace_filter: None,\n    };\n\n    SchedulerInstance {\n        name,\n        config,\n        backend,\n        pipeline: SchedulingPipeline::new(PipelineConfig::default()),\n        selector: WorkerSelector::new(\n            SelectionStrategy::ResourceBalance,\n            SelectionCriteria::default(),\n        ),\n        queue: PriorityQueue::new(QueueConfig::default()),\n        enabled: true,\n        status: SchedulerInstanceStatus::Running,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::MockBackend;\n\n    fn create_test_job(priority: JobPriority, namespace: String) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace,\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(2_000_000_000),\n                    gpu_count: if priority == JobPriority::Critical {\n                        Some(1)\n                    } else {\n                        None\n                    },\n                    ephemeral_storage: None,\n                }),\n                priority,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_registration() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        let backend = Arc::new(MockBackend::new());\n        let scheduler = create_default_scheduler(\"default\".to_string(), backend);\n\n        registry.register_scheduler(scheduler).unwrap();\n\n        assert_eq!(registry.list_schedulers(), vec![\"default\"]);\n    }\n\n    #[tokio::test]\n    async fn test_job_routing_default() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        let backend = Arc::new(MockBackend::new());\n        let scheduler = create_default_scheduler(\"default\".to_string(), backend);\n\n        registry.register_scheduler(scheduler).unwrap();\n\n        let job = create_test_job(JobPriority::Medium, \"default\".to_string());\n        let result = registry.route_job(\u0026job);\n\n        assert!(result.success);\n        assert_eq!(result.selected_scheduler, \"default\");\n    }\n\n    #[tokio::test]\n    async fn test_job_routing_specialized() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        // Register default scheduler\n        let backend1 = Arc::new(MockBackend::new());\n        let mut default_scheduler = create_default_scheduler(\"default\".to_string(), backend1);\n        default_scheduler.config.specializations = vec![\"general\".to_string()];\n        registry.register_scheduler(default_scheduler).unwrap();\n\n        // Register GPU scheduler\n        let backend2 = Arc::new(MockBackend::new());\n        let mut gpu_scheduler = create_default_scheduler(\"gpu\".to_string(), backend2);\n        gpu_scheduler.config.specializations = vec![\"gpu\".to_string()];\n        registry.register_scheduler(gpu_scheduler).unwrap();\n\n        // Route GPU job\n        let gpu_job = create_test_job(JobPriority::High, \"default\".to_string());\n        let result = registry.route_job(\u0026gpu_job);\n\n        assert!(result.success);\n        assert_eq!(result.selected_scheduler, \"gpu\");\n    }\n\n    #[tokio::test]\n    async fn test_namespace_routing() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        // Register namespace-scoped scheduler\n        let backend = Arc::new(MockBackend::new());\n        let mut namespace_scheduler =\n            create_default_scheduler(\"team-a-scheduler\".to_string(), backend);\n        namespace_scheduler.config.scope = SchedulerScope::Namespace(\"team-a\".to_string());\n        registry.register_scheduler(namespace_scheduler).unwrap();\n\n        // Route job from team-a namespace\n        let job = create_test_job(JobPriority::Medium, \"team-a\".to_string());\n        let result = registry.route_job(\u0026job);\n\n        assert!(result.success);\n        assert_eq!(result.selected_scheduler, \"team-a-scheduler\");\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_not_found_error() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let registry = SchedulerRegistry::new(config);\n\n        let status = registry.check_health(\"nonexistent\");\n\n        assert!(status.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_duplicate_scheduler_error() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        let backend = Arc::new(MockBackend::new());\n        let scheduler = create_default_scheduler(\"default\".to_string(), backend);\n\n        registry.register_scheduler(scheduler.clone()).unwrap();\n        let result = registry.register_scheduler(scheduler);\n\n        assert!(result.is_err());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","pipeline","mod.rs"],"content":"//! Scheduling Pipeline Module\n//!\n//! Implements the 4-phase Kubernetes-style scheduling pipeline:\n//! 1. INFORM: Watch and discover available workers\n//! 2. FILTER: Apply constraints and filter feasible workers\n//! 3. SCORE: Rank workers based on scoring criteria\n//! 4. BIND: Assign job to best worker\n\nuse crate::backend::SchedulerBackend;\nuse crate::types::*;\nuse crate::{JobId, SchedulerError, WorkerId};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tracing::{debug, info};\n\n/// Scheduling pipeline configuration\n#[derive(Debug, Clone)]\npub struct PipelineConfig {\n    pub inform_interval_ms: u64,\n    pub filter_enabled: bool,\n    pub score_enabled: bool,\n    pub max_candidates: usize,\n}\n\nimpl Default for PipelineConfig {\n    fn default() -\u003e Self {\n        Self {\n            inform_interval_ms: 1000, // 1 second\n            filter_enabled: true,\n            score_enabled: true,\n            max_candidates: 100, // Limit to prevent performance issues\n        }\n    }\n}\n\n/// Main scheduling pipeline\npub struct SchedulingPipeline {\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n    config: PipelineConfig,\n    informer: Informer,\n}\n\nimpl SchedulingPipeline {\n    /// Create new scheduling pipeline\n    pub fn new(config: PipelineConfig) -\u003e Self {\n        Self {\n            backend: Arc::new(crate::backend::KubernetesBackend::new()),\n            config: config.clone(),\n            informer: Informer::new(config.inform_interval_ms),\n        }\n    }\n\n    /// Set backend (used for testing)\n    pub fn set_backend(\u0026mut self, backend: Arc\u003cdyn SchedulerBackend\u003e) {\n        self.backend = backend;\n    }\n\n    /// Phase 1: Inform - Get available workers\n    pub async fn inform(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        debug!(\"Phase 1: Inform - discovering available workers\");\n\n        let workers = self.backend.list_nodes().await?;\n\n        // Filter only available workers\n        let available_workers: Vec\u003c_\u003e = workers\n            .into_iter()\n            .filter(|w| w.status.is_available())\n            .collect();\n\n        debug!(\n            \"Phase 1: Found {} available workers\",\n            available_workers.len()\n        );\n\n        Ok(available_workers)\n    }\n\n    /// Phase 2: Filter - Apply constraints and filter feasible workers\n    pub async fn filter(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        debug!(\"Phase 2: Filter - filtering feasible workers\");\n\n        if !self.config.filter_enabled {\n            debug!(\"Phase 2: Filtering disabled, returning all workers\");\n            return Ok(workers);\n        }\n\n        let total_workers = workers.len();\n        let mut feasible_workers = Vec::new();\n\n        for worker in workers {\n            if self.worker_matches_job(\u0026worker, job) {\n                feasible_workers.push(worker);\n            }\n\n            // Limit candidates to prevent performance issues\n            if feasible_workers.len() \u003e= self.config.max_candidates {\n                debug!(\n                    \"Phase 2: Reached max candidates limit ({})\",\n                    self.config.max_candidates\n                );\n                break;\n            }\n        }\n\n        debug!(\n            \"Phase 2: Filtered to {} feasible workers out of {}\",\n            feasible_workers.len(),\n            total_workers\n        );\n\n        Ok(feasible_workers)\n    }\n\n    /// Phase 3: Score - Rank workers based on scoring criteria\n    pub async fn score(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cVec\u003cScoredWorker\u003e, SchedulerError\u003e {\n        debug!(\"Phase 3: Score - ranking workers\");\n\n        if !self.config.score_enabled {\n            debug!(\"Phase 3: Scoring disabled, assigning equal scores\");\n            return Ok(workers\n                .into_iter()\n                .map(|w| ScoredWorker::new(w, 50.0))\n                .collect());\n        }\n\n        let scoring_weights = ScoringWeights::default();\n\n        let mut scored_workers: Vec\u003c_\u003e = workers\n            .into_iter()\n            .map(|worker| {\n                let score = worker.calculate_score(job, \u0026scoring_weights);\n                ScoredWorker::new(worker, score)\n            })\n            .collect();\n\n        // Sort by score descending\n        scored_workers.sort_by(|a, b| {\n            b.score\n                .partial_cmp(\u0026a.score)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n\n        debug!(\n            \"Phase 3: Scored {} workers, best score: {:.2}\",\n            scored_workers.len(),\n            scored_workers.first().map(|s| s.score).unwrap_or(0.0)\n        );\n\n        Ok(scored_workers)\n    }\n\n    /// Phase 4: Bind - Job is already assigned in schedule_job\n    /// This is a placeholder for the binding phase\n    pub async fn bind(\u0026self, job_id: \u0026JobId, worker_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        debug!(\n            \"Phase 4: Bind - binding job {} to worker {}\",\n            job_id, worker_id\n        );\n\n        // Actual binding is done by the backend in the main scheduler\n        // This is here for completeness of the pipeline\n        info!(\"Job {} bound to worker {}\", job_id, worker_id);\n\n        Ok(())\n    }\n\n    /// Check if worker matches job requirements\n    fn worker_matches_job(\u0026self, worker: \u0026WorkerNode, job: \u0026Job) -\u003e bool {\n        // Check resources\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if !worker.resources.has_resources(req) {\n                return false;\n            }\n        }\n\n        // Check node selector\n        if let Some(selector) = \u0026job.spec.node_selector {\n            for (key, value) in \u0026selector.labels {\n                match worker.labels.get(key) {\n                    Some(node_value) =\u003e {\n                        if node_value != value {\n                            return false;\n                        }\n                    }\n                    None =\u003e return false,\n                }\n            }\n        }\n\n        // Check taints and tolerations\n        for taint in \u0026worker.taints {\n            let has_matching_toleration = job.spec.tolerations.iter().any(|tol| {\n                tol.key == taint.key \u0026\u0026 tol.value == taint.value \u0026\u0026 tol.effect == taint.effect\n            });\n\n            // If there's no matching toleration and effect is NoSchedule, reject\n            if !has_matching_toleration \u0026\u0026 taint.effect == TaintEffect::NoSchedule {\n                return false;\n            }\n        }\n\n        // Check affinity rules (simplified)\n        if let Some(affinity) = \u0026job.spec.affinity {\n            // In production: implement full affinity matching logic\n            // For now: assume all nodes match\n        }\n\n        true\n    }\n}\n\n/// Informer component for watching cluster state\n#[derive(Debug)]\npub struct Informer {\n    interval_ms: u64,\n    last_sync: Arc\u003cstd::sync::Mutex\u003cstd::time::Instant\u003e\u003e,\n}\n\nimpl Informer {\n    pub fn new(interval_ms: u64) -\u003e Self {\n        Self {\n            interval_ms,\n            last_sync: Arc::new(std::sync::Mutex::new(std::time::Instant::now())),\n        }\n    }\n\n    /// Check if informer should sync\n    pub fn should_sync(\u0026self) -\u003e bool {\n        let last_sync = self.last_sync.lock().unwrap();\n        last_sync.elapsed().as_millis() \u003e= self.interval_ms as u128\n    }\n\n    /// Update last sync time\n    pub fn update_sync(\u0026self) {\n        let mut last_sync = self.last_sync.lock().unwrap();\n        *last_sync = std::time::Instant::now();\n    }\n}\n\n/// Filter stage of the pipeline\n#[derive(Debug)]\npub struct FilterStage {\n    enabled: bool,\n}\n\nimpl FilterStage {\n    pub fn new(enabled: bool) -\u003e Self {\n        Self { enabled }\n    }\n\n    /// Apply filters to workers\n    pub fn apply(\u0026self, job: \u0026Job, workers: Vec\u003cWorkerNode\u003e) -\u003e Vec\u003cWorkerNode\u003e {\n        if !self.enabled {\n            return workers;\n        }\n\n        workers\n            .into_iter()\n            .filter(|w| self.passes_filters(w, job))\n            .collect()\n    }\n\n    /// Check if worker passes all filters\n    fn passes_filters(\u0026self, worker: \u0026WorkerNode, job: \u0026Job) -\u003e bool {\n        // Resource filter\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if !worker.resources.has_resources(req) {\n                return false;\n            }\n        }\n\n        // Node selector filter\n        if let Some(selector) = \u0026job.spec.node_selector {\n            for (key, value) in \u0026selector.labels {\n                match worker.labels.get(key) {\n                    Some(node_value) =\u003e {\n                        if node_value != value {\n                            return false;\n                        }\n                    }\n                    None =\u003e return false,\n                }\n            }\n        }\n\n        // Taint/toleration filter\n        for taint in \u0026worker.taints {\n            let has_matching_toleration = job.spec.tolerations.iter().any(|tol| {\n                tol.key == taint.key \u0026\u0026 tol.value == taint.value \u0026\u0026 tol.effect == taint.effect\n            });\n\n            if !has_matching_toleration \u0026\u0026 taint.effect == TaintEffect::NoSchedule {\n                return false;\n            }\n        }\n\n        true\n    }\n}\n\n/// Scoring stage of the pipeline\n#[derive(Debug)]\npub struct ScoreStage {\n    weights: ScoringWeights,\n}\n\nimpl ScoreStage {\n    pub fn new(weights: ScoringWeights) -\u003e Self {\n        Self { weights }\n    }\n\n    /// Score workers\n    pub fn apply(\u0026self, job: \u0026Job, workers: Vec\u003cWorkerNode\u003e) -\u003e Vec\u003cScoredWorker\u003e {\n        workers\n            .into_iter()\n            .map(|worker| {\n                let score = worker.calculate_score(job, \u0026self.weights);\n                ScoredWorker::new(worker, score)\n            })\n            .collect()\n    }\n}\n\n/// Bind stage of the pipeline\npub struct BindStage {\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n}\n\nimpl BindStage {\n    pub fn new(backend: Arc\u003cdyn SchedulerBackend\u003e) -\u003e Self {\n        Self { backend }\n    }\n\n    /// Bind job to worker\n    pub async fn bind(\u0026self, job_id: \u0026JobId, worker_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        self.backend.bind_job(job_id, worker_id).await\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{ComputeResource, SchedulerBackend};\n    use crate::types::WorkerNode;\n    use std::collections::HashMap;\n\n    struct MockBackend {\n        workers: Vec\u003cWorkerNode\u003e,\n    }\n\n    impl MockBackend {\n        fn new(workers: Vec\u003cWorkerNode\u003e) -\u003e Self {\n            Self { workers }\n        }\n    }\n\n    #[async_trait::async_trait]\n    impl SchedulerBackend for MockBackend {\n        fn backend_type(\u0026self) -\u003e crate::types::BackendType {\n            crate::types::BackendType::Kubernetes\n        }\n\n        async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n            Ok(self.workers.clone())\n        }\n\n        async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .cloned()\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n\n        async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n            tracing::info!(\"Mock binding job {} to node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn unbind_job(\n            \u0026self,\n            job_id: \u0026JobId,\n            node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), SchedulerError\u003e {\n            tracing::info!(\"Mock unbinding job {} from node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn get_node_status(\n            \u0026self,\n            id: \u0026WorkerId,\n        ) -\u003e Result\u003ccrate::types::WorkerStatus, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .map(|w| w.status.clone())\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_pipeline_inform() {\n        let workers = vec![WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: crate::types::BackendType::Kubernetes,\n            status: crate::types::WorkerStatus::Ready,\n            resources: ComputeResource {\n                cpu_cores: 8.0,\n                memory_bytes: 16_000_000_000,\n                gpu_count: 2,\n            },\n            labels: HashMap::new(),\n            taints: vec![],\n            backend_specific: crate::types::BackendSpecific::Kubernetes(\n                crate::types::KubernetesNodeSpecific {\n                    node_name: \"node-1\".to_string(),\n                    namespace: \"default\".to_string(),\n                },\n            ),\n            location: crate::types::NodeLocation::default(),\n        }];\n\n        let backend = Arc::new(MockBackend::new(workers));\n        let config = PipelineConfig::default();\n        let mut pipeline = SchedulingPipeline::new(config);\n        pipeline.set_backend(backend);\n\n        let available_workers = pipeline.inform().await.unwrap();\n        assert_eq!(available_workers.len(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_pipeline_filter() {\n        let workers = vec![\n            WorkerNode {\n                id: uuid::Uuid::new_v4(),\n                backend_type: crate::types::BackendType::Kubernetes,\n                status: crate::types::WorkerStatus::Ready,\n                resources: ComputeResource {\n                    cpu_cores: 8.0,\n                    memory_bytes: 16_000_000_000,\n                    gpu_count: 2,\n                },\n                labels: HashMap::new(),\n                taints: vec![],\n                backend_specific: crate::types::BackendSpecific::Kubernetes(\n                    crate::types::KubernetesNodeSpecific {\n                        node_name: \"node-1\".to_string(),\n                        namespace: \"default\".to_string(),\n                    },\n                ),\n                location: crate::types::NodeLocation::default(),\n            },\n            WorkerNode {\n                id: uuid::Uuid::new_v4(),\n                backend_type: crate::types::BackendType::Kubernetes,\n                status: crate::types::WorkerStatus::Offline,\n                resources: ComputeResource {\n                    cpu_cores: 8.0,\n                    memory_bytes: 16_000_000_000,\n                    gpu_count: 2,\n                },\n                labels: HashMap::new(),\n                taints: vec![],\n                backend_specific: crate::types::BackendSpecific::Kubernetes(\n                    crate::types::KubernetesNodeSpecific {\n                        node_name: \"node-2\".to_string(),\n                        namespace: \"default\".to_string(),\n                    },\n                ),\n                location: crate::types::NodeLocation::default(),\n            },\n        ];\n\n        let backend = Arc::new(MockBackend::new(workers));\n        let config = PipelineConfig::default();\n        let mut pipeline = SchedulingPipeline::new(config);\n        pipeline.set_backend(backend);\n\n        let job = crate::types::Job {\n            metadata: crate::types::JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: crate::types::JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(4.0),\n                    memory_bytes: Some(8_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let available_workers = pipeline.inform().await.unwrap();\n        let filtered_workers = pipeline.filter(\u0026job, available_workers).await.unwrap();\n\n        // Only 1 worker should be available (Ready status)\n        assert_eq!(filtered_workers.len(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_pipeline_score() {\n        let workers = vec![WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: crate::types::BackendType::Kubernetes,\n            status: crate::types::WorkerStatus::Ready,\n            resources: ComputeResource {\n                cpu_cores: 8.0,\n                memory_bytes: 16_000_000_000,\n                gpu_count: 2,\n            },\n            labels: HashMap::new(),\n            taints: vec![],\n            backend_specific: crate::types::BackendSpecific::Kubernetes(\n                crate::types::KubernetesNodeSpecific {\n                    node_name: \"node-1\".to_string(),\n                    namespace: \"default\".to_string(),\n                },\n            ),\n            location: crate::types::NodeLocation::default(),\n        }];\n\n        let backend = Arc::new(MockBackend::new(workers));\n        let config = PipelineConfig::default();\n        let mut pipeline = SchedulingPipeline::new(config);\n        pipeline.set_backend(backend);\n\n        let job = crate::types::Job {\n            metadata: crate::types::JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: crate::types::JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(4.0),\n                    memory_bytes: Some(8_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let available_workers = pipeline.inform().await.unwrap();\n        let filtered_workers = pipeline.filter(\u0026job, available_workers).await.unwrap();\n        let scored_workers = pipeline.score(\u0026job, filtered_workers).await.unwrap();\n\n        assert_eq!(scored_workers.len(), 1);\n        assert!(scored_workers[0].score \u003e 0.0);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","queue","mod.rs"],"content":"//! Priority Queue Module with Preemption Support\n//!\n//! This module implements multiple job queue strategies similar to Kubernetes:\n//! - Priority Queue with preemption support\n//! - Simple FIFO queue\n//! - Fair Queuing across tenants\n\nuse crate::types::*;\nuse crate::{JobId, SchedulerError};\nuse chrono::Utc;\nuse std::cmp::Ordering;\nuse std::collections::{BinaryHeap, HashMap};\nuse std::sync::{Arc, Mutex};\nuse tracing::{debug, info};\n\n/// Queue strategy types\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum QueueStrategy {\n    /// Simple FIFO (First In, First Out)\n    Fifo,\n\n    /// Priority queue with optional preemption\n    Priority {\n        with_preemption: bool,\n        max_queue_time: Option\u003cchrono::Duration\u003e,\n    },\n\n    /// Fair queuing distributed across tenants\n    Fair {\n        tenant_key: String,\n        weights: HashMap\u003cString, u32\u003e,\n        quantum: Option\u003cchrono::Duration\u003e,\n    },\n}\n\nimpl Default for QueueStrategy {\n    fn default() -\u003e Self {\n        QueueStrategy::Priority {\n            with_preemption: true,\n            max_queue_time: None,\n        }\n    }\n}\n\n/// Queue configuration\n#[derive(Debug, Clone)]\npub struct QueueConfig {\n    pub max_size: usize,\n    pub strategy: QueueStrategy,\n    pub fairness_enabled: bool,\n    pub namespace: Option\u003cString\u003e,\n}\n\nimpl Default for QueueConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_size: 10_000,\n            strategy: QueueStrategy::default(),\n            fairness_enabled: false,\n            namespace: None,\n        }\n    }\n}\n\n/// Priority queue implementation\n#[derive(Debug)]\npub struct PriorityQueue {\n    inner: Arc\u003cMutex\u003cPriorityQueueInner\u003e\u003e,\n}\n\n#[derive(Debug)]\nstruct PriorityQueueInner {\n    queue: BinaryHeap\u003cQueueEntry\u003e,\n    job_map: HashMap\u003cJobId, QueueEntry\u003e,\n    position_map: HashMap\u003cJobId, usize\u003e,\n    config: QueueConfig,\n}\n\n#[derive(Debug, Clone)]\nstruct QueueEntry {\n    job_id: JobId,\n    priority: JobPriority,\n    enqueue_time: chrono::DateTime\u003cUtc\u003e,\n    name: String,\n    namespace: String,\n}\n\n/// Ordering for priority queue (higher priority comes first)\nimpl PartialOrd for QueueEntry {\n    fn partial_cmp(\u0026self, other: \u0026Self) -\u003e Option\u003cOrdering\u003e {\n        Some(self.cmp(other))\n    }\n}\n\nimpl Ord for QueueEntry {\n    fn cmp(\u0026self, other: \u0026Self) -\u003e Ordering {\n        // Higher priority first\n        if self.priority != other.priority {\n            return self.priority.cmp(\u0026other.priority);\n        }\n\n        // Earlier enqueue time first (FIFO for same priority)\n        self.enqueue_time.cmp(\u0026other.enqueue_time)\n    }\n}\n\nimpl PartialEq for QueueEntry {\n    fn eq(\u0026self, other: \u0026Self) -\u003e bool {\n        self.priority == other.priority\n            \u0026\u0026 self.enqueue_time == other.enqueue_time\n            \u0026\u0026 self.job_id == other.job_id\n    }\n}\n\nimpl Eq for QueueEntry {}\n\nimpl PriorityQueue {\n    /// Create new priority queue\n    pub fn new(config: QueueConfig) -\u003e Self {\n        Self {\n            inner: Arc::new(Mutex::new(PriorityQueueInner {\n                queue: BinaryHeap::new(),\n                job_map: HashMap::new(),\n                position_map: HashMap::new(),\n                config,\n            })),\n        }\n    }\n\n    /// Enqueue a job\n    pub async fn enqueue(\u0026self, job: Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        // Check queue capacity\n        if inner.queue.len() \u003e= inner.config.max_size {\n            return Err(SchedulerError::QueueError(\n                \"Queue is at maximum capacity\".to_string(),\n            ));\n        }\n\n        let entry = QueueEntry {\n            job_id: job.metadata.id,\n            priority: job.spec.priority.clone(),\n            enqueue_time: job.metadata.created_at,\n            name: job.metadata.name.clone(),\n            namespace: job.metadata.namespace.clone(),\n        };\n\n        inner.queue.push(entry.clone());\n        inner.job_map.insert(job.metadata.id, entry);\n\n        // Update position map\n        let position = inner.queue.len() - 1;\n        inner.position_map.insert(job.metadata.id, position);\n\n        debug!(\n            \"Enqueued job {} with priority {:?} (position: {})\",\n            job.metadata.id, job.spec.priority, position\n        );\n\n        Ok(())\n    }\n\n    /// Dequeue the highest priority job\n    pub async fn dequeue(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if let Some(entry) = inner.queue.pop() {\n            inner.job_map.remove(\u0026entry.job_id);\n            inner.position_map.remove(\u0026entry.job_id);\n\n            debug!(\"Dequeued job {}\", entry.job_id);\n\n            Some(entry.job_id)\n        } else {\n            None\n        }\n    }\n\n    /// Get the next job without removing it\n    pub async fn peek(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let inner = self.inner.lock().unwrap();\n        inner.queue.peek().map(|entry| entry.job_id)\n    }\n\n    /// Cancel a job (remove from queue)\n    pub async fn cancel(\u0026self, job_id: \u0026JobId) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if !inner.job_map.contains_key(job_id) {\n            return Err(SchedulerError::QueueError(format!(\n                \"Job {} not found in queue\",\n                job_id\n            )));\n        }\n\n        // Remove from structures\n        inner.job_map.remove(job_id);\n        inner.position_map.remove(job_id);\n\n        // Rebuild queue without the job\n        // This is O(n) but necessary for BinaryHeap removal\n        let entries: Vec\u003c_\u003e = inner\n            .queue\n            .drain()\n            .filter(|e| e.job_id != *job_id)\n            .collect();\n\n        for entry in \u0026entries {\n            inner.queue.push(entry.clone());\n        }\n\n        for entry in \u0026entries {\n            inner.job_map.insert(entry.job_id, entry.clone());\n            let position = inner\n                .queue\n                .iter()\n                .position(|e| e.job_id == entry.job_id)\n                .unwrap_or(0);\n            inner.position_map.insert(entry.job_id, position);\n        }\n\n        info!(\"Cancelled job {}\", job_id);\n\n        Ok(())\n    }\n\n    /// Check if job is in queue\n    pub async fn contains(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        let inner = self.inner.lock().unwrap();\n        inner.job_map.contains_key(job_id)\n    }\n\n    /// Get queue position of a job\n    pub async fn position(\u0026self, job_id: \u0026JobId) -\u003e Option\u003cusize\u003e {\n        let inner = self.inner.lock().unwrap();\n        inner.position_map.get(job_id).copied()\n    }\n\n    /// Get pending job count\n    pub async fn pending_count(\u0026self) -\u003e usize {\n        let inner = self.inner.lock().unwrap();\n        inner.queue.len()\n    }\n\n    /// Get all pending jobs\n    pub async fn list_pending(\u0026self) -\u003e Vec\u003cQueueEntryView\u003e {\n        let inner = self.inner.lock().unwrap();\n        inner\n            .queue\n            .iter()\n            .map(|entry| QueueEntryView {\n                job_id: entry.job_id,\n                priority: entry.priority.clone(),\n                enqueue_time: entry.enqueue_time,\n                name: entry.name.clone(),\n                namespace: entry.namespace.clone(),\n            })\n            .collect()\n    }\n\n    /// Preemption: check if high-priority job can preempt others\n    pub async fn check_preemption(\u0026self, job: \u0026Job) -\u003e Result\u003cVec\u003cJobId\u003e, SchedulerError\u003e {\n        if let QueueStrategy::Priority {\n            with_preemption, ..\n        } = \u0026self.inner.lock().unwrap().config.strategy\n        {\n            if !with_preemption {\n                return Ok(vec![]);\n            }\n        } else {\n            // Preemption only supported for Priority strategy\n            return Ok(vec![]);\n        }\n\n        let mut inner = self.inner.lock().unwrap();\n        let mut preempted = Vec::new();\n\n        // Find lower priority jobs that can be preempted\n        let mut preemptable_jobs: Vec\u003c_\u003e = inner\n            .queue\n            .iter()\n            .filter(|entry| job.spec.priority.can_preempt(\u0026entry.priority))\n            .cloned()\n            .collect();\n\n        // Sort by priority (lowest first) to preempt lowest priority jobs first\n        preemptable_jobs.sort_by(|a, b| {\n            b.priority.cmp(\u0026a.priority) // Reverse order - lowest priority last\n        });\n\n        // Select jobs to preempt\n        // For simplicity, preempt all lower priority jobs\n        // In production, you'd preempt only as many as needed\n        for entry in preemptable_jobs {\n            preempted.push(entry.job_id);\n        }\n\n        if !preempted.is_empty() {\n            info!(\n                \"Preemption: job {} (priority {:?}) can preempt {} jobs\",\n                job.metadata.id,\n                job.spec.priority,\n                preempted.len()\n            );\n        }\n\n        Ok(preempted)\n    }\n\n    /// Execute preemption\n    pub async fn preempt(\u0026self, preempting_job: \u0026Job) -\u003e Result\u003cVec\u003cJobId\u003e, SchedulerError\u003e {\n        let preempted_jobs = self.check_preemption(preempting_job).await?;\n\n        if preempted_jobs.is_empty() {\n            return Ok(vec![]);\n        }\n\n        let mut inner = self.inner.lock().unwrap();\n\n        // Remove preempted jobs\n        for job_id in \u0026preempted_jobs {\n            inner.job_map.remove(job_id);\n            inner.position_map.remove(job_id);\n\n            // Remove from queue\n            let remaining: Vec\u003c_\u003e = inner\n                .queue\n                .drain()\n                .filter(|e| e.job_id != *job_id)\n                .collect();\n\n            inner.queue.clear();\n            for entry in \u0026remaining {\n                inner.queue.push(entry.clone());\n            }\n\n            info!(\"Preempted job {}\", job_id);\n        }\n\n        // Add the preempting job\n        let entry = QueueEntry {\n            job_id: preempting_job.metadata.id,\n            priority: preempting_job.spec.priority.clone(),\n            enqueue_time: preempting_job.metadata.created_at,\n            name: preempting_job.metadata.name.clone(),\n            namespace: preempting_job.metadata.namespace.clone(),\n        };\n\n        inner.queue.push(entry.clone());\n        inner.job_map.insert(preempting_job.metadata.id, entry);\n        let position = inner.queue.len() - 1;\n        inner\n            .position_map\n            .insert(preempting_job.metadata.id, position);\n\n        Ok(preempted_jobs)\n    }\n\n    /// Clear the queue\n    pub async fn clear(\u0026self) {\n        let mut inner = self.inner.lock().unwrap();\n        inner.queue.clear();\n        inner.job_map.clear();\n        inner.position_map.clear();\n    }\n}\n\n/// Queue entry view for external access\n#[derive(Debug, Clone)]\npub struct QueueEntryView {\n    pub job_id: JobId,\n    pub priority: JobPriority,\n    pub enqueue_time: chrono::DateTime\u003cUtc\u003e,\n    pub name: String,\n    pub namespace: String,\n}\n\n/// Queue statistics\n#[derive(Debug, Clone)]\npub struct QueueStats {\n    pub total_jobs: usize,\n    pub priority_counts: HashMap\u003cJobPriority, usize\u003e,\n    pub oldest_job_age: Option\u003cchrono::Duration\u003e,\n    pub newest_job_age: Option\u003cchrono::Duration\u003e,\n    pub tenant_distribution: HashMap\u003cString, usize\u003e,\n}\n\nimpl PriorityQueue {\n    /// Get queue statistics\n    pub async fn stats(\u0026self) -\u003e QueueStats {\n        let inner = self.inner.lock().unwrap();\n\n        let mut priority_counts = HashMap::new();\n        let mut tenant_distribution = HashMap::new();\n        let mut oldest_time = None;\n        let mut newest_time = None;\n\n        for entry in \u0026inner.queue {\n            *priority_counts.entry(entry.priority.clone()).or_insert(0) += 1;\n            *tenant_distribution\n                .entry(entry.namespace.clone())\n                .or_insert(0) += 1;\n\n            if oldest_time.is_none() || entry.enqueue_time \u003c oldest_time.unwrap() {\n                oldest_time = Some(entry.enqueue_time);\n            }\n\n            if newest_time.is_none() || entry.enqueue_time \u003e newest_time.unwrap() {\n                newest_time = Some(entry.enqueue_time);\n            }\n        }\n\n        let now = Utc::now();\n        let oldest_job_age = oldest_time.map(|t| now - t);\n        let newest_job_age = newest_time.map(|t| now - t);\n\n        QueueStats {\n            total_jobs: inner.queue.len(),\n            priority_counts,\n            oldest_job_age,\n            newest_job_age,\n            tenant_distribution,\n        }\n    }\n}\n\n/// FIFO Queue implementation (simple first-in, first-out)\n#[derive(Debug)]\npub struct FifoQueue {\n    inner: Arc\u003cMutex\u003cFifoQueueInner\u003e\u003e,\n}\n\n#[derive(Debug)]\nstruct FifoQueueInner {\n    queue: Vec\u003cQueueEntry\u003e,\n    job_map: HashMap\u003cJobId, QueueEntry\u003e,\n    config: QueueConfig,\n}\n\nimpl FifoQueue {\n    /// Create new FIFO queue\n    pub fn new(config: QueueConfig) -\u003e Self {\n        Self {\n            inner: Arc::new(Mutex::new(FifoQueueInner {\n                queue: Vec::new(),\n                job_map: HashMap::new(),\n                config,\n            })),\n        }\n    }\n\n    /// Enqueue a job\n    pub async fn enqueue(\u0026self, job: Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if inner.queue.len() \u003e= inner.config.max_size {\n            return Err(SchedulerError::QueueError(\n                \"Queue is at maximum capacity\".to_string(),\n            ));\n        }\n\n        let entry = QueueEntry {\n            job_id: job.metadata.id,\n            priority: job.spec.priority,\n            enqueue_time: job.metadata.created_at,\n            name: job.metadata.name.clone(),\n            namespace: job.metadata.namespace.clone(),\n        };\n\n        inner.queue.push(entry.clone());\n        inner.job_map.insert(job.metadata.id, entry);\n\n        debug!(\"FIFO: Enqueued job {}\", job.metadata.id);\n\n        Ok(())\n    }\n\n    /// Dequeue the first job (FIFO order)\n    pub async fn dequeue(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if let Some(entry) = inner.queue.first().cloned() {\n            inner.queue.remove(0);\n            inner.job_map.remove(\u0026entry.job_id);\n\n            debug!(\"FIFO: Dequeued job {}\", entry.job_id);\n\n            Some(entry.job_id)\n        } else {\n            None\n        }\n    }\n\n    /// Check if job is in queue\n    pub async fn contains(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        let inner = self.inner.lock().unwrap();\n        inner.job_map.contains_key(job_id)\n    }\n\n    /// Get pending job count\n    pub async fn pending_count(\u0026self) -\u003e usize {\n        let inner = self.inner.lock().unwrap();\n        inner.queue.len()\n    }\n\n    /// Cancel a job\n    pub async fn cancel(\u0026self, job_id: \u0026JobId) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if !inner.job_map.contains_key(job_id) {\n            return Err(SchedulerError::QueueError(format!(\n                \"Job {} not found in queue\",\n                job_id\n            )));\n        }\n\n        inner.job_map.remove(job_id);\n        inner.queue.retain(|e| e.job_id != *job_id);\n\n        info!(\"FIFO: Cancelled job {}\", job_id);\n\n        Ok(())\n    }\n\n    /// Clear the queue\n    pub async fn clear(\u0026self) {\n        let mut inner = self.inner.lock().unwrap();\n        inner.queue.clear();\n        inner.job_map.clear();\n    }\n}\n\n/// Fair Queue implementation (weighted round-robin by tenant)\n#[derive(Debug)]\npub struct FairQueue {\n    inner: Arc\u003cMutex\u003cFairQueueInner\u003e\u003e,\n}\n\n#[derive(Debug)]\nstruct FairQueueInner {\n    tenant_queues: HashMap\u003cString, BinaryHeap\u003cQueueEntry\u003e\u003e,\n    current_tenant: String,\n    weights: HashMap\u003cString, u32\u003e,\n    quantum: chrono::Duration,\n    config: QueueConfig,\n}\n\nimpl FairQueue {\n    /// Create new Fair queue\n    pub fn new(config: QueueConfig) -\u003e Result\u003cSelf, SchedulerError\u003e {\n        if let QueueStrategy::Fair {\n            tenant_key,\n            weights,\n            quantum,\n        } = \u0026config.strategy\n        {\n            Ok(Self {\n                inner: Arc::new(Mutex::new(FairQueueInner {\n                    tenant_queues: HashMap::new(),\n                    current_tenant: tenant_key.clone(),\n                    weights: weights.clone(),\n                    quantum: quantum.unwrap_or_else(|| chrono::Duration::seconds(1)),\n                    config,\n                })),\n            })\n        } else {\n            Err(SchedulerError::QueueError(\n                \"FairQueue requires Fair strategy\".to_string(),\n            ))\n        }\n    }\n\n    /// Enqueue a job\n    pub async fn enqueue(\u0026self, job: Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        let total_jobs: usize = inner.tenant_queues.values().map(|q| q.len()).sum();\n        if total_jobs \u003e= inner.config.max_size {\n            return Err(SchedulerError::QueueError(\n                \"Queue is at maximum capacity\".to_string(),\n            ));\n        }\n\n        let entry = QueueEntry {\n            job_id: job.metadata.id,\n            priority: job.spec.priority,\n            enqueue_time: job.metadata.created_at,\n            name: job.metadata.name.clone(),\n            namespace: job.metadata.namespace.clone(),\n        };\n\n        let tenant_key = job.metadata.namespace.clone();\n\n        inner\n            .tenant_queues\n            .entry(tenant_key.clone())\n            .or_insert_with(BinaryHeap::new)\n            .push(entry);\n\n        info!(\n            \"FairQueue: Enqueued job {} to tenant {}\",\n            job.metadata.id, tenant_key\n        );\n\n        Ok(())\n    }\n\n    /// Dequeue using weighted round-robin\n    pub async fn dequeue(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if inner.tenant_queues.is_empty() {\n            return None;\n        }\n\n        // Get sorted tenants by weight and dequeued count\n        let mut tenants: Vec\u003c_\u003e = inner.tenant_queues.keys().cloned().collect();\n        tenants.sort_by(|a, b| {\n            let weight_a = inner.weights.get(a).unwrap_or(\u00261);\n            let weight_b = inner.weights.get(b).unwrap_or(\u00261);\n            weight_b.cmp(weight_a)\n        });\n\n        // Try to dequeue from each tenant in order\n        for tenant in tenants.iter() {\n            if let Some(queue) = inner.tenant_queues.get_mut(tenant) {\n                if let Some(entry) = queue.pop() {\n                    debug!(\n                        \"FairQueue: Dequeued job {} from tenant {}\",\n                        entry.job_id, tenant\n                    );\n\n                    // Update current tenant for next round\n                    inner.current_tenant = tenant.clone();\n\n                    return Some(entry.job_id);\n                }\n            }\n        }\n\n        None\n    }\n\n    /// Get pending job count\n    pub async fn pending_count(\u0026self) -\u003e usize {\n        let inner = self.inner.lock().unwrap();\n        inner.tenant_queues.values().map(|q| q.len()).sum()\n    }\n\n    /// Get queue statistics\n    pub async fn stats(\u0026self) -\u003e QueueStats {\n        let inner = self.inner.lock().unwrap();\n\n        let mut tenant_distribution = HashMap::new();\n\n        for (tenant, queue) in \u0026inner.tenant_queues {\n            tenant_distribution.insert(tenant.clone(), queue.len());\n\n            // Also calculate priority distribution\n            for entry in queue.iter() {\n                // Note: We could calculate priority_counts here too if needed\n            }\n        }\n\n        QueueStats {\n            total_jobs: inner.tenant_queues.values().map(|q| q.len()).sum(),\n            priority_counts: HashMap::new(), // Not calculated for FairQueue\n            oldest_job_age: None,\n            newest_job_age: None,\n            tenant_distribution,\n        }\n    }\n\n    /// Clear the queue\n    pub async fn clear(\u0026self) {\n        let mut inner = self.inner.lock().unwrap();\n        inner.tenant_queues.clear();\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::{Job, JobMetadata, JobSpec, ResourceRequirements};\n\n    fn create_test_job(id: u32, priority: JobPriority) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::from_u128(id as u128),\n                name: format!(\"test-job-{}\", id),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(4_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_queue_enqueue_dequeue() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        let job1 = create_test_job(1, JobPriority::Medium);\n        let job2 = create_test_job(2, JobPriority::High);\n\n        queue.enqueue(job1.clone()).await.unwrap();\n        queue.enqueue(job2.clone()).await.unwrap();\n\n        assert_eq!(queue.pending_count().await, 2);\n\n        // High priority job should be dequeued first\n        let first = queue.dequeue().await;\n        assert_eq!(first, Some(job2.metadata.id));\n\n        let second = queue.dequeue().await;\n        assert_eq!(second, Some(job1.metadata.id));\n    }\n\n    #[tokio::test]\n    async fn test_queue_priority_ordering() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        // Add jobs in random order\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::High))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(3, JobPriority::Medium))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(4, JobPriority::Critical))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(5, JobPriority::Batch))\n            .await\n            .unwrap();\n\n        assert_eq!(queue.pending_count().await, 5);\n\n        // Dequeue and verify priority order\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(4)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(2)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(3)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(1)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(5)));\n    }\n\n    #[tokio::test]\n    async fn test_queue_cancel() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        let job1 = create_test_job(1, JobPriority::Medium);\n        queue.enqueue(job1.clone()).await.unwrap();\n\n        assert!(queue.contains(\u0026job1.metadata.id).await);\n\n        queue.cancel(\u0026job1.metadata.id).await.unwrap();\n\n        assert!(!queue.contains(\u0026job1.metadata.id).await);\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_queue_position() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        let job1 = create_test_job(1, JobPriority::Low);\n        let job2 = create_test_job(2, JobPriority::High);\n\n        queue.enqueue(job1.clone()).await.unwrap();\n        queue.enqueue(job2.clone()).await.unwrap();\n\n        // Job 2 (High) should be before job1 (Low) in priority queue\n        // High priority jobs come first regardless of insertion order\n        assert!(queue.position(\u0026job2.metadata.id).await.is_some());\n        assert!(queue.position(\u0026job1.metadata.id).await.is_some());\n\n        // Both jobs should be in the queue\n        assert!(queue.contains(\u0026job1.metadata.id).await);\n        assert!(queue.contains(\u0026job2.metadata.id).await);\n    }\n\n    #[tokio::test]\n    async fn test_preemption() {\n        let config = QueueConfig {\n            strategy: QueueStrategy::Priority {\n                with_preemption: true,\n                max_queue_time: None,\n            },\n            ..Default::default()\n        };\n        let queue = PriorityQueue::new(config);\n\n        // Add low priority jobs\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::Medium))\n            .await\n            .unwrap();\n\n        // Add high priority job that should trigger preemption\n        let high_job = create_test_job(3, JobPriority::High);\n        let preempted = queue.preempt(\u0026high_job).await.unwrap();\n\n        // High priority job should preempt lower priority jobs\n        assert!(!preempted.is_empty());\n\n        // Check that high priority job is now in queue\n        assert!(queue.contains(\u0026high_job.metadata.id).await);\n    }\n\n    #[tokio::test]\n    async fn test_queue_stats() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::High))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(3, JobPriority::Medium))\n            .await\n            .unwrap();\n\n        let stats = queue.stats().await;\n\n        assert_eq!(stats.total_jobs, 3);\n        assert_eq!(stats.priority_counts[\u0026JobPriority::Low], 1);\n        assert_eq!(stats.priority_counts[\u0026JobPriority::Medium], 1);\n        assert_eq!(stats.priority_counts[\u0026JobPriority::High], 1);\n    }\n\n    #[tokio::test]\n    async fn test_queue_capacity() {\n        let config = QueueConfig {\n            max_size: 2,\n            strategy: QueueStrategy::default(),\n            ..Default::default()\n        };\n        let queue = PriorityQueue::new(config);\n\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::Medium))\n            .await\n            .unwrap();\n\n        // Third enqueue should fail\n        let result = queue.enqueue(create_test_job(3, JobPriority::High)).await;\n\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_fifo_queue_basic() {\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fifo,\n            ..Default::default()\n        };\n        let queue = FifoQueue::new(config);\n\n        // Enqueue jobs in specific order\n        let job1 = create_test_job(1, JobPriority::Medium);\n        let job2 = create_test_job(2, JobPriority::High);\n        let job3 = create_test_job(3, JobPriority::Low);\n\n        queue.enqueue(job1).await.unwrap();\n        queue.enqueue(job2).await.unwrap();\n        queue.enqueue(job3).await.unwrap();\n\n        // FIFO should return jobs in insertion order\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(1)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(2)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(3)));\n\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_fifo_queue_cancel() {\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fifo,\n            ..Default::default()\n        };\n        let queue = FifoQueue::new(config);\n\n        let job1 = create_test_job(1, JobPriority::Medium);\n        queue.enqueue(job1.clone()).await.unwrap();\n\n        assert!(queue.contains(\u0026job1.metadata.id).await);\n\n        queue.cancel(\u0026job1.metadata.id).await.unwrap();\n\n        assert!(!queue.contains(\u0026job1.metadata.id).await);\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_fair_queue_basic() {\n        let mut weights = HashMap::new();\n        weights.insert(\"tenant-a\".to_string(), 2);\n        weights.insert(\"tenant-b\".to_string(), 1);\n\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fair {\n                tenant_key: \"namespace\".to_string(),\n                weights: weights.clone(),\n                quantum: None,\n            },\n            ..Default::default()\n        };\n\n        let queue = FairQueue::new(config).unwrap();\n\n        // Create jobs with different tenants\n        let mut job1 = create_test_job(1, JobPriority::Medium);\n        job1.metadata.namespace = \"tenant-a\".to_string();\n        queue.enqueue(job1).await.unwrap();\n\n        let mut job2 = create_test_job(2, JobPriority::High);\n        job2.metadata.namespace = \"tenant-b\".to_string();\n        queue.enqueue(job2).await.unwrap();\n\n        let mut job3 = create_test_job(3, JobPriority::Low);\n        job3.metadata.namespace = \"tenant-a\".to_string();\n        queue.enqueue(job3).await.unwrap();\n\n        // Fair queue should alternate between tenants\n        // tenant-a has weight 2, so should get more jobs\n        let first = queue.dequeue().await;\n        assert!(first.is_some());\n\n        let second = queue.dequeue().await;\n        assert!(second.is_some());\n\n        let third = queue.dequeue().await;\n        assert!(third.is_some());\n\n        // All jobs should be dequeued\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_fair_queue_stats() {\n        let mut weights = HashMap::new();\n        weights.insert(\"tenant-a\".to_string(), 2);\n        weights.insert(\"tenant-b\".to_string(), 1);\n\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fair {\n                tenant_key: \"namespace\".to_string(),\n                weights: weights.clone(),\n                quantum: None,\n            },\n            ..Default::default()\n        };\n\n        let queue = FairQueue::new(config).unwrap();\n\n        // Create jobs with different tenants\n        for i in 1..=3 {\n            let mut job = create_test_job(i, JobPriority::Medium);\n            job.metadata.namespace = \"tenant-a\".to_string();\n            queue.enqueue(job).await.unwrap();\n        }\n\n        for i in 4..=5 {\n            let mut job = create_test_job(i, JobPriority::Medium);\n            job.metadata.namespace = \"tenant-b\".to_string();\n            queue.enqueue(job).await.unwrap();\n        }\n\n        let stats = queue.stats().await;\n\n        assert_eq!(stats.total_jobs, 5);\n        assert_eq!(stats.tenant_distribution[\"tenant-a\"], 3);\n        assert_eq!(stats.tenant_distribution[\"tenant-b\"], 2);\n    }\n\n    #[tokio::test]\n    async fn test_queue_strategy_different_types() {\n        // Test Priority Queue\n        let priority_config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Priority {\n                with_preemption: true,\n                max_queue_time: Some(chrono::Duration::minutes(5)),\n            },\n            ..Default::default()\n        };\n        let priority_queue = PriorityQueue::new(priority_config);\n        assert!(priority_queue.pending_count().await == 0);\n\n        // Test FIFO Queue\n        let fifo_config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fifo,\n            ..Default::default()\n        };\n        let fifo_queue = FifoQueue::new(fifo_config);\n        assert!(fifo_queue.pending_count().await == 0);\n\n        // Test Fair Queue\n        let mut weights = HashMap::new();\n        weights.insert(\"tenant-1\".to_string(), 1);\n\n        let fair_config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fair {\n                tenant_key: \"namespace\".to_string(),\n                weights: weights.clone(),\n                quantum: None,\n            },\n            ..Default::default()\n        };\n        let fair_queue = FairQueue::new(fair_config).unwrap();\n        assert!(fair_queue.pending_count().await == 0);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","selection","mod.rs"],"content":"//! Worker Selection Algorithms Module\n//!\n//! This module implements various algorithms for selecting the best worker\n//! for a job based on different criteria and optimization goals.\n\nuse crate::SchedulerError;\nuse crate::backend::ComputeResource;\nuse crate::types::*;\nuse std::collections::HashMap;\n\n/// Worker selection strategy\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SelectionStrategy {\n    /// Select worker with lowest current load\n    LeastLoaded,\n\n    /// Select worker that balances resources across cluster\n    ResourceBalance,\n\n    /// Pack jobs efficiently to minimize fragmentation\n    BinPacking,\n\n    /// Distribute jobs evenly across workers (Round Robin)\n    RoundRobin,\n\n    /// Custom strategy with plugin support\n    Custom(String),\n}\n\n/// Selection criteria configuration\n#[derive(Debug, Clone)]\npub struct SelectionCriteria {\n    pub resource_weights: ResourceWeights,\n    pub balance_threshold: f64, // How balanced resources should be\n    pub locality_aware: bool,   // Prefer local workers\n}\n\n/// Resource weights for scoring\n#[derive(Debug, Clone)]\npub struct ResourceWeights {\n    pub cpu_weight: f64,\n    pub memory_weight: f64,\n    pub gpu_weight: f64,\n    pub network_weight: f64,\n}\n\nimpl Default for SelectionCriteria {\n    fn default() -\u003e Self {\n        Self {\n            resource_weights: ResourceWeights {\n                cpu_weight: 1.0,\n                memory_weight: 1.0,\n                gpu_weight: 2.0, // GPUs are more critical\n                network_weight: 0.5,\n            },\n            balance_threshold: 0.8, // 80% balance threshold\n            locality_aware: false,\n        }\n    }\n}\n\n/// Selection result\n#[derive(Debug, Clone)]\npub struct SelectionResult {\n    pub selected_worker: WorkerNode,\n    pub score: f64,\n    pub reason: String,\n}\n\n/// Worker Selection Engine\npub struct WorkerSelector {\n    strategy: SelectionStrategy,\n    criteria: SelectionCriteria,\n    round_robin_index: usize,\n}\n\nimpl WorkerSelector {\n    /// Create new worker selector\n    pub fn new(mut strategy: SelectionStrategy, criteria: SelectionCriteria) -\u003e Self {\n        // For Round Robin, we don't need mutable state in the selector\n        // The index can be managed externally if needed\n        let round_robin_index = 0;\n\n        Self {\n            strategy,\n            criteria,\n            round_robin_index,\n        }\n    }\n\n    /// Select best worker from available workers\n    pub fn select_worker(\n        \u0026self,\n        job: \u0026Job,\n        mut workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        if workers.is_empty() {\n            return Err(SchedulerError::SelectionError(\n                \"No workers available for selection\".to_string(),\n            ));\n        }\n\n        match self.strategy {\n            SelectionStrategy::LeastLoaded =\u003e self.select_least_loaded(job, workers),\n            SelectionStrategy::ResourceBalance =\u003e self.select_resource_balance(job, workers),\n            SelectionStrategy::BinPacking =\u003e self.select_bin_packing(job, workers),\n            SelectionStrategy::RoundRobin =\u003e {\n                // For round robin, shuffle workers deterministically\n                workers.sort_by(|a, b| a.labels.get(\"name\").cmp(\u0026b.labels.get(\"name\")));\n                self.select_round_robin(workers)\n            }\n            SelectionStrategy::Custom(_) =\u003e {\n                // For now, fallback to Least Loaded\n                // TODO: Implement plugin-based custom strategies\n                self.select_least_loaded(job, workers)\n            }\n        }\n    }\n\n    /// Least Loaded: Select worker with lowest current load\n    fn select_least_loaded(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        let mut best_worker = None;\n        let mut best_load = f64::MAX;\n\n        for worker in workers.iter() {\n            let load = self.calculate_load(\u0026worker);\n\n            // Check if worker can fit the job\n            if self.can_fit_job(job, \u0026worker) \u0026\u0026 load \u003c best_load {\n                best_load = load;\n                best_worker = Some(worker);\n            }\n        }\n\n        if let Some(worker) = best_worker {\n            Ok(SelectionResult {\n                selected_worker: worker.clone(),\n                score: 1.0 / (1.0 + best_load),\n                reason: format!(\"Lowest load: {:.2}\", best_load),\n            })\n        } else {\n            Err(SchedulerError::SelectionError(\n                \"No worker can accommodate the job\".to_string(),\n            ))\n        }\n    }\n\n    /// Resource Balance: Select worker that best balances cluster resources\n    fn select_resource_balance(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        let mut best_worker = None;\n        let mut best_score = f64::MIN;\n\n        for worker in workers.iter() {\n            // Check if worker can fit the job\n            if !self.can_fit_job(job, \u0026worker) {\n                continue;\n            }\n\n            let balance_score = self.calculate_balance_score(\u0026workers, \u0026worker);\n\n            if balance_score \u003e best_score {\n                best_score = balance_score;\n                best_worker = Some(worker);\n            }\n        }\n\n        if let Some(worker) = best_worker {\n            Ok(SelectionResult {\n                selected_worker: worker.clone(),\n                score: best_score,\n                reason: format!(\"Best balance score: {:.2}\", best_score),\n            })\n        } else {\n            Err(SchedulerError::SelectionError(\n                \"No worker can accommodate the job\".to_string(),\n            ))\n        }\n    }\n\n    /// Bin Packing: Efficiently pack jobs to minimize fragmentation\n    fn select_bin_packing(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        let mut best_worker = None;\n        let mut best_fit_score = f64::MAX;\n\n        for worker in workers.iter() {\n            // Check if worker can fit the job\n            if !self.can_fit_job(job, \u0026worker) {\n                continue;\n            }\n\n            // Calculate fit score: how well the job fits\n            let fit_score = self.calculate_fit_score(job, \u0026worker);\n\n            // Lower is better (First Fit Decreasing)\n            if fit_score \u003c best_fit_score {\n                best_fit_score = fit_score;\n                best_worker = Some(worker);\n            }\n        }\n\n        if let Some(worker) = best_worker {\n            Ok(SelectionResult {\n                selected_worker: worker.clone(),\n                score: 1.0 / (1.0 + best_fit_score),\n                reason: format!(\"Best fit score: {:.2}\", best_fit_score),\n            })\n        } else {\n            Err(SchedulerError::SelectionError(\n                \"No worker can accommodate the job\".to_string(),\n            ))\n        }\n    }\n\n    /// Round Robin: Distribute jobs evenly across workers\n    fn select_round_robin(\n        \u0026self,\n        mut workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        // Sort workers by name for deterministic round-robin\n        workers.sort_by(|a, b| a.labels.get(\"name\").cmp(\u0026b.labels.get(\"name\")));\n\n        // Select first worker (deterministic)\n        let worker = workers.first().cloned().unwrap();\n\n        Ok(SelectionResult {\n            selected_worker: worker,\n            score: 1.0,\n            reason: \"Round Robin selection\".to_string(),\n        })\n    }\n\n    /// Calculate current load of a worker\n    fn calculate_load(\u0026self, worker: \u0026WorkerNode) -\u003e f64 {\n        let resource = \u0026worker.resources;\n        // Calculate load as a simple utilization metric\n        // Since we don't have used/available breakdown, assume some baseline utilization\n        // For this example, we'll use a heuristic based on cpu and memory\n        let cpu_utilization = if resource.cpu_cores \u003e 0.0 {\n            (resource.cpu_cores / 100.0).min(1.0) // Normalize\n        } else {\n            0.0\n        };\n\n        let memory_utilization = if resource.memory_bytes \u003e 0 {\n            ((resource.memory_bytes as f64 / 1_000_000_000.0) / 100.0).min(1.0) // Normalize GB to 0-1\n        } else {\n            0.0\n        };\n\n        // Weighted average\n        (cpu_utilization * self.criteria.resource_weights.cpu_weight\n            + memory_utilization * self.criteria.resource_weights.memory_weight)\n            / (self.criteria.resource_weights.cpu_weight\n                + self.criteria.resource_weights.memory_weight)\n    }\n\n    /// Check if worker can fit the job's resource requirements\n    fn can_fit_job(\u0026self, job: \u0026Job, worker: \u0026WorkerNode) -\u003e bool {\n        if let Some(requirements) = \u0026job.spec.resource_requirements {\n            let resource = \u0026worker.resources;\n            // Check CPU\n            if let Some(cpu_required) = requirements.cpu_cores {\n                if resource.cpu_cores \u003c cpu_required {\n                    return false;\n                }\n            }\n\n            // Check Memory\n            if let Some(memory_required) = requirements.memory_bytes {\n                if resource.memory_bytes \u003c memory_required {\n                    return false;\n                }\n            }\n\n            // Check GPU\n            if let Some(gpu_required) = requirements.gpu_count {\n                if resource.gpu_count \u003c gpu_required {\n                    return false;\n                }\n            }\n\n            true\n        } else {\n            true // No resource requirements\n        }\n    }\n\n    /// Calculate balance score for resource balance strategy\n    fn calculate_balance_score(\u0026self, workers: \u0026Vec\u003cWorkerNode\u003e, candidate: \u0026WorkerNode) -\u003e f64 {\n        let resource = \u0026candidate.resources;\n\n        // Simple balance: check how well the candidate's resources compare to cluster average\n        let avg_cpu: f64 =\n            workers.iter().map(|w| w.resources.cpu_cores).sum::\u003cf64\u003e() / workers.len() as f64;\n        let avg_memory: f64 = workers\n            .iter()\n            .map(|w| w.resources.memory_bytes as f64)\n            .sum::\u003cf64\u003e()\n            / workers.len() as f64;\n\n        let cpu_deviation = (resource.cpu_cores - avg_cpu).abs() / avg_cpu;\n        let memory_deviation = ((resource.memory_bytes as f64 - avg_memory) / avg_memory).abs();\n\n        // Lower deviation is better (negative because we want to maximize)\n        -(cpu_deviation + memory_deviation) / 2.0\n    }\n\n    /// Calculate fit score for bin packing strategy\n    fn calculate_fit_score(\u0026self, job: \u0026Job, worker: \u0026WorkerNode) -\u003e f64 {\n        if let Some(requirements) = \u0026job.spec.resource_requirements {\n            let resource = \u0026worker.resources;\n            let mut fit_score = 0.0;\n\n            // CPU fit: how well the job fits in available CPU\n            if let Some(cpu_required) = requirements.cpu_cores {\n                fit_score += (resource.cpu_cores - cpu_required) / resource.cpu_cores;\n            }\n\n            // Memory fit\n            if let Some(memory_required) = requirements.memory_bytes {\n                let memory_ratio = memory_required as f64 / resource.memory_bytes as f64;\n                fit_score += 1.0 - memory_ratio; // Lower ratio is better (more space left)\n            }\n\n            fit_score\n        } else {\n            0.0\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::{BackendSpecific, JobMetadata, NodeLocation};\n\n    fn create_test_worker(\n        id: \u0026str,\n        cpu_cores: f64,\n        memory_bytes: u64,\n        _used_cpu: f64,\n        _used_memory: u64,\n    ) -\u003e WorkerNode {\n        WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: BackendType::Kubernetes,\n            status: WorkerStatus::Running,\n            resources: ComputeResource {\n                cpu_cores,\n                memory_bytes,\n                gpu_count: 0,\n            },\n            labels: {\n                let mut labels = HashMap::new();\n                labels.insert(\"name\".to_string(), format!(\"worker-{}\", id));\n                labels\n            },\n            taints: vec![],\n            backend_specific: BackendSpecific::Kubernetes(KubernetesNodeSpecific {\n                node_name: format!(\"worker-{}\", id),\n                namespace: \"default\".to_string(),\n            }),\n            location: NodeLocation::default(),\n        }\n    }\n\n    fn create_test_job(cpu_cores: f64, memory_bytes: u64) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(cpu_cores),\n                    memory_bytes: Some(memory_bytes),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_least_loaded_strategy() {\n        let mut selector =\n            WorkerSelector::new(SelectionStrategy::LeastLoaded, SelectionCriteria::default());\n\n        let workers = vec![\n            create_test_worker(\"1\", 8.0, 8_000_000_000, 6.0, 6_000_000_000), // 75% load\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 2.0, 2_000_000_000), // 25% load (best)\n            create_test_worker(\"3\", 8.0, 8_000_000_000, 4.0, 4_000_000_000), // 50% load\n        ];\n\n        let job = create_test_job(1.0, 1_000_000_000);\n\n        let result = selector.select_worker(\u0026job, workers).unwrap();\n\n        // Should select a valid worker\n        assert!(result.selected_worker.labels.get(\"name\").is_some());\n        assert!(result.score \u003e 0.0);\n    }\n\n    #[tokio::test]\n    async fn test_resource_balance_strategy() {\n        let selector = WorkerSelector::new(\n            SelectionStrategy::ResourceBalance,\n            SelectionCriteria::default(),\n        );\n\n        let workers = vec![\n            create_test_worker(\"1\", 8.0, 8_000_000_000, 2.0, 6_000_000_000), // CPU: 25%, Mem: 75% (imbalanced)\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 4.0, 4_000_000_000), // CPU: 50%, Mem: 50% (balanced)\n            create_test_worker(\"3\", 8.0, 8_000_000_000, 6.0, 2_000_000_000), // CPU: 75%, Mem: 25% (imbalanced)\n        ];\n\n        let job = create_test_job(1.0, 1_000_000_000);\n\n        let result = selector.select_worker(\u0026job, workers).unwrap();\n\n        // Should select a valid worker\n        assert!(result.selected_worker.labels.get(\"name\").is_some());\n    }\n\n    #[tokio::test]\n    async fn test_bin_packing_strategy() {\n        let selector =\n            WorkerSelector::new(SelectionStrategy::BinPacking, SelectionCriteria::default());\n\n        let workers = vec![\n            create_test_worker(\"1\", 16.0, 16_000_000_000, 8.0, 8_000_000_000), // Half full\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 2.0, 2_000_000_000), // Quarter full (best fit)\n            create_test_worker(\"3\", 4.0, 4_000_000_000, 1.0, 1_000_000_000), // Quarter full\n        ];\n\n        let job = create_test_job(2.0, 2_000_000_000);\n\n        let result = selector.select_worker(\u0026job, workers).unwrap();\n\n        // Should select a well-fitting worker\n        assert!(result.score \u003e 0.0);\n    }\n\n    #[tokio::test]\n    async fn test_round_robin_strategy() {\n        let mut selector =\n            WorkerSelector::new(SelectionStrategy::RoundRobin, SelectionCriteria::default());\n\n        let workers = vec![\n            create_test_worker(\"1\", 8.0, 8_000_000_000, 0.0, 0),\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 0.0, 0),\n            create_test_worker(\"3\", 8.0, 8_000_000_000, 0.0, 0),\n        ];\n\n        // All selections should return the same worker (deterministic)\n        let result1 = selector\n            .select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers.clone())\n            .unwrap();\n        assert_eq!(\n            result1.selected_worker.labels.get(\"name\"),\n            Some(\u0026\"worker-1\".to_string())\n        );\n\n        let result2 = selector\n            .select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers.clone())\n            .unwrap();\n        assert_eq!(\n            result2.selected_worker.labels.get(\"name\"),\n            Some(\u0026\"worker-1\".to_string())\n        );\n\n        let result3 = selector\n            .select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers.clone())\n            .unwrap();\n        assert_eq!(\n            result3.selected_worker.labels.get(\"name\"),\n            Some(\u0026\"worker-1\".to_string())\n        );\n    }\n\n    #[tokio::test]\n    async fn test_selection_with_insufficient_resources() {\n        let selector =\n            WorkerSelector::new(SelectionStrategy::LeastLoaded, SelectionCriteria::default());\n\n        // Workers with insufficient resources\n        let workers = vec![\n            create_test_worker(\"1\", 2.0, 2_000_000_000, 1.5, 1_500_000_000), // Can't fit 4 CPU job\n        ];\n\n        let job = create_test_job(4.0, 4_000_000_000); // Requires more than available\n\n        let result = selector.select_worker(\u0026job, workers);\n\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_no_workers_available() {\n        let selector =\n            WorkerSelector::new(SelectionStrategy::LeastLoaded, SelectionCriteria::default());\n\n        let workers = vec![]; // No workers\n\n        let result = selector.select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers);\n\n        assert!(result.is_err());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","types","mod.rs"],"content":"//! Core Types and Data Structures for Scheduler\n//!\n//! This module defines all the core data types used by the scheduler framework,\n//! including jobs, workers, priorities, affinity rules, taints, etc.\n\nuse chrono::{DateTime, Utc};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n/// Job priority levels (higher value = higher priority)\n#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize)]\npub enum JobPriority {\n    Batch = 1,    // Low-priority batch processing\n    Low = 2,      // Background jobs\n    Medium = 3,   // Regular CI/CD jobs\n    High = 4,     // Production deployments\n    Critical = 5, // System critical jobs\n}\n\nimpl JobPriority {\n    /// Check if this priority can preempt another\n    pub fn can_preempt(\u0026self, other: \u0026JobPriority) -\u003e bool {\n        self \u003e other\n    }\n\n    /// Get numeric value\n    pub fn value(\u0026self) -\u003e u32 {\n        match self {\n            JobPriority::Critical =\u003e 5,\n            JobPriority::High =\u003e 4,\n            JobPriority::Medium =\u003e 3,\n            JobPriority::Low =\u003e 2,\n            JobPriority::Batch =\u003e 1,\n        }\n    }\n}\n\nimpl std::fmt::Display for JobPriority {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            JobPriority::Critical =\u003e write!(f, \"Critical\"),\n            JobPriority::High =\u003e write!(f, \"High\"),\n            JobPriority::Medium =\u003e write!(f, \"Medium\"),\n            JobPriority::Low =\u003e write!(f, \"Low\"),\n            JobPriority::Batch =\u003e write!(f, \"Batch\"),\n        }\n    }\n}\n\n/// Backend types supported by the scheduler\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub enum BackendType {\n    Kubernetes,\n    Docker,\n    CloudVm,\n    BareMetal,\n    Serverless,\n    Hpc,\n}\n\nimpl std::fmt::Display for BackendType {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            BackendType::Kubernetes =\u003e write!(f, \"Kubernetes\"),\n            BackendType::Docker =\u003e write!(f, \"Docker\"),\n            BackendType::CloudVm =\u003e write!(f, \"Cloud VM\"),\n            BackendType::BareMetal =\u003e write!(f, \"Bare Metal\"),\n            BackendType::Serverless =\u003e write!(f, \"Serverless\"),\n            BackendType::Hpc =\u003e write!(f, \"HPC\"),\n        }\n    }\n}\n\n/// Worker node status\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum WorkerStatus {\n    Pending,  // Node is starting up\n    Ready,    // Node is ready to accept jobs\n    Running,  // Node is running jobs\n    Draining, // Node is being drained (no new jobs)\n    Offline,  // Node is offline\n    Failed,   // Node has failed\n}\n\nimpl WorkerStatus {\n    pub fn is_available(\u0026self) -\u003e bool {\n        matches!(self, WorkerStatus::Ready | WorkerStatus::Running)\n    }\n}\n\n/// Job definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Job {\n    pub metadata: JobMetadata,\n    pub spec: JobSpec,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct JobMetadata {\n    pub id: super::JobId,\n    pub name: String,\n    pub namespace: String,\n    pub labels: HashMap\u003cString, String\u003e,\n    pub created_at: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct JobSpec {\n    pub resource_requirements: Option\u003cResourceRequirements\u003e,\n    pub priority: JobPriority,\n    pub node_selector: Option\u003cNodeSelector\u003e,\n    pub affinity: Option\u003cNodeAffinity\u003e,\n    pub tolerations: Vec\u003cToleration\u003e,\n    pub max_retries: u32,\n}\n\n/// Resource requirements for a job\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct ResourceRequirements {\n    pub cpu_cores: Option\u003cf64\u003e,\n    pub memory_bytes: Option\u003cu64\u003e,\n    pub gpu_count: Option\u003cu32\u003e,\n    pub ephemeral_storage: Option\u003cu64\u003e,\n}\n\nimpl ResourceRequirements {\n    /// Check if requirements are valid\n    pub fn is_valid(\u0026self) -\u003e bool {\n        if let Some(cpu) = self.cpu_cores {\n            if cpu \u003c= 0.0 {\n                return false;\n            }\n        }\n\n        if let Some(memory) = self.memory_bytes {\n            if memory == 0 {\n                return false;\n            }\n        }\n\n        if let Some(gpu) = self.gpu_count {\n            if gpu == 0 {\n                return false;\n            }\n        }\n\n        true\n    }\n}\n\n/// Node selector for job placement\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct NodeSelector {\n    pub labels: HashMap\u003cString, String\u003e,\n}\n\n/// Node affinity rules\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct NodeAffinity {\n    pub required_during_scheduling: Vec\u003cLabelSelector\u003e,\n    pub preferred_during_scheduling: Vec\u003cWeightedLabelSelector\u003e,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct LabelSelector {\n    pub key: String,\n    pub operator: LabelSelectorOperator,\n    pub values: Option\u003cVec\u003cString\u003e\u003e,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct WeightedLabelSelector {\n    pub selector: LabelSelector,\n    pub weight: i32,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum LabelSelectorOperator {\n    In,\n    NotIn,\n    Exists,\n    DoesNotExist,\n}\n\n/// Taints for node dedication\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct Taint {\n    pub key: String,\n    pub operator: TaintOperator,\n    pub value: String,\n    pub effect: TaintEffect,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum TaintOperator {\n    Equal,\n    NotEqual,\n    Exists,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum TaintEffect {\n    NoSchedule,\n    PreferNoSchedule,\n    NoExecute,\n}\n\n/// Tolerations to override taints\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct Toleration {\n    pub key: String,\n    pub operator: TaintOperator,\n    pub value: String,\n    pub effect: TaintEffect,\n    pub toleration_seconds: Option\u003ci64\u003e,\n}\n\n/// Backend-specific information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum BackendSpecific {\n    Kubernetes(KubernetesNodeSpecific),\n    Docker(DockerNodeSpecific),\n    CloudVm(CloudVmNodeSpecific),\n    BareMetal(BareMetalNodeSpecific),\n    Serverless(ServerlessNodeSpecific),\n    Hpc(HpcNodeSpecific),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KubernetesNodeSpecific {\n    pub node_name: String,\n    pub namespace: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DockerNodeSpecific {\n    pub host: String,\n    pub socket_path: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CloudVmNodeSpecific {\n    pub instance_id: String,\n    pub region: String,\n    pub zone: String,\n    pub instance_type: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BareMetalNodeSpecific {\n    pub hostname: String,\n    pub ip_address: String,\n    pub rack_id: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ServerlessNodeSpecific {\n    pub provider: String,\n    pub runtime: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct HpcNodeSpecific {\n    pub cluster_name: String,\n    pub partition: String,\n}\n\n/// Node location information\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct NodeLocation {\n    pub region: Option\u003cString\u003e,\n    pub zone: Option\u003cString\u003e,\n    pub datacenter: Option\u003cString\u003e,\n    pub rack: Option\u003cString\u003e,\n}\n\n/// Scoring weights for worker selection\n#[derive(Debug, Clone)]\npub struct ScoringWeights {\n    pub resource_fit: f64,\n    pub location: f64,\n    pub backend: f64,\n    pub affinity: f64,\n    pub taints: f64,\n}\n\nimpl Default for ScoringWeights {\n    fn default() -\u003e Self {\n        Self {\n            resource_fit: 0.40, // 40%\n            location: 0.25,     // 25%\n            backend: 0.15,      // 15%\n            affinity: 0.15,     // 15%\n            taints: 0.05,       // 5%\n        }\n    }\n}\n\nimpl ScoringWeights {\n    pub fn new(resource_fit: f64, location: f64, backend: f64, affinity: f64, taints: f64) -\u003e Self {\n        Self {\n            resource_fit,\n            location,\n            backend,\n            affinity,\n            taints,\n        }\n    }\n}\n\n/// Scored worker with score\n#[derive(Debug, Clone)]\npub struct ScoredWorker {\n    pub node: WorkerNode,\n    pub score: f64,\n    pub reasons: Vec\u003cString\u003e,\n}\n\nimpl ScoredWorker {\n    pub fn new(node: WorkerNode, score: f64) -\u003e Self {\n        Self {\n            node,\n            score,\n            reasons: vec![],\n        }\n    }\n\n    pub fn with_reason(mut self, reason: String) -\u003e Self {\n        self.reasons.push(reason);\n        self\n    }\n}\n\n/// Worker selection strategies\n#[derive(Debug, Clone)]\npub enum WorkerSelectionStrategy {\n    LeastLoaded,\n    ResourceBalance,\n    BinPacking,\n    RoundRobin,\n    Custom(String), // Plugin-based strategy\n}\n\n/// Worker node representation\n#[derive(Debug, Clone)]\npub struct WorkerNode {\n    pub id: super::WorkerId,\n    pub backend_type: BackendType,\n    pub status: WorkerStatus,\n    pub resources: super::backend::ComputeResource,\n    pub labels: HashMap\u003cString, String\u003e,\n    pub taints: Vec\u003cTaint\u003e,\n    pub backend_specific: BackendSpecific,\n    pub location: NodeLocation,\n}\n\nimpl WorkerNode {\n    /// Check if node matches job requirements\n    pub fn matches_requirements(\u0026self, job: \u0026Job) -\u003e bool {\n        // Check node selector\n        if let Some(selector) = \u0026job.spec.node_selector {\n            if !self.matches_node_selector(selector) {\n                return false;\n            }\n        }\n\n        // Check taints and tolerations\n        if !self.has_matching_tolerations(\u0026job.spec.tolerations) {\n            return false;\n        }\n\n        // Check affinity rules (simplified)\n        if let Some(affinity) = \u0026job.spec.affinity {\n            if !self.matches_affinity(affinity) {\n                return false;\n            }\n        }\n\n        // Check resources\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if !self.resources.has_resources(req) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node matches node selector\n    fn matches_node_selector(\u0026self, selector: \u0026NodeSelector) -\u003e bool {\n        for (key, value) in \u0026selector.labels {\n            match self.labels.get(key) {\n                Some(node_value) =\u003e {\n                    if node_value != value {\n                        return false;\n                    }\n                }\n                None =\u003e return false,\n            }\n        }\n        true\n    }\n\n    /// Check if node has matching tolerations\n    fn has_matching_tolerations(\u0026self, tolerations: \u0026[Toleration]) -\u003e bool {\n        for taint in \u0026self.taints {\n            let has_matching_toleration = tolerations.iter().any(|tol| {\n                tol.key == taint.key \u0026\u0026 tol.value == taint.value \u0026\u0026 tol.effect == taint.effect\n            });\n\n            // If there's no matching toleration and effect is NoSchedule, reject\n            if !has_matching_toleration \u0026\u0026 taint.effect == TaintEffect::NoSchedule {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node matches affinity rules (simplified)\n    fn matches_affinity(\u0026self, _affinity: \u0026NodeAffinity) -\u003e bool {\n        // Simplified: assume all nodes match for now\n        // In production: implement full affinity matching logic\n        true\n    }\n\n    /// Calculate score for this node based on job\n    pub fn calculate_score(\u0026self, job: \u0026Job, weights: \u0026ScoringWeights) -\u003e f64 {\n        let mut score = 0.0;\n\n        // Resource fit score (0-100)\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            let utilization = self.resources.utilization(req);\n            let resource_score = 100.0 * (1.0 - utilization.abs()); // Lower utilization = higher score\n            score += resource_score * weights.resource_fit;\n        }\n\n        // Location score\n        let location_score = 100.0; // Simplified: all locations equal\n        score += location_score * weights.location;\n\n        // Backend score\n        let backend_score = if self.backend_type == BackendType::Kubernetes {\n            100.0\n        } else {\n            80.0\n        };\n        score += backend_score * weights.backend;\n\n        score\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_job_priority_comparison() {\n        assert!(JobPriority::Critical \u003e JobPriority::High);\n        assert!(JobPriority::High \u003e JobPriority::Medium);\n        assert!(JobPriority::Medium \u003e JobPriority::Low);\n        assert!(JobPriority::Low \u003e JobPriority::Batch);\n    }\n\n    #[test]\n    fn test_job_priority_preemption() {\n        let critical = JobPriority::Critical;\n        let medium = JobPriority::Medium;\n\n        assert!(critical.can_preempt(\u0026medium));\n        assert!(!medium.can_preempt(\u0026critical));\n    }\n\n    #[test]\n    fn test_job_priority_value() {\n        assert_eq!(JobPriority::Critical.value(), 5);\n        assert_eq!(JobPriority::High.value(), 4);\n        assert_eq!(JobPriority::Medium.value(), 3);\n        assert_eq!(JobPriority::Low.value(), 2);\n        assert_eq!(JobPriority::Batch.value(), 1);\n    }\n\n    #[test]\n    fn test_worker_status_is_available() {\n        assert!(WorkerStatus::Ready.is_available());\n        assert!(WorkerStatus::Running.is_available());\n        assert!(!WorkerStatus::Pending.is_available());\n        assert!(!WorkerStatus::Draining.is_available());\n        assert!(!WorkerStatus::Offline.is_available());\n        assert!(!WorkerStatus::Failed.is_available());\n    }\n\n    #[test]\n    fn test_resource_requirements_is_valid() {\n        let valid = ResourceRequirements {\n            cpu_cores: Some(2.0),\n            memory_bytes: Some(4_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(valid.is_valid());\n\n        let invalid = ResourceRequirements {\n            cpu_cores: Some(0.0),\n            memory_bytes: Some(4_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(!invalid.is_valid());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","correlation.rs"],"content":"//! Correlation module for distributed tracing\n\nuse serde::{Deserialize, Serialize};\nuse crate::Uuid;\n\n/// Correlation identifier for distributed tracing\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct CorrelationId(Uuid);\n\nimpl CorrelationId {\n    pub fn new() -\u003e Self {\n        Self(Uuid::new_v4())\n    }\n\n    pub fn from_uuid(uuid: Uuid) -\u003e Self {\n        Self(uuid)\n    }\n\n    pub fn as_uuid(\u0026self) -\u003e Uuid {\n        self.0\n    }\n}\n\nimpl Default for CorrelationId {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Trace context for distributed tracing\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct TraceContext {\n    pub trace_id: Uuid,\n    pub span_id: Uuid,\n    pub parent_span_id: Option\u003cUuid\u003e,\n}\n\nimpl TraceContext {\n    pub fn new() -\u003e Self {\n        Self {\n            trace_id: Uuid::new_v4(),\n            span_id: Uuid::new_v4(),\n            parent_span_id: None,\n        }\n    }\n\n    pub fn with_parent(mut self, parent: Uuid) -\u003e Self {\n        self.parent_span_id = Some(parent);\n        self\n    }\n}\n\nimpl Default for TraceContext {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","error.rs"],"content":"//! Error types shared across the system\n\nuse thiserror::Error;\n\n/// Base error type for the entire system\n#[derive(Error, Debug)]\npub enum DomainError {\n    #[error(\"validation error: {0}\")]\n    Validation(String),\n\n    #[error(\"invalid state transition from {from} to {to}\")]\n    InvalidStateTransition { from: String, to: String },\n\n    #[error(\"resource not found: {0}\")]\n    NotFound(String),\n\n    #[error(\"concurrency error: {0}\")]\n    Concurrency(String),\n\n    #[error(\"infrastructure error: {0}\")]\n    Infrastructure(String),\n\n    #[error(\"authorization error: {0}\")]\n    Authorization(String),\n\n    #[error(\"timeout: {0}\")]\n    Timeout(String),\n}\n\nimpl DomainError {\n    pub fn invalid_state_transition(from: \u0026str, to: \u0026str) -\u003e Self {\n        Self::InvalidStateTransition {\n            from: from.to_string(),\n            to: to.to_string(),\n        }\n    }\n}\n","traces":[{"line":31,"address":[5333136,5333330,5333336],"length":1,"stats":{"Line":0}},{"line":33,"address":[5333182],"length":1,"stats":{"Line":0}},{"line":34,"address":[5333221],"length":1,"stats":{"Line":0}}],"covered":0,"coverable":3},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","health_checks.rs"],"content":"//! Health check protocols and types\n\nuse chrono::{DateTime, Utc};\nuse serde::{Deserialize, Serialize};\n\n/// Health status\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum HealthStatus {\n    Healthy,\n    Degraded,\n    Unhealthy,\n}\n\n/// Health check result\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct HealthCheck {\n    pub component: String,\n    pub status: HealthStatus,\n    pub message: Option\u003cString\u003e,\n    pub timestamp: DateTime\u003cUtc\u003e,\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","job_definitions.rs"],"content":"//! Job definition types and schemas\n\nuse crate::Uuid;\nuse crate::error::DomainError;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n/// Job identifier\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct JobId(pub Uuid);\n\nimpl JobId {\n    pub fn new() -\u003e Self {\n        Self(Uuid::new_v4())\n    }\n\n    pub fn from_uuid(uuid: Uuid) -\u003e Self {\n        Self(uuid)\n    }\n\n    pub fn as_uuid(\u0026self) -\u003e Uuid {\n        self.0\n    }\n}\n\nimpl Default for JobId {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Resource requirements for a job\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct ResourceQuota {\n    pub cpu_m: u64,      // CPU in millicores\n    pub memory_mb: u64,  // Memory in MB\n    pub gpu: Option\u003cu8\u003e, // Optional GPU requirement\n}\n\nimpl ResourceQuota {\n    pub fn new(cpu_m: u64, memory_mb: u64) -\u003e Self {\n        Self {\n            cpu_m,\n            memory_mb,\n            gpu: None,\n        }\n    }\n\n    pub fn with_gpu(mut self, gpu: u8) -\u003e Self {\n        self.gpu = Some(gpu);\n        self\n    }\n}\n\n/// Job specification (immutable value object)\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct JobSpec {\n    pub name: String,\n    pub image: String,\n    pub command: Vec\u003cString\u003e,\n    pub resources: ResourceQuota,\n    pub timeout_ms: u64,\n    pub retries: u8,\n    pub env: HashMap\u003cString, String\u003e,\n    pub secret_refs: Vec\u003cString\u003e,\n}\n\nimpl JobSpec {\n    pub fn validate(\u0026self) -\u003e Result\u003c(), DomainError\u003e {\n        if self.name.trim().is_empty() {\n            return Err(DomainError::Validation(\n                \"job name cannot be empty\".to_string(),\n            ));\n        }\n\n        if self.image.trim().is_empty() {\n            return Err(DomainError::Validation(\"image cannot be empty\".to_string()));\n        }\n\n        if self.command.is_empty() {\n            return Err(DomainError::Validation(\n                \"command cannot be empty\".to_string(),\n            ));\n        }\n\n        if self.timeout_ms == 0 {\n            return Err(DomainError::Validation(\n                \"timeout must be greater than 0\".to_string(),\n            ));\n        }\n\n        Ok(())\n    }\n}\n\n/// Job state value object\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct JobState(String);\n\nimpl JobState {\n    pub const PENDING: \u0026'static str = \"PENDING\";\n    pub const SCHEDULED: \u0026'static str = \"SCHEDULED\";\n    pub const RUNNING: \u0026'static str = \"RUNNING\";\n    pub const SUCCESS: \u0026'static str = \"SUCCESS\";\n    pub const FAILED: \u0026'static str = \"FAILED\";\n    pub const CANCELLED: \u0026'static str = \"CANCELLED\";\n\n    pub fn new(state: String) -\u003e Result\u003cSelf, DomainError\u003e {\n        match state.as_str() {\n            Self::PENDING\n            | Self::SCHEDULED\n            | Self::RUNNING\n            | Self::SUCCESS\n            | Self::FAILED\n            | Self::CANCELLED =\u003e Ok(Self(state)),\n            _ =\u003e Err(DomainError::Validation(format!(\n                \"invalid job state: {}\",\n                state\n            ))),\n        }\n    }\n\n    pub fn can_transition_to(\u0026self, target: \u0026Self) -\u003e bool {\n        match (self.0.as_str(), target.0.as_str()) {\n            (Self::PENDING, Self::SCHEDULED) =\u003e true,\n            (Self::PENDING, Self::CANCELLED) =\u003e true,\n            (Self::SCHEDULED, Self::RUNNING) =\u003e true,\n            (Self::SCHEDULED, Self::CANCELLED) =\u003e true,\n            (Self::RUNNING, Self::SUCCESS) =\u003e true,\n            (Self::RUNNING, Self::FAILED) =\u003e true,\n            (Self::RUNNING, Self::CANCELLED) =\u003e true,\n            (Self::FAILED, Self::PENDING) =\u003e true, // For retry\n            (Self::FAILED, Self::CANCELLED) =\u003e true,\n            _ =\u003e false,\n        }\n    }\n\n    pub fn as_str(\u0026self) -\u003e \u0026str {\n        \u0026self.0\n    }\n}\n\nimpl From\u003cString\u003e for JobState {\n    fn from(s: String) -\u003e Self {\n        Self::new(s).expect(\"valid state\")\n    }\n}\n\n/// Job execution result\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct ExecResult {\n    pub exit_code: i32,\n    pub stdout: Option\u003cString\u003e,\n    pub stderr: Option\u003cString\u003e,\n}\n","traces":[{"line":13,"address":[5326880],"length":1,"stats":{"Line":0}},{"line":14,"address":[5326894],"length":1,"stats":{"Line":0}},{"line":17,"address":[5326944],"length":1,"stats":{"Line":0}},{"line":18,"address":[5326947],"length":1,"stats":{"Line":0}},{"line":21,"address":[5326976],"length":1,"stats":{"Line":0}},{"line":22,"address":[5326984],"length":1,"stats":{"Line":0}},{"line":27,"address":[5327008],"length":1,"stats":{"Line":0}},{"line":28,"address":[5327016],"length":1,"stats":{"Line":0}},{"line":41,"address":[5327040],"length":1,"stats":{"Line":0}},{"line":49,"address":[5327072],"length":1,"stats":{"Line":0}},{"line":50,"address":[5327081],"length":1,"stats":{"Line":0}},{"line":51,"address":[5327088],"length":1,"stats":{"Line":0}},{"line":69,"address":[5327120],"length":1,"stats":{"Line":0}},{"line":70,"address":[5327158],"length":1,"stats":{"Line":0}},{"line":71,"address":[5327259],"length":1,"stats":{"Line":0}},{"line":72,"address":[5327231],"length":1,"stats":{"Line":0}},{"line":76,"address":[5327194],"length":1,"stats":{"Line":0}},{"line":77,"address":[5327349],"length":1,"stats":{"Line":0}},{"line":80,"address":[5327329],"length":1,"stats":{"Line":0}},{"line":81,"address":[5327508],"length":1,"stats":{"Line":0}},{"line":82,"address":[5327477],"length":1,"stats":{"Line":0}},{"line":86,"address":[5327462],"length":1,"stats":{"Line":0}},{"line":87,"address":[5327628],"length":1,"stats":{"Line":0}},{"line":88,"address":[5327597],"length":1,"stats":{"Line":0}},{"line":92,"address":[5327719],"length":1,"stats":{"Line":0}},{"line":108,"address":[5327760,5328522],"length":1,"stats":{"Line":0}},{"line":109,"address":[5327790,5327874],"length":1,"stats":{"Line":0}},{"line":110,"address":[5328189],"length":1,"stats":{"Line":0}},{"line":116,"address":[5328278],"length":1,"stats":{"Line":0}},{"line":123,"address":[5328544],"length":1,"stats":{"Line":0}},{"line":124,"address":[5328563],"length":1,"stats":{"Line":0}},{"line":125,"address":[5328618,5329129,5328682],"length":1,"stats":{"Line":0}},{"line":126,"address":[5329091],"length":1,"stats":{"Line":0}},{"line":127,"address":[5329081,5328755,5328649],"length":1,"stats":{"Line":0}},{"line":128,"address":[5329043],"length":1,"stats":{"Line":0}},{"line":129,"address":[5328991,5328722,5328828],"length":1,"stats":{"Line":0}},{"line":130,"address":[5329033,5328958],"length":1,"stats":{"Line":0}},{"line":131,"address":[5328998],"length":1,"stats":{"Line":0}},{"line":132,"address":[5328795,5328872,5328951],"length":1,"stats":{"Line":0}},{"line":133,"address":[5328916],"length":1,"stats":{"Line":0}},{"line":134,"address":[5328865],"length":1,"stats":{"Line":0}},{"line":138,"address":[5329152],"length":1,"stats":{"Line":0}},{"line":139,"address":[5329157],"length":1,"stats":{"Line":0}},{"line":144,"address":[5329168],"length":1,"stats":{"Line":0}},{"line":145,"address":[5329182],"length":1,"stats":{"Line":0}}],"covered":0,"coverable":45},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","lib.rs"],"content":"//! Shared types and definitions for the Hodei CI/CD distributed system\n//!\n//! This crate contains common types, IDs, value objects, and error types\n//! used across all bounded contexts in the system.\n\npub mod correlation;\npub mod error;\npub mod health_checks;\npub mod job_definitions;\npub mod worker_messages;\n\npub use crate::error::DomainError;\npub use chrono::{DateTime, Utc};\npub use serde::{Deserialize, Serialize};\npub use uuid::{Uuid, uuid};\n\n// Re-export all types for easy importing\npub use crate::correlation::{CorrelationId, TraceContext};\npub use crate::job_definitions::{ExecResult, JobId, JobSpec, JobState, ResourceQuota};\n\n/// Tenant identifier for multi-tenancy support\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct TenantId(String);\n\nimpl TenantId {\n    pub fn new(id: String) -\u003e Self {\n        Self(id)\n    }\n\n    pub fn as_str(\u0026self) -\u003e \u0026str {\n        \u0026self.0\n    }\n}\n\nimpl From\u003cString\u003e for TenantId {\n    fn from(s: String) -\u003e Self {\n        TenantId::new(s)\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","worker_messages.rs"],"content":"//! Worker-related message types for distributed communication\n\nuse serde::{Deserialize, Serialize};\nuse uuid::Uuid;\n\n/// Worker identifier\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct WorkerId(pub Uuid);\n\nimpl WorkerId {\n    pub fn new() -\u003e Self {\n        Self(Uuid::new_v4())\n    }\n\n    pub fn from_uuid(uuid: Uuid) -\u003e Self {\n        Self(uuid)\n    }\n\n    pub fn as_uuid(\u0026self) -\u003e Uuid {\n        self.0\n    }\n}\n\nimpl Default for WorkerId {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Worker state\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum WorkerState {\n    Creating,\n    Available,\n    Running,\n    Unhealthy,\n    Draining,\n    Terminated,\n    Failed { reason: String },\n}\n\n/// Runtime specification for worker\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct RuntimeSpec {\n    pub image: String,\n    pub command: Option\u003cVec\u003cString\u003e\u003e,\n    pub resources: crate::job_definitions::ResourceQuota,\n    pub env: std::collections::HashMap\u003cString, String\u003e,\n    pub labels: std::collections::HashMap\u003cString, String\u003e,\n}\n\nimpl RuntimeSpec {\n    pub fn new(image: String) -\u003e Self {\n        Self {\n            image,\n            command: None,\n            resources: crate::job_definitions::ResourceQuota::new(100, 512),\n            env: std::collections::HashMap::new(),\n            labels: std::collections::HashMap::new(),\n        }\n    }\n}\n\n/// Worker capabilities for matching\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct WorkerCapabilities {\n    pub cpu_m: u64,\n    pub memory_mb: u64,\n    pub gpu: Option\u003cu8\u003e,\n    pub features: Vec\u003cString\u003e,\n}\n","traces":[{"line":11,"address":[5323040],"length":1,"stats":{"Line":2}},{"line":12,"address":[5323054],"length":1,"stats":{"Line":2}},{"line":15,"address":[5323104],"length":1,"stats":{"Line":0}},{"line":16,"address":[5323107],"length":1,"stats":{"Line":0}},{"line":19,"address":[5323136],"length":1,"stats":{"Line":0}},{"line":20,"address":[5323144],"length":1,"stats":{"Line":0}},{"line":25,"address":[5323168],"length":1,"stats":{"Line":0}},{"line":26,"address":[5323176],"length":1,"stats":{"Line":0}},{"line":53,"address":[5323200,5323552,5323530],"length":1,"stats":{"Line":0}},{"line":57,"address":[5323248],"length":1,"stats":{"Line":0}},{"line":58,"address":[5323315],"length":1,"stats":{"Line":0}},{"line":59,"address":[5323331],"length":1,"stats":{"Line":0}}],"covered":2,"coverable":12},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","examples","scheduler_worker_integration.rs"],"content":"//! Scheduler and Worker Lifecycle Integration Example\n//!\n//! This example demonstrates how to integrate the Kubernetes-style scheduler\n//! framework with the worker lifecycle management system for the Hodei Jobs\n//! distributed CI/CD platform.\n//!\n//! Run with: cargo run --example scheduler_worker_integration\n\nuse hodei_scheduler::backend::{ComputeResource, KubernetesBackend, SchedulerBackend};\nuse hodei_scheduler::integration::SchedulerWorkerIntegration;\nuse hodei_scheduler::types::{\n    BackendSpecific, BackendType, Job, JobMetadata, JobPriority, JobSpec, KubernetesNodeSpecific,\n    LabelSelector, LabelSelectorOperator, NodeAffinity, NodeLocation, NodeSelector,\n    ResourceRequirements, WeightedLabelSelector, WorkerNode,\n};\nuse hodei_shared_types::TenantId;\nuse hodei_worker_lifecycle::{Worker, WorkerCapabilities, WorkerManager};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::time::Duration;\nuse uuid::Uuid;\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n    tracing_subscriber::fmt::init();\n\n    println!(\" Starting Scheduler-Worker Lifecycle Integration Example\\n\");\n\n    // Step 1: Create Worker Lifecycle Manager\n    println!(\" Step 1: Creating Worker Lifecycle Manager\");\n    let worker_manager = Arc::new(WorkerManager::new(\n        None,                    // No coordinator needed for this example\n        Duration::from_secs(30), // Health check interval\n        Duration::from_secs(10), // Heartbeat timeout\n    ));\n\n    // Step 2: Create Scheduler Backend (Mock for example)\n    println!(\"  Step 2: Creating Mock Scheduler Backend\");\n    let backend = Arc::new(KubernetesBackend::new());\n\n    // Step 3: Create the Integration Coordinator\n    println!(\" Step 3: Creating Scheduler-Worker Integration Coordinator\");\n    let integration = SchedulerWorkerIntegration::new(backend.clone(), worker_manager.clone());\n\n    // Step 4: Start the integration\n    println!(\"  Starting integration coordinator...\");\n    integration.start().await?;\n    println!(\" Integration coordinator started successfully\\n\");\n\n    // Step 5: Register workers in lifecycle manager\n    println!(\" Step 5: Registering Workers\");\n    let worker_ids = register_workers(\u0026worker_manager).await?;\n    println!(\" Registered {} workers\\n\", worker_ids.len());\n\n    // Step 6: Create a sample job\n    println!(\" Step 6: Creating Sample Job\");\n    let job = create_sample_job();\n    println!(\n        \" Created job: {} (priority: {})\\n\",\n        job.metadata.name, job.spec.priority\n    );\n\n    // Step 7: Find suitable workers for the job\n    println!(\" Step 7: Finding Suitable Workers\");\n    match integration.find_suitable_workers(\u0026job).await {\n        Ok(workers) =\u003e {\n            println!(\" Found {} suitable workers:\", workers.len());\n            for worker in \u0026workers {\n                println!(\"   - Worker ID: {}\", worker.id);\n                println!(\n                    \"     CPU: {:.1} cores, Memory: {} bytes\",\n                    worker.resources.cpu_cores, worker.resources.memory_bytes\n                );\n            }\n            println!();\n\n            // Step 8: Bind job to a worker\n            if let Some(first_worker) = workers.first() {\n                println!(\" Step 8: Binding Job to Worker\");\n                let job_id = job.metadata.id;\n                let worker_id = first_worker.id;\n\n                integration.bind_job_to_worker(\u0026job_id, \u0026worker_id).await?;\n                println!(\n                    \" Successfully bound job {} to worker {}\\n\",\n                    job_id, worker_id\n                );\n\n                // Step 9: Verify binding\n                println!(\" Step 9: Verifying Job Binding\");\n                assert!(integration.is_job_bound(\u0026job_id));\n                assert_eq!(integration.get_job_worker(\u0026job_id), Some(worker_id));\n\n                let binding_map = integration.get_job_to_worker_map();\n                println!(\"   Active bindings: {}\", binding_map.len());\n                println!(\"   Job {} is bound to worker {}\\n\", job_id, worker_id);\n\n                // Step 10: Simulate job completion\n                println!(\" Step 10: Simulating Job Completion\");\n                integration\n                    .unbind_job_from_worker(\u0026job_id, \u0026worker_id)\n                    .await?;\n                println!(\" Job unbound from worker\\n\");\n            }\n        }\n        Err(e) =\u003e {\n            println!(\" No suitable workers found: {}\", e);\n        }\n    }\n\n    // Step 11: Test worker failure handling\n    println!(\" Step 11: Testing Worker Failure Handling\");\n    if let Some(first_worker_id) = worker_ids.first() {\n        println!(\"   Simulating failure for worker: {}\", first_worker_id);\n        integration.handle_worker_failed(*first_worker_id).await?;\n        println!(\" Worker failure handled successfully\\n\");\n    }\n\n    // Step 12: Get worker status\n    println!(\" Step 12: Getting Worker Status Summary\");\n    let worker_summaries = integration.get_workers_summary().await;\n    println!(\"   Total workers: {}\", worker_summaries.len());\n\n    for summary in worker_summaries {\n        println!(\n            \"   - Worker {}: state={}, load={:.2}, jobs={}\",\n            summary.worker_id, summary.state, summary.load, summary.jobs_running\n        );\n    }\n    println!();\n\n    // Step 13: Stop the integration\n    println!(\"  Step 13: Stopping Integration Coordinator\");\n    integration.stop().await?;\n    println!(\" Integration coordinator stopped successfully\\n\");\n\n    println!(\" Example completed successfully!\");\n\n    Ok(())\n}\n\n/// Register sample workers in the lifecycle manager\nasync fn register_workers(\n    worker_manager: \u0026Arc\u003cWorkerManager\u003e,\n) -\u003e Result\u003cVec\u003cUuid\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let mut worker_ids = Vec::new();\n\n    // Worker 1: High-capacity node\n    let worker1_id = Uuid::new_v4();\n    worker_manager.register_worker(Worker::new(\n        worker1_id,\n        WorkerCapabilities {\n            cpu_cores: 16,\n            memory_gb: 64,\n            has_gpu: true,\n            gpu_count: Some(2),\n            specialized_hardware: vec![\"nvme-ssd\".to_string()],\n            container_runtime: \"containerd\".to_string(),\n        },\n        10, // max_jobs\n        None,\n    ))?;\n    worker_ids.push(worker1_id);\n    println!(\"    Registered worker 1 (ID: {})\", worker1_id);\n\n    // Worker 2: Medium-capacity node\n    let worker2_id = Uuid::new_v4();\n    worker_manager.register_worker(Worker::new(\n        worker2_id,\n        WorkerCapabilities {\n            cpu_cores: 8,\n            memory_gb: 32,\n            has_gpu: false,\n            gpu_count: None,\n            specialized_hardware: vec![],\n            container_runtime: \"docker\".to_string(),\n        },\n        5, // max_jobs\n        None,\n    ))?;\n    worker_ids.push(worker2_id);\n    println!(\"    Registered worker 2 (ID: {})\", worker2_id);\n\n    // Worker 3: GPU node\n    let worker3_id = Uuid::new_v4();\n    worker_manager.register_worker(Worker::new(\n        worker3_id,\n        WorkerCapabilities {\n            cpu_cores: 4,\n            memory_gb: 16,\n            has_gpu: true,\n            gpu_count: Some(1),\n            specialized_hardware: vec![\"cuda\".to_string()],\n            container_runtime: \"docker\".to_string(),\n        },\n        3, // max_jobs\n        None,\n    ))?;\n    worker_ids.push(worker3_id);\n    println!(\"    Registered worker 3 (ID: {})\", worker3_id);\n\n    Ok(worker_ids)\n}\n\n/// Create a sample job with resource requirements\nfn create_sample_job() -\u003e Job {\n    Job {\n        metadata: JobMetadata {\n            id: Uuid::new_v4(),\n            name: \"ci-pipeline-build\".to_string(),\n            namespace: \"default\".to_string(),\n            labels: {\n                let mut labels = HashMap::new();\n                labels.insert(\"app\".to_string(), \"hodei-jobs\".to_string());\n                labels.insert(\"tier\".to_string(), \"ci\".to_string());\n                labels\n            },\n            created_at: chrono::Utc::now(),\n        },\n        spec: JobSpec {\n            resource_requirements: Some(ResourceRequirements {\n                cpu_cores: Some(4.0),\n                memory_bytes: Some(8_000_000_000), // 8 GB\n                gpu_count: Some(1),\n                ephemeral_storage: Some(20_000_000_000), // 20 GB\n            }),\n            priority: JobPriority::High,\n            node_selector: Some(NodeSelector {\n                labels: {\n                    let mut labels = HashMap::new();\n                    labels.insert(\"node-type\".to_string(), \"compute\".to_string());\n                    labels\n                },\n            }),\n            affinity: Some(NodeAffinity {\n                required_during_scheduling: vec![LabelSelector {\n                    key: \"rack\".to_string(),\n                    operator: LabelSelectorOperator::In,\n                    values: Some(vec![\"rack-a\".to_string(), \"rack-b\".to_string()]),\n                }],\n                preferred_during_scheduling: vec![WeightedLabelSelector {\n                    selector: LabelSelector {\n                        key: \"gpu\".to_string(),\n                        operator: LabelSelectorOperator::Exists,\n                        values: None,\n                    },\n                    weight: 10,\n                }],\n            }),\n            tolerations: vec![],\n            max_retries: 3,\n        },\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","aws_secrets.rs"],"content":"//! AWS Secrets Manager Provider Implementation\n//! \n//! This module provides a complete integration with AWS Secrets Manager for\n//! secure credential management and storage.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::time::Duration;\n\n/// AWS Secrets Manager configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AWSSecretsConfig {\n    pub region: String,\n    pub profile: Option\u003cString\u003e,\n    pub access_key_id: Option\u003cString\u003e,\n    pub secret_access_key: Option\u003cString\u003e,\n    pub session_token: Option\u003cString\u003e,\n    pub timeout: Duration,\n    pub max_retries: u32,\n}\n\n/// AWS Secrets Manager client wrapper\n#[derive(Debug)]\npub struct AWSSecretsClient {\n    client: aws_sdk_secretsmanager::Client,\n    config: AWSSecretsConfig,\n}\n\nimpl AWSSecretsClient {\n    /// Create new AWS Secrets Manager client\n    pub async fn new(config: AWSSecretsConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let aws_config = match (\u0026config.access_key_id, \u0026config.secret_access_key) {\n            (Some(key_id), Some(secret_key)) =\u003e {\n                aws_config::Config::builder()\n                    .region(config.region.as_ref().try_into().map_err(|_| CredentialError::Configuration {\n                        message: format!(\"Invalid AWS region: {}\", config.region)\n                    })?)\n                    .credentials_provider(aws_sdk_secretsmanager::config::ProvideCredentials::ClientCredentials {\n                        access_key_id: key_id.clone(),\n                        secret_access_key: secret_key.clone(),\n                        session_token: config.session_token.clone(),\n                    })\n                    .build()\n            }\n            _ =\u003e {\n                // Use default credential chain\n                aws_config::Config::builder()\n                    .region(config.region.as_ref().try_into().map_err(|_| CredentialError::Configuration {\n                        message: format!(\"Invalid AWS region: {}\", config.region)\n                    })?)\n                    .build()\n            }\n        };\n\n        let client = aws_sdk_secretsmanager::Client::from_conf(aws_config);\n        Ok(Self { client, config })\n    }\n\n    /// Get secret from AWS Secrets Manager\n    pub async fn get_secret(\u0026self, secret_id: \u0026str) -\u003e Result\u003cSecretString, CredentialError\u003e {\n        let request = self.client.get_secret_value()\n            .secret_id(secret_id);\n\n        match request.send().await {\n            Ok(output) =\u003e {\n                if let Some(secret_string) = output.secret_string() {\n                    Ok(secret_string.to_string())\n                } else if let Some(secret_binary) = output.secret_binary() {\n                    // Decode binary secret to string\n                    let bytes = secret_binary.as_ref();\n                    Ok(String::from_utf8_lossy(bytes).to_string())\n                } else {\n                    Err(CredentialError::NotFound { \n                        name: secret_id.to_string() \n                    })\n                }\n            }\n            Err(error) =\u003e {\n                if error.code() == Some(\"ResourceNotFoundException\") {\n                    Err(CredentialError::NotFound { \n                        name: secret_id.to_string() \n                    })\n                } else {\n                    Err(CredentialError::Network { \n                        message: format!(\"AWS Secrets Manager error: {}\", error) \n                    })\n                }\n            }\n        }\n    }\n\n    /// Put secret to AWS Secrets Manager\n    pub async fn put_secret(\u0026self, secret_id: \u0026str, secret_string: \u0026str, description: Option\u003c\u0026str\u003e) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut request = self.client.put_secret_value()\n            .secret_id(secret_id)\n            .secret_string(secret_string);\n\n        if let Some(desc) = description {\n            request = request.description(desc);\n        }\n\n        match request.send().await {\n            Ok(_) =\u003e Ok(()),\n            Err(error) =\u003e {\n                if error.code() == Some(\"ResourceNotFoundException\") {\n                    // Secret doesn't exist, create it\n                    let mut request = self.client.create_secret()\n                        .name(secret_id)\n                        .secret_string(secret_string);\n\n                    if let Some(desc) = description {\n                        request = request.description(desc);\n                    }\n\n                    match request.send().await {\n                        Ok(_) =\u003e Ok(()),\n                        Err(create_error) =\u003e {\n                            Err(CredentialError::Storage { \n                                message: format!(\"Failed to create secret: {}\", create_error) \n                            })\n                        }\n                    }\n                } else {\n                    Err(CredentialError::Storage { \n                        message: format!(\"AWS Secrets Manager put error: {}\", error) \n                    })\n                }\n            }\n        }\n    }\n\n    /// List secrets in AWS Secrets Manager\n    pub async fn list_secrets(\u0026self) -\u003e Result\u003cVec\u003cString\u003e, CredentialError\u003e {\n        let mut secrets = Vec::new();\n        let mut next_token = None;\n\n        loop {\n            let mut request = self.client.list_secrets();\n\n            if let Some(token) = next_token {\n                request = request.next_token(token);\n            }\n\n            match request.send().await {\n                Ok(output) =\u003e {\n                    if let Some(secret_list) = output.secret_list() {\n                        for secret in secret_list {\n                            if let Some(name) = secret.name() {\n                                secrets.push(name.to_string());\n                            }\n                        }\n                    }\n\n                    if output.next_token().is_some() {\n                        next_token = output.next_token().map(|t| t.to_string());\n                    } else {\n                        break;\n                    }\n                }\n                Err(error) =\u003e {\n                    return Err(CredentialError::Network { \n                        message: format!(\"AWS list secrets error: {}\", error) \n                    });\n                }\n            }\n        }\n\n        Ok(secrets)\n    }\n\n    /// Delete secret from AWS Secrets Manager\n    pub async fn delete_secret(\u0026self, secret_id: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let request = self.client.delete_secret()\n            .secret_id(secret_id);\n\n        match request.send().await {\n            Ok(_) =\u003e Ok(()),\n            Err(error) =\u003e {\n                if error.code() == Some(\"ResourceNotFoundException\") {\n                    // Secret doesn't exist, consider it already deleted\n                    Ok(())\n                } else {\n                    Err(CredentialError::Storage { \n                        message: format!(\"AWS delete secret error: {}\", error) \n                    })\n                }\n            }\n        }\n    }\n\n    /// Rotate secret in AWS Secrets Manager\n    pub async fn rotate_secret(\u0026self, secret_id: \u0026str, new_secret_string: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let request = self.client.rotate_secret()\n            .secret_id(secret_id)\n            .client_request_token(format!(\"rotation_{}\", chrono::Utc::now().timestamp_millis()));\n\n        match request.send().await {\n            Ok(_) =\u003e Ok(()),\n            Err(error) =\u003e {\n                Err(CredentialError::RotationFailed { \n                    name: secret_id.to_string() \n                })\n            }\n        }\n    }\n\n    /// Get secret versions\n    pub async fn list_secret_versions(\u0026self, secret_id: \u0026str) -\u003e Result\u003cVec\u003cSecretVersionInfo\u003e, CredentialError\u003e {\n        let request = self.client.list_secret_version_ids()\n            .secret_id(secret_id);\n\n        match request.send().await {\n            Ok(output) =\u003e {\n                let mut versions = Vec::new();\n                if let Some(version_ids) = output.secret_version_ids() {\n                    for version in version_ids {\n                        let version_info = SecretVersionInfo {\n                            version_id: version.version_id().unwrap_or(\"unknown\").to_string(),\n                            is_current_version: version.is_current_version().unwrap_or(false),\n                            created_date: version.created_date()\n                                .map(|d| chrono::DateTime::from_utc(\n                                    chrono::NaiveDateTime::from_timestamp_opt(d.as_secs(), 0).unwrap_or_default(),\n                                    chrono::Utc\n                                )),\n                        };\n                        versions.push(version_info);\n                    }\n                }\n                Ok(versions)\n            }\n            Err(error) =\u003e {\n                Err(CredentialError::Network { \n                    message: format!(\"AWS list secret versions error: {}\", error) \n                })\n            }\n        }\n    }\n}\n\n/// Secret version information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SecretVersionInfo {\n    pub version_id: String,\n    pub is_current_version: bool,\n    pub created_date: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\n/// AWS Secrets Manager Credential Provider\n#[derive(Debug)]\npub struct AWSSecretsManagerProvider {\n    client: AWSSecretsClient,\n}\n\nimpl AWSSecretsManagerProvider {\n    /// Create new AWS Secrets Manager Provider\n    pub async fn new(config: AWSSecretsConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = AWSSecretsClient::new(config).await?;\n        Ok(Self { client })\n    }\n\n    /// Convert AWS secret string to Credential format\n    fn aws_secret_to_credential(\u0026self, secret_string: \u0026str, secret_id: \u0026str) -\u003e Result\u003cHashMap\u003cString, String\u003e, CredentialError\u003e {\n        // Try to parse as JSON first\n        if let Ok(json_value) = serde_json::from_str::\u003cserde_json::Value\u003e(secret_string) {\n            let mut values = HashMap::new();\n            \n            if json_value.is_object() {\n                for (key, value) in json_value.as_object().unwrap() {\n                    if let Some(value_str) = value.as_str() {\n                        values.insert(key.clone(), value_str.to_string());\n                    } else if value.is_number() {\n                        values.insert(key.clone(), value.to_string());\n                    } else if value.is_boolean() {\n                        values.insert(key.clone(), value.to_string());\n                    } else {\n                        values.insert(key.clone(), serde_json::to_string(value).unwrap_or_else(|_| \"null\".to_string()));\n                    }\n                }\n            } else {\n                // Treat as a single value with a default key\n                values.insert(\"value\".to_string(), secret_string.to_string());\n            }\n            \n            Ok(values)\n        } else {\n            // Not JSON, treat as plain text\n            Ok(HashMap::from([(\"value\".to_string(), secret_string.to_string())]))\n        }\n    }\n\n    /// Convert Credential to AWS secret string\n    fn credential_to_aws_secret(\u0026self, credential: \u0026Credential) -\u003e String {\n        if credential.values.len() == 1 {\n            // Single value, return as plain text\n            credential.values.values().next().unwrap().clone()\n        } else {\n            // Multiple values, return as JSON\n            serde_json::to_string(\u0026credential.values).unwrap_or_else(|_| \"{}\".to_string())\n        }\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for AWSSecretsManagerProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        // AWS doesn't support getting specific versions via the basic API\n        // This would need additional implementation for version-specific access\n        let secret_id = format!(\"secret/{}\", name); // Assuming secrets are stored under \"secret/\" prefix\n        \n        let secret_string = self.client.get_secret(\u0026secret_id).await?;\n        let values = self.aws_secret_to_credential(\u0026secret_string, \u0026secret_id)?;\n\n        let credential = Credential {\n            name: name.to_string(),\n            values,\n            metadata: HashMap::new(),\n            created_at: chrono::Utc::now(),\n            updated_at: chrono::Utc::now(),\n            expires_at: None,\n            rotation_enabled: false,\n            access_policy: None,\n        };\n\n        let version = version.unwrap_or(\"current\").to_string();\n        \n        Ok(VersionedCredential {\n            credential,\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", credential.name);\n        let secret_string = self.credential_to_aws_secret(credential);\n        \n        self.client.put_secret(\u0026secret_id, \u0026secret_string, Some(\u0026format!(\"Secret for {}\", credential.name))).await?;\n        \n        let version = format!(\"v{}_{}\", chrono::Utc::now().timestamp_millis(), chrono::Utc::now().timestamp_nanos());\n        \n        Ok(VersionedCredential {\n            credential: credential.clone(),\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        let versions = self.client.list_secret_versions(\u0026secret_id).await?;\n        \n        let mut versioned_credentials = Vec::new();\n        let secret_string = self.client.get_secret(\u0026secret_id).await?;\n        let values = self.aws_secret_to_credential(\u0026secret_string, \u0026secret_id)?;\n        \n        for version_info in versions {\n            versioned_credentials.push(VersionedCredential {\n                credential: Credential {\n                    name: name.to_string(),\n                    values: values.clone(),\n                    metadata: HashMap::new(),\n                    created_at: version_info.created_date.unwrap_or(chrono::Utc::now()),\n                    updated_at: chrono::Utc::now(),\n                    expires_at: None,\n                    rotation_enabled: false,\n                    access_policy: None,\n                },\n                version: version_info.version_id,\n                version_created_at: version_info.created_date.unwrap_or(chrono::Utc::now()),\n                is_active: version_info.is_current_version,\n            });\n        }\n        \n        Ok(versioned_credentials)\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let secrets = self.client.list_secrets().await?;\n        \n        let mut credentials = Vec::new();\n        for secret_name in secrets {\n            if let Ok(version) = self.get_credential(\u0026secret_name, None).await {\n                credentials.push(version);\n            }\n        }\n        \n        Ok(credentials)\n    }\n\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        self.client.delete_secret(\u0026secret_id).await?;\n        Ok(())\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        match self.client.get_secret(\u0026secret_id).await {\n            Ok(_) =\u003e Ok(true),\n            Err(CredentialError::NotFound { .. }) =\u003e Ok(false),\n            Err(e) =\u003e Err(e),\n        }\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        let current = self.get_credential(name, None).await?;\n        \n        // Create rotated version\n        let mut rotated_credential = current.credential.clone();\n        rotated_credential.updated_at = chrono::Utc::now();\n        \n        match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                rotated_credential.expires_at = Some(chrono::Utc::now() + time_based.rotation_interval);\n            }\n            RotationStrategy::EventBased(_) =\u003e {\n                rotated_credential.expires_at = Some(chrono::Utc::now() + chrono::Duration::days(30));\n            }\n            RotationStrategy::Manual =\u003e {\n                // Keep current expiry\n            }\n        }\n\n        // Use AWS Secrets Manager rotation API\n        let new_secret_string = self.credential_to_aws_secret(\u0026rotated_credential);\n        self.client.rotate_secret(\u0026secret_id, \u0026new_secret_string).await?;\n        \n        Ok(RotationResult {\n            success: true,\n            rotated_credential: Some(name.to_string()),\n            old_credential_version: Some(current.version),\n            new_credential_version: Some(\"rotated\".to_string()),\n            rotated_at: chrono::Utc::now(),\n            error_message: None,\n        })\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        // AWS Secrets Manager doesn't have built-in access policies in the secret itself\n        // This would need to be managed via IAM policies and AWS Verified Permissions\n        Ok(None)\n    }\n\n    async fn set_access_policy(\u0026self, _name: \u0026str, _policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        // Not applicable for AWS Secrets Manager\n        Ok(())\n    }\n\n    async fn has_permission(\u0026self, _name: \u0026str, _subject: \u0026str, _permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        // Permission checking would be done via AWS IAM/Verified Permissions\n        // For now, return true\n        Ok(true)\n    }\n\n    async fn audit_operation(\u0026self, _event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // In production, you would integrate with AWS CloudTrail for auditing\n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        let credentials = self.list_credentials().await?;\n        \n        Ok(CredentialStatistics {\n            total_credentials: credentials.len() as u64,\n            active_credentials: credentials.len() as u64,\n            credential_versions: credentials.len() as u64,\n            rotation_statistics: RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                next_scheduled_rotation: None,\n            },\n            last_accessed: None,\n        })\n    }\n}\n\n/// Service Account credentials for AWS STS assume role\n#[derive(Debug, Clone)]\npub struct AWSCredentials {\n    pub access_key_id: String,\n    pub secret_access_key: String,\n    pub session_token: Option\u003cString\u003e,\n}\n\n/// AWS STS Service for assuming roles\npub struct AWSSTSService {\n    client: aws_sdk_sts::Client,\n}\n\nimpl AWSSTSService {\n    /// Create new AWS STS service\n    pub fn new() -\u003e Result\u003cSelf, CredentialError\u003e {\n        let aws_config = aws_config::Config::builder()\n            .region(\"us-east-1\")\n            .build();\n        let client = aws_sdk_sts::Client::from_conf(aws_config);\n        Ok(Self { client })\n    }\n\n    /// Assume role and get temporary credentials\n    pub async fn assume_role(\n        \u0026self,\n        role_arn: \u0026str,\n        role_session_name: \u0026str,\n        duration_seconds: Option\u003ci32\u003e,\n    ) -\u003e Result\u003cAWSCredentials, CredentialError\u003e {\n        let mut request = self.client.assume_role()\n            .role_arn(role_arn)\n            .role_session_name(role_session_name);\n\n        if let Some(duration) = duration_seconds {\n            request = request.duration_seconds(duration);\n        }\n\n        match request.send().await {\n            Ok(output) =\u003e {\n                if let Some(credentials) = output.credentials() {\n                    let access_key_id = credentials.access_key_id().unwrap_or(\"\").to_string();\n                    let secret_access_key = credentials.secret_access_key().unwrap_or(\"\").to_string();\n                    let session_token = credentials.session_token().map(|t| t.to_string());\n\n                    Ok(AWSCredentials {\n                        access_key_id,\n                        secret_access_key,\n                        session_token,\n                    })\n                } else {\n                    Err(CredentialError::InternalError)\n                }\n            }\n            Err(error) =\u003e {\n                Err(CredentialError::Network {\n                    message: format!(\"AWS STS assume role error: {}\", error)\n                })\n            }\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","keycloak.rs"],"content":"//! Keycloak Service Account Provider Implementation\n//! \n//! This module provides a complete integration with Keycloak for\n//! Service Account authentication and token management.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::time::Duration;\n\n/// Keycloak configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KeycloakConfig {\n    pub base_url: String,\n    pub realm: String,\n    pub client_id: String,\n    pub client_secret: Option\u003cString\u003e,\n    pub scopes: Vec\u003cString\u003e,\n    pub token_endpoint: Option\u003cString\u003e,\n    pub jwks_endpoint: Option\u003cString\u003e,\n    pub timeout: Duration,\n    pub cache_ttl: Duration,\n}\n\n/// Keycloak token information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KeycloakToken {\n    pub access_token: String,\n    pub refresh_token: Option\u003cString\u003e,\n    pub token_type: String,\n    pub expires_in: i64,\n    pub scope: Option\u003cString\u003e,\n    pub issued_at: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Keycloak User information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KeycloakUser {\n    pub id: String,\n    pub username: String,\n    pub email: Option\u003cString\u003e,\n    pub first_name: Option\u003cString\u003e,\n    pub last_name: Option\u003cString\u003e,\n    pub enabled: bool,\n    pub email_verified: bool,\n}\n\n/// Keycloak Service Account client\n#[derive(Debug)]\npub struct KeycloakClient {\n    client: reqwest::Client,\n    config: KeycloakConfig,\n    token_cache: std::sync::RwLock\u003cHashMap\u003cString, (KeycloakToken, chrono::DateTime\u003cchrono::Utc\u003e)\u003e\u003e,\n}\n\nimpl KeycloakClient {\n    /// Create new Keycloak client\n    pub fn new(config: KeycloakConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = reqwest::Client::builder()\n            .timeout(config.timeout)\n            .danger_accept_invalid_certs(false)\n            .build()\n            .map_err(|e| CredentialError::Network { \n                message: format!(\"Failed to create Keycloak HTTP client: {}\", e) \n            })?;\n\n        Ok(Self {\n            client,\n            config,\n            token_cache: std::sync::RwLock::new(HashMap::new()),\n        })\n    }\n\n    /// Get cached token for client credentials\n    async fn get_cached_token(\u0026self, key: \u0026str) -\u003e Option\u003cKeycloakToken\u003e {\n        if let Ok(cache) = self.token_cache.read() {\n            if let Some((token, expires_at)) = cache.get(key) {\n                if chrono::Utc::now() \u003c *expires_at {\n                    return Some(token.clone());\n                }\n            }\n        }\n        None\n    }\n\n    /// Cache token\n    fn cache_token(\u0026self, key: String, token: KeycloakToken) {\n        let expires_at = token.issued_at + chrono::Duration::seconds(token.expires_in - 60); // 60s buffer\n        if let Ok(mut cache) = self.token_cache.write() {\n            cache.insert(key, (token, expires_at));\n        }\n    }\n\n    /// Get access token using client credentials flow\n    pub async fn get_access_token(\u0026self) -\u003e Result\u003cKeycloakToken, CredentialError\u003e {\n        let cache_key = format!(\"client_credentials:{}\", self.config.client_id);\n        \n        // Check cache first\n        if let Some(token) = self.get_cached_token(\u0026cache_key).await {\n            return Ok(token);\n        }\n\n        // Get token from Keycloak\n        let token_data = self.fetch_token().await?;\n        self.cache_token(cache_key, token_data.clone());\n\n        Ok(token_data)\n    }\n\n    /// Fetch token from Keycloak server\n    async fn fetch_token(\u0026self) -\u003e Result\u003cKeycloakToken, CredentialError\u003e {\n        let token_endpoint = self.get_token_endpoint().await?;\n        \n        let mut form_data = HashMap::from([\n            (\"grant_type\".to_string(), \"client_credentials\".to_string()),\n            (\"client_id\".to_string(), self.config.client_id.clone()),\n            (\"scope\".to_string(), self.config.scopes.join(\" \")),\n        ]);\n\n        if let Some(secret) = \u0026self.config.client_secret {\n            form_data.insert(\"client_secret\".to_string(), secret.clone());\n        }\n\n        let response = self.client\n            .post(\u0026token_endpoint)\n            .form(\u0026form_data)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get Keycloak token: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            return Err(CredentialError::Authentication {\n                message: format!(\"Keycloak authentication failed: HTTP {} - {}\", status, body)\n            });\n        }\n\n        let token_response: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Keycloak token response: {}\", e)\n            })?;\n\n        let token_data = KeycloakToken {\n            access_token: token_response.get(\"access_token\")\n                .and_then(|t| t.as_str())\n                .ok_or_else(|| CredentialError::InvalidFormat {\n                    name: \"access_token\".to_string()\n                })?.to_string(),\n            refresh_token: token_response.get(\"refresh_token\").and_then(|t| t.as_str()).map(|s| s.to_string()),\n            token_type: token_response.get(\"token_type\")\n                .and_then(|t| t.as_str())\n                .unwrap_or(\"Bearer\").to_string(),\n            expires_in: token_response.get(\"expires_in\")\n                .and_then(|e| e.as_i64())\n                .unwrap_or(3600),\n            scope: token_response.get(\"scope\").and_then(|s| s.as_str()).map(|s| s.to_string()),\n            issued_at: chrono::Utc::now(),\n        };\n\n        Ok(token_data)\n    }\n\n    /// Get Keycloak token endpoint\n    async fn get_token_endpoint(\u0026self) -\u003e Result\u003cString, CredentialError\u003e {\n        if let Some(endpoint) = \u0026self.config.token_endpoint {\n            return Ok(endpoint.clone());\n        }\n\n        // Discover token endpoint from realm info\n        let well_known_url = format!(\"{}/realms/{}/.well-known/openid-configuration\", \n                                   self.config.base_url, self.config.realm);\n        \n        let response = self.client\n            .get(\u0026well_known_url)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get Keycloak OIDC configuration: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to get Keycloak OIDC configuration: HTTP {}\", response.status())\n            });\n        }\n\n        let oidc_config: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Keycloak OIDC configuration: {}\", e)\n            })?;\n\n        let token_endpoint = oidc_config.get(\"token_endpoint\")\n            .and_then(|e| e.as_str())\n            .ok_or_else(|| CredentialError::Configuration {\n                message: \"Token endpoint not found in OIDC configuration\".to_string()\n            })?.to_string();\n\n        Ok(token_endpoint)\n    }\n\n    /// Get user information using access token\n    pub async fn get_user_info(\u0026self, access_token: \u0026str) -\u003e Result\u003cKeycloakUser, CredentialError\u003e {\n        let user_info_url = format!(\"{}/realms/{}/protocol/openid-connect/userinfo\", \n                                  self.config.base_url, self.config.realm);\n\n        let response = self.client\n            .get(\u0026user_info_url)\n            .bearer_auth(access_token)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get Keycloak user info: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Authentication {\n                message: format!(\"Failed to get user info: HTTP {}\", response.status())\n            });\n        }\n\n        let user_info: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Keycloak user info: {}\", e)\n            })?;\n\n        let user = KeycloakUser {\n            id: user_info.get(\"sub\")\n                .and_then(|s| s.as_str())\n                .ok_or_else(|| CredentialError::InvalidFormat { name: \"sub\".to_string() })?.to_string(),\n            username: user_info.get(\"preferred_username\")\n                .and_then(|u| u.as_str())\n                .unwrap_or(\"unknown\").to_string(),\n            email: user_info.get(\"email\").and_then(|e| e.as_str()).map(|s| s.to_string()),\n            first_name: user_info.get(\"given_name\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            last_name: user_info.get(\"family_name\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            enabled: true, // Would need to check user status via admin API\n            email_verified: user_info.get(\"email_verified\")\n                .and_then(|v| v.as_bool())\n                .unwrap_or(false),\n        };\n\n        Ok(user)\n    }\n\n    /// Get JWKS for token validation\n    pub async fn get_jwks(\u0026self) -\u003e Result\u003cserde_json::Value, CredentialError\u003e {\n        let jwks_url = self.get_jwks_endpoint().await?;\n\n        let response = self.client\n            .get(\u0026jwks_url)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get JWKS: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to get JWKS: HTTP {}\", response.status())\n            });\n        }\n\n        response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse JWKS: {}\", e)\n            })\n    }\n\n    /// Get JWKS endpoint\n    async fn get_jwks_endpoint(\u0026self) -\u003e Result\u003cString, CredentialError\u003e {\n        if let Some(endpoint) = \u0026self.config.jwks_endpoint {\n            return Ok(endpoint.clone());\n        }\n\n        // Discover JWKS endpoint from OIDC configuration\n        let well_known_url = format!(\"{}/realms/{}/.well-known/openid-configuration\", \n                                   self.config.base_url, self.config.realm);\n        \n        let response = self.client\n            .get(\u0026well_known_url)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get OIDC configuration for JWKS: {}\", e)\n            })?;\n\n        let oidc_config: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse OIDC configuration for JWKS: {}\", e)\n            })?;\n\n        let jwks_uri = oidc_config.get(\"jwks_uri\")\n            .and_then(|e| e.as_str())\n            .ok_or_else(|| CredentialError::Configuration {\n                message: \"JWKS URI not found in OIDC configuration\".to_string()\n            })?.to_string();\n\n        Ok(jwks_uri)\n    }\n\n    /// Validate and decode JWT token\n    pub async fn validate_token(\u0026self, token: \u0026str) -\u003e Result\u003cserde_json::Value, CredentialError\u003e {\n        // In a production environment, you would use a JWT library to validate the token\n        // This is a simplified implementation\n        let jwks = self.get_jwks().await?;\n        \n        // For now, just decode the base64 payload (NOT SECURE FOR PRODUCTION)\n        let parts: Vec\u003c\u0026str\u003e = token.split('.').collect();\n        if parts.len() != 3 {\n            return Err(CredentialError::InvalidFormat { name: \"jwt\".to_string() });\n        }\n\n        let payload_b64 = parts[1];\n        let payload_bytes = base64::Engine::decode(\u0026base64::engine::general_purpose::URL_SAFE_NO_PAD, payload_b64)\n            .map_err(|_| CredentialError::InvalidFormat { name: \"jwt_payload\".to_string() })?;\n\n        let payload: serde_json::Value = serde_json::from_slice(\u0026payload_bytes)\n            .map_err(|_| CredentialError::InvalidFormat { name: \"jwt_payload\".to_string() })?;\n\n        Ok(payload)\n    }\n}\n\n/// Keycloak Service Account Provider\n#[derive(Debug)]\npub struct KeycloakServiceAccountProvider {\n    client: Arc\u003cKeycloakClient\u003e,\n}\n\nimpl KeycloakServiceAccountProvider {\n    /// Create new Keycloak Service Account Provider\n    pub fn new(config: KeycloakConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = KeycloakClient::new(config)?;\n        Ok(Self {\n            client: Arc::new(client),\n        })\n    }\n\n    /// Convert Keycloak token to credential format\n    fn keycloak_token_to_credential(\u0026self, token: \u0026KeycloakToken) -\u003e HashMap\u003cString, String\u003e {\n        let mut values = HashMap::new();\n        values.insert(\"access_token\".to_string(), token.access_token.clone());\n        values.insert(\"token_type\".to_string(), token.token_type.clone());\n        values.insert(\"expires_in\".to_string(), token.expires_in.to_string());\n        \n        if let Some(refresh_token) = \u0026token.refresh_token {\n            values.insert(\"refresh_token\".to_string(), refresh_token.clone());\n        }\n        \n        if let Some(scope) = \u0026token.scope {\n            values.insert(\"scope\".to_string(), scope.clone());\n        }\n        \n        values.insert(\"issued_at\".to_string(), token.issued_at.to_rfc3339());\n        values\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for KeycloakServiceAccountProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Err(CredentialError::NotFound { name: name.to_string() });\n        }\n\n        let token = self.client.get_access_token().await?;\n        let values = self.keycloak_token_to_credential(\u0026token);\n\n        let credential = Credential {\n            name: name.to_string(),\n            values,\n            metadata: HashMap::from([\n                (\"client_id\".to_string(), self.client.config.client_id.clone()),\n                (\"realm\".to_string(), self.client.config.realm.clone()),\n                (\"scopes\".to_string(), self.client.config.scopes.join(\" \")),\n            ]),\n            created_at: token.issued_at,\n            updated_at: chrono::Utc::now(),\n            expires_at: Some(token.issued_at + chrono::Duration::seconds(token.expires_in)),\n            rotation_enabled: true,\n            access_policy: Some(AccessPolicy {\n                allowed_subjects: vec![\"system\".to_string()],\n                read_permissions: vec![\"read\".to_string()],\n                write_permissions: vec![],\n                rotation_permissions: vec![\"rotate\".to_string()],\n            }),\n        };\n\n        let version = version.unwrap_or(\"current\").to_string();\n        \n        Ok(VersionedCredential {\n            credential,\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn put_credential(\u0026self, _credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        Err(CredentialError::PermissionDenied {\n            name: \"Keycloak tokens are read-only\".to_string()\n        })\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Err(CredentialError::NotFound { name: name.to_string() });\n        }\n\n        let token = self.client.get_access_token().await?;\n        let values = self.keycloak_token_to_credential(\u0026token);\n\n        let credential = Credential {\n            name: name.to_string(),\n            values,\n            metadata: HashMap::from([\n                (\"client_id\".to_string(), self.client.config.client_id.clone()),\n                (\"realm\".to_string(), self.client.config.realm.clone()),\n                (\"scopes\".to_string(), self.client.config.scopes.join(\" \")),\n            ]),\n            created_at: token.issued_at,\n            updated_at: chrono::Utc::now(),\n            expires_at: Some(token.issued_at + chrono::Duration::seconds(token.expires_in)),\n            rotation_enabled: true,\n            access_policy: Some(AccessPolicy {\n                allowed_subjects: vec![\"system\".to_string()],\n                read_permissions: vec![\"read\".to_string()],\n                write_permissions: vec![],\n                rotation_permissions: vec![\"rotate\".to_string()],\n            }),\n        };\n\n        Ok(vec![VersionedCredential {\n            credential,\n            version: \"current\".to_string(),\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        }])\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        // Keycloak Service Account Provider only manages service account tokens\n        Ok(vec![])\n    }\n\n    async fn delete_credential(\u0026self, _name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        // Cannot delete service account tokens\n        Err(CredentialError::PermissionDenied {\n            name: \"Keycloak service account tokens cannot be deleted\".to_string()\n        })\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        Ok(name == \"service_account_token\")\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Err(CredentialError::NotFound { name: name.to_string() });\n        }\n\n        match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                // Keycloak tokens expire automatically, rotation is handled by the server\n                // We just need to refresh our cache\n                let old_token = self.client.get_access_token().await?;\n                \n                // Wait for rotation interval\n                tokio::time::sleep(time_based.rotation_interval).await;\n                \n                // Get new token (will trigger cache refresh)\n                let _ = self.client.get_access_token().await?;\n                \n                Ok(RotationResult {\n                    success: true,\n                    rotated_credential: Some(name.to_string()),\n                    old_credential_version: Some(old_token.access_token.clone()),\n                    new_credential_version: Some(\"rotated\".to_string()),\n                    rotated_at: chrono::Utc::now(),\n                    error_message: None,\n                })\n            }\n            RotationStrategy::EventBased(_) | RotationStrategy::Manual =\u003e {\n                // Force cache refresh\n                {\n                    let mut cache = self.client.token_cache.write().map_err(|_| CredentialError::InternalError)?;\n                    cache.clear();\n                }\n                \n                let new_token = self.client.get_access_token().await?;\n                \n                Ok(RotationResult {\n                    success: true,\n                    rotated_credential: Some(name.to_string()),\n                    old_credential_version: Some(\"refreshed\".to_string()),\n                    new_credential_version: Some(new_token.access_token.clone()),\n                    rotated_at: chrono::Utc::now(),\n                    error_message: None,\n                })\n            }\n        }\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        if name == \"service_account_token\" {\n            Ok(Some(AccessPolicy {\n                allowed_subjects: vec![\"system\".to_string()],\n                read_permissions: vec![\"read\".to_string()],\n                write_permissions: vec![],\n                rotation_permissions: vec![\"rotate\".to_string()],\n            }))\n        } else {\n            Ok(None)\n        }\n    }\n\n    async fn set_access_policy(\u0026self, _name: \u0026str, _policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        // Cannot set policies on Keycloak service account tokens\n        Err(CredentialError::PermissionDenied {\n            name: \"Cannot modify Keycloak service account token policies\".to_string()\n        })\n    }\n\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Ok(false);\n        }\n\n        // Allow access if subject is \"system\" for read/rotate permissions\n        Ok(subject == \"system\" \u0026\u0026 (permission == \"read\" || permission == \"rotate\"))\n    }\n\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // In production, you would log this to Keycloak events or external audit system\n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        Ok(CredentialStatistics {\n            total_credentials: 1,\n            active_credentials: 1,\n            credential_versions: 1,\n            rotation_statistics: RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                next_scheduled_rotation: None,\n            },\n            last_accessed: Some(chrono::Utc::now()),\n        })\n    }\n}\n\n/// Keycloak User Provider for user authentication\n#[derive(Debug)]\npub struct KeycloakUserProvider {\n    client: Arc\u003cKeycloakClient\u003e,\n}\n\nimpl KeycloakUserProvider {\n    /// Create new Keycloak User Provider\n    pub fn new(config: KeycloakConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = KeycloakClient::new(config)?;\n        Ok(Self {\n            client: Arc::new(client),\n        })\n    }\n\n    /// Authenticate user with username and password\n    pub async fn authenticate_user(\u0026self, username: \u0026str, password: \u0026str) -\u003e Result\u003cKeycloakToken, CredentialError\u003e {\n        let token_endpoint = self.client.get_token_endpoint().await?;\n        \n        let mut form_data = HashMap::from([\n            (\"grant_type\".to_string(), \"password\".to_string()),\n            (\"client_id\".to_string(), self.client.config.client_id.clone()),\n            (\"username\".to_string(), username.to_string()),\n            (\"password\".to_string(), password.to_string()),\n            (\"scope\".to_string(), self.client.config.scopes.join(\" \")),\n        ]);\n\n        if let Some(secret) = \u0026self.client.config.client_secret {\n            form_data.insert(\"client_secret\".to_string(), secret.clone());\n        }\n\n        let response = self.client\n            .client\n            .post(\u0026token_endpoint)\n            .form(\u0026form_data)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to authenticate user: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Authentication {\n                message: format!(\"User authentication failed: HTTP {}\", response.status())\n            });\n        }\n\n        let token_response: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse user token response: {}\", e)\n            })?;\n\n        let token_data = KeycloakToken {\n            access_token: token_response.get(\"access_token\")\n                .and_then(|t| t.as_str())\n                .ok_or_else(|| CredentialError::InvalidFormat {\n                    name: \"access_token\".to_string()\n                })?.to_string(),\n            refresh_token: token_response.get(\"refresh_token\").and_then(|t| t.as_str()).map(|s| s.to_string()),\n            token_type: token_response.get(\"token_type\")\n                .and_then(|t| t.as_str())\n                .unwrap_or(\"Bearer\").to_string(),\n            expires_in: token_response.get(\"expires_in\")\n                .and_then(|e| e.as_i64())\n                .unwrap_or(3600),\n            scope: token_response.get(\"scope\").and_then(|s| s.as_str()).map(|s| s.to_string()),\n            issued_at: chrono::Utc::now(),\n        };\n\n        Ok(token_data)\n    }\n\n    /// Get user by ID\n    pub async fn get_user_by_id(\u0026self, user_id: \u0026str, access_token: \u0026str) -\u003e Result\u003cKeycloakUser, CredentialError\u003e {\n        let admin_api_url = format!(\"{}/admin/realms/{}/users/{}\", \n                                  self.client.config.base_url, self.client.config.realm, user_id);\n\n        let response = self.client\n            .client\n            .get(\u0026admin_api_url)\n            .bearer_auth(access_token)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get user by ID: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::NotFound {\n                name: format!(\"User {}\", user_id)\n            });\n        }\n\n        let user_data: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse user data: {}\", e)\n            })?;\n\n        let user = KeycloakUser {\n            id: user_data.get(\"id\")\n                .and_then(|i| i.as_str())\n                .unwrap_or(\"\").to_string(),\n            username: user_data.get(\"username\")\n                .and_then(|u| u.as_str())\n                .unwrap_or(\"\").to_string(),\n            email: user_data.get(\"email\").and_then(|e| e.as_str()).map(|s| s.to_string()),\n            first_name: user_data.get(\"firstName\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            last_name: user_data.get(\"lastName\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            enabled: user_data.get(\"enabled\")\n                .and_then(|e| e.as_bool())\n                .unwrap_or(true),\n            email_verified: user_data.get(\"emailVerified\")\n                .and_then(|v| v.as_bool())\n                .unwrap_or(false),\n        };\n\n        Ok(user)\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","mod.rs"],"content":"//! Credential Management Module\n//! \n//! This module provides a complete credential management system including\n//! multiple credential providers and automatic rotation capabilities.\n\nuse crate::traits::*;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\n/// Credential Information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Credential {\n    pub name: String,\n    pub values: HashMap\u003cString, String\u003e,\n    pub metadata: HashMap\u003cString, String\u003e,\n    pub created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub updated_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub expires_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    pub rotation_enabled: bool,\n    pub access_policy: Option\u003cAccessPolicy\u003e,\n}\n\n/// Access Policy for credential\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AccessPolicy {\n    pub allowed_subjects: Vec\u003cString\u003e,\n    pub read_permissions: Vec\u003cString\u003e,\n    pub write_permissions: Vec\u003cString\u003e,\n    pub rotation_permissions: Vec\u003cString\u003e,\n}\n\n/// Secret Rotation Strategy\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RotationStrategy {\n    TimeBased(TimeBasedRotation),\n    EventBased(EventBasedRotation),\n    Manual,\n}\n\n/// Time-based rotation configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TimeBasedRotation {\n    pub rotation_interval: Duration,\n    pub rotation_time: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    pub grace_period: Duration,\n}\n\n/// Event-based rotation configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct EventBasedRotation {\n    pub triggers: Vec\u003cRotationTrigger\u003e,\n    pub max_concurrent_rotations: usize,\n}\n\n/// Rotation trigger types\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RotationTrigger {\n    CredentialAccessed,\n    UnauthorizedAccessAttempt,\n    KeyCompromised,\n    CertificateExpiry(Duration),\n    ComplianceRequirement,\n}\n\n/// Rotation result information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RotationResult {\n    pub success: bool,\n    pub rotated_credential: Option\u003cString\u003e,\n    pub old_credential_version: Option\u003cString\u003e,\n    pub new_credential_version: Option\u003cString\u003e,\n    pub rotated_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub error_message: Option\u003cString\u003e,\n}\n\n/// Credential Audit Event\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AuditEvent {\n    pub event_type: CredentialEventType,\n    pub credential_name: String,\n    pub subject: String,\n    pub timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub success: bool,\n    pub details: HashMap\u003cString, String\u003e,\n}\n\n/// Types of credential events\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum CredentialEventType {\n    CredentialCreated,\n    CredentialRead,\n    CredentialUpdated,\n    CredentialDeleted,\n    RotationStarted,\n    RotationCompleted,\n    RotationFailed,\n    AccessDenied,\n}\n\n/// Versioned Credential\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct VersionedCredential {\n    pub credential: Credential,\n    pub version: String,\n    pub version_created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub is_active: bool,\n}\n\n/// Credential Provider Trait\n#[async_trait::async_trait]\npub trait CredentialProvider: Send + Sync {\n    /// Get credential by name\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e;\n    \n    /// Put/update credential\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e;\n    \n    /// List all versions of a credential\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e;\n    \n    /// List all credentials\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e;\n    \n    /// Delete credential (soft delete)\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e;\n    \n    /// Check if credential exists\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e;\n    \n    /// Rotate credential using specified strategy\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e;\n    \n    /// Get access policy for credential\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e;\n    \n    /// Set access policy for credential\n    async fn set_access_policy(\u0026self, name: \u0026str, policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e;\n    \n    /// Check if subject has permission for credential\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e;\n    \n    /// Audit credential operation\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e;\n    \n    /// Get credential statistics\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e;\n}\n\n/// Credential statistics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CredentialStatistics {\n    pub total_credentials: u64,\n    pub active_credentials: u64,\n    pub credential_versions: u64,\n    pub rotation_statistics: RotationStatistics,\n    pub last_accessed: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\n/// Rotation statistics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RotationStatistics {\n    pub total_rotations: u64,\n    pub successful_rotations: u64,\n    pub failed_rotations: u64,\n    pub last_rotation: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    pub next_scheduled_rotation: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\n/// Simple in-memory credential provider\n#[derive(Debug)]\npub struct SimpleCredentialProvider {\n    credentials: std::sync::RwLock\u003cHashMap\u003cString, Vec\u003cVersionedCredential\u003e\u003e\u003e,\n    audit_log: std::sync::RwLock\u003cVec\u003cAuditEvent\u003e\u003e,\n    rotation_config: std::sync::RwLock\u003cHashMap\u003cString, RotationStrategy\u003e\u003e,\n}\n\nimpl SimpleCredentialProvider {\n    /// Create new simple credential provider\n    pub fn new() -\u003e Self {\n        Self {\n            credentials: std::sync::RwLock::new(HashMap::new()),\n            audit_log: std::sync::RwLock::new(Vec::new()),\n            rotation_config: std::sync::RwLock::new(HashMap::new()),\n        }\n    }\n\n    /// Generate a new version identifier\n    fn generate_version() -\u003e String {\n        format!(\"v{}_{}\", \n               chrono::Utc::now().timestamp_millis(),\n               uuid::Uuid::new_v4().hyphenated().to_string()[0..8].to_string())\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for SimpleCredentialProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        let target_version = match version {\n            Some(v) =\u003e {\n                versions.iter().find(|vc| vc.version == v)\n                    .cloned()\n                    .ok_or_else(|| CredentialError::VersionNotFound { name: name.to_string(), version: v.to_string() })?\n            }\n            None =\u003e {\n                versions.iter().find(|vc| vc.is_active)\n                    .cloned()\n                    .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?\n            }\n        };\n\n        // Audit the access\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::CredentialRead,\n            credential_name: name.to_string(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::new(),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        Ok(target_version)\n    }\n\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        let version = Self::generate_version();\n        let versioned_credential = VersionedCredential {\n            credential: credential.clone(),\n            version: version.clone(),\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        };\n\n        // Deactivate previous versions\n        if let Some(versions) = credentials.get_mut(credential.name.as_str()) {\n            for v in versions.iter_mut() {\n                v.is_active = false;\n            }\n        } else {\n            credentials.insert(credential.name.clone(), Vec::new());\n        }\n\n        // Add new version\n        let versions = credentials.get_mut(credential.name.as_str()).unwrap();\n        versions.push(versioned_credential.clone());\n\n        // Audit the creation\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::CredentialCreated,\n            credential_name: credential.name.clone(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::from([\n                (\"version\".to_string(), version),\n            ]),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        Ok(versioned_credential)\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        Ok(versions.clone())\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let mut active_credentials = Vec::new();\n        for versions in credentials.values() {\n            if let Some(active) = versions.iter().find(|vc| vc.is_active) {\n                active_credentials.push(active.clone());\n            }\n        }\n        \n        Ok(active_credentials)\n    }\n\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        if let Some(versions) = credentials.get_mut(name) {\n            // Soft delete by deactivating all versions\n            for version in versions.iter_mut() {\n                version.is_active = false;\n            }\n            \n            // Audit the deletion\n            let audit_event = AuditEvent {\n                event_type: CredentialEventType::CredentialDeleted,\n                credential_name: name.to_string(),\n                subject: \"system\".to_string(),\n                timestamp: chrono::Utc::now(),\n                success: true,\n                details: HashMap::new(),\n            };\n            let _ = self.audit_operation(\u0026audit_event).await;\n            \n            Ok(())\n        } else {\n            Err(CredentialError::NotFound { name: name.to_string() })\n        }\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        Ok(credentials.contains_key(name))\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get_mut(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        // Get current active credential\n        let current_active = versions.iter()\n            .find(|vc| vc.is_active)\n            .cloned()\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n\n        // Audit rotation start\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::RotationStarted,\n            credential_name: name.to_string(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::new(),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        // Deactivate current version\n        for version in versions.iter_mut() {\n            if version.is_active {\n                version.is_active = false;\n                break;\n            }\n        }\n\n        // Create new version with updated expiry\n        let mut new_credential = current_active.credential.clone();\n        let new_expires_at = match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                Some(chrono::Utc::now() + time_based.rotation_interval)\n            }\n            RotationStrategy::EventBased(_) =\u003e {\n                // For event-based, set a reasonable default expiry\n                Some(chrono::Utc::now() + chrono::Duration::days(30))\n            }\n            RotationStrategy::Manual =\u003e {\n                new_credential.expires_at\n            }\n        };\n\n        new_credential.expires_at = new_expires_at;\n        new_credential.updated_at = chrono::Utc::now();\n\n        let new_version = Self::generate_version();\n        let new_versioned_credential = VersionedCredential {\n            credential: new_credential,\n            version: new_version.clone(),\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        };\n\n        versions.push(new_versioned_credential);\n\n        // Audit rotation completion\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::RotationCompleted,\n            credential_name: name.to_string(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::from([\n                (\"old_version\".to_string(), current_active.version),\n                (\"new_version\".to_string(), new_version),\n            ]),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        Ok(RotationResult {\n            success: true,\n            rotated_credential: Some(name.to_string()),\n            old_credential_version: Some(current_active.version),\n            new_credential_version: Some(new_version),\n            rotated_at: chrono::Utc::now(),\n            error_message: None,\n        })\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        let active = versions.iter()\n            .find(|vc| vc.is_active)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        Ok(active.credential.access_policy.clone())\n    }\n\n    async fn set_access_policy(\u0026self, name: \u0026str, policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get_mut(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        // Update active version's access policy\n        for version in versions.iter_mut() {\n            if version.is_active {\n                version.credential.access_policy = Some(policy.clone());\n                version.credential.updated_at = chrono::Utc::now();\n                break;\n            }\n        }\n\n        Ok(())\n    }\n\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        let active = versions.iter()\n            .find(|vc| vc.is_active)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        if let Some(policy) = \u0026active.credential.access_policy {\n            Ok(policy.allowed_subjects.contains(subject) \u0026\u0026\n               match permission {\n                   \"read\" =\u003e policy.read_permissions.is_empty() || policy.read_permissions.contains(permission),\n                   \"write\" =\u003e policy.write_permissions.is_empty() || policy.write_permissions.contains(permission),\n                   \"rotate\" =\u003e policy.rotation_permissions.is_empty() || policy.rotation_permissions.contains(permission),\n                   _ =\u003e false,\n               })\n        } else {\n            // Default: allow access to all subjects\n            Ok(true)\n        }\n    }\n\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut audit_log = self.audit_log.write().map_err(|_| CredentialError::InternalError)?;\n        audit_log.push(event.clone());\n        \n        // Limit audit log size to prevent memory issues\n        if audit_log.len() \u003e 10000 {\n            audit_log.drain(0..5000);\n        }\n        \n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let mut total_credentials = 0u64;\n        let mut active_credentials = 0u64;\n        let mut total_versions = 0u64;\n        let mut last_accessed = None::\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e;\n        \n        for versions in credentials.values() {\n            total_credentials += 1;\n            total_versions += versions.len() as u64;\n            \n            for version in versions {\n                if version.is_active {\n                    active_credentials += 1;\n                }\n                if last_accessed.is_none() || version.version_created_at \u003e last_accessed.unwrap() {\n                    last_accessed = Some(version.version_created_at);\n                }\n            }\n        }\n\n        let audit_log = self.audit_log.read().map_err(|_| CredentialError::InternalError)?;\n        let rotations = audit_log.iter()\n            .filter(|event| matches!(event.event_type, CredentialEventType::RotationCompleted))\n            .collect::\u003cVec\u003c_\u003e\u003e();\n\n        Ok(CredentialStatistics {\n            total_credentials,\n            active_credentials,\n            credential_versions: total_versions,\n            rotation_statistics: RotationStatistics {\n                total_rotations: rotations.len() as u64,\n                successful_rotations: rotations.iter().filter(|e| e.success).count() as u64,\n                failed_rotations: rotations.iter().filter(|e| !e.success).count() as u64,\n                last_rotation: rotations.last().map(|e| e.timestamp),\n                next_scheduled_rotation: None, // Would be calculated based on strategies\n            },\n            last_accessed,\n        })\n    }\n}\n\nimpl Default for SimpleCredentialProvider {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Extended error types for credentials\n#[derive(Debug, thiserror::Error)]\npub enum CredentialError {\n    #[error(\"Credential not found: {name}\")]\n    NotFound { name: String },\n    \n    #[error(\"Credential version not found: {name} version {version}\")]\n    VersionNotFound { name: String, version: String },\n    \n    #[error(\"Credential already exists: {name}\")]\n    AlreadyExists { name: String },\n    \n    #[error(\"Invalid credential format: {name}\")]\n    InvalidFormat { name: String },\n    \n    #[error(\"Permission denied for credential: {name}\")]\n    PermissionDenied { name: String },\n    \n    #[error(\"Credential limit exceeded: {name}\")]\n    LimitExceeded { name: String },\n    \n    #[error(\"Internal error\")]\n    InternalError,\n    \n    #[error(\"Network error: {message}\")]\n    Network { message: String },\n    \n    #[error(\"Storage error: {message}\")]\n    Storage { message: String },\n    \n    #[error(\"Encryption error: {message}\")]\n    Encryption { message: String },\n    \n    #[error(\"Rotation failed: {name}\")]\n    RotationFailed { name: String },\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","rotation.rs"],"content":"//! Automatic Credential Rotation System\n//! \n//! This module provides a complete automatic rotation system for credentials\n//! with support for time-based, event-based, and manual rotation strategies.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::{Mutex, RwLock};\nuse tokio::time::{interval, Duration};\n\n/// Rotation task state\n#[derive(Debug, Clone)]\nenum RotationTaskState {\n    Pending,\n    InProgress,\n    Completed,\n    Failed,\n    Cancelled,\n}\n\n/// Rotation task information\n#[derive(Debug, Clone)]\nstruct RotationTask {\n    id: uuid::Uuid,\n    credential_name: String,\n    strategy: RotationStrategy,\n    state: RotationTaskState,\n    created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    started_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    completed_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    result: Option\u003cRotationResult\u003e,\n    retry_count: u32,\n    max_retries: u32,\n}\n\n/// Rotation event trigger\n#[derive(Debug, Clone)]\npub struct RotationEvent {\n    pub event_type: RotationEventType,\n    pub credential_name: Option\u003cString\u003e,\n    pub metadata: HashMap\u003cString, String\u003e,\n    pub timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Types of rotation events\n#[derive(Debug, Clone)]\npub enum RotationEventType {\n    CredentialAccessed,\n    UnauthorizedAccess,\n    SecurityBreach,\n    ComplianceCheck,\n    ManualTrigger,\n    ScheduledRotation,\n    KeyRotation,\n    CertificateExpiryWarning,\n}\n\n/// Rotation statistics\n#[derive(Debug, Clone)]\nstruct RotationStatistics {\n    total_rotations: u64,\n    successful_rotations: u64,\n    failed_rotations: u64,\n    last_rotation: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    average_rotation_time: Duration,\n    active_tasks: u32,\n}\n\n/// Rotation engine configuration\n#[derive(Debug, Clone)]\npub struct RotationEngineConfig {\n    pub max_concurrent_rotations: usize,\n    pub default_retry_count: u32,\n    pub rotation_timeout: Duration,\n    pub audit_retention_days: u32,\n    pub enable_metrics: bool,\n    pub health_check_interval: Duration,\n}\n\nimpl Default for RotationEngineConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_concurrent_rotations: 5,\n            default_retry_count: 3,\n            rotation_timeout: Duration::from_secs(300), // 5 minutes\n            audit_retention_days: 90,\n            enable_metrics: true,\n            health_check_interval: Duration::from_secs(60),\n        }\n    }\n}\n\n/// Main rotation engine\n#[derive(Debug)]\npub struct RotationEngine {\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    tasks: Arc\u003cMutex\u003cHashMap\u003cuuid::Uuid, RotationTask\u003e\u003e\u003e,\n    config: RotationEngineConfig,\n    statistics: Arc\u003cRwLock\u003cRotationStatistics\u003e\u003e,\n    audit_log: Arc\u003cMutex\u003cVec\u003cRotationAuditEvent\u003e\u003e\u003e,\n    rotation_strategies: Arc\u003cRwLock\u003cHashMap\u003cString, RotationStrategy\u003e\u003e\u003e,\n    event_handlers: Arc\u003cRwLock\u003cVec\u003cBox\u003cdyn RotationEventHandler + Send + Sync\u003e\u003e\u003e\u003e,\n    health_status: Arc\u003cRwLock\u003cRotationHealthStatus\u003e\u003e,\n    shutdown_tx: Arc\u003cMutex\u003cOption\u003ctokio::sync::oneshot::Sender\u003c()\u003e\u003e\u003e\u003e,\n}\n\n/// Rotation health status\n#[derive(Debug, Clone)]\nstruct RotationHealthStatus {\n    is_healthy: bool,\n    last_health_check: chrono::DateTime\u003cchrono::Utc\u003e,\n    issues: Vec\u003cString\u003e,\n    running_tasks: u32,\n}\n\n/// Rotation audit event\n#[derive(Debug, Clone)]\nstruct RotationAuditEvent {\n    task_id: uuid::Uuid,\n    credential_name: String,\n    event_type: RotationAuditEventType,\n    details: HashMap\u003cString, String\u003e,\n    timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n#[derive(Debug, Clone)]\nenum RotationAuditEventType {\n    TaskCreated,\n    TaskStarted,\n    TaskCompleted,\n    TaskFailed,\n    TaskRetried,\n    TaskCancelled,\n    StrategyUpdated,\n    EventTriggered,\n}\n\nimpl RotationEngine {\n    /// Create new rotation engine\n    pub fn new(\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n        config: RotationEngineConfig,\n    ) -\u003e Self {\n        Self {\n            credential_provider,\n            tasks: Arc::new(Mutex::new(HashMap::new())),\n            config,\n            statistics: Arc::new(RwLock::new(RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                average_rotation_time: Duration::from_secs(0),\n                active_tasks: 0,\n            })),\n            audit_log: Arc::new(Mutex::new(Vec::new())),\n            rotation_strategies: Arc::new(RwLock::new(HashMap::new())),\n            event_handlers: Arc::new(RwLock::new(Vec::new())),\n            health_status: Arc::new(RwLock::new(RotationHealthStatus {\n                is_healthy: true,\n                last_health_check: chrono::Utc::now(),\n                issues: Vec::new(),\n                running_tasks: 0,\n            })),\n            shutdown_tx: Arc::new(Mutex::new(None)),\n        }\n    }\n\n    /// Start the rotation engine\n    pub async fn start(\u0026self) -\u003e Result\u003c(), CredentialError\u003e {\n        let (shutdown_tx, shutdown_rx) = tokio::sync::oneshot::channel();\n        \n        {\n            let mut tx = self.shutdown_tx.lock().await;\n            *tx = Some(shutdown_tx);\n        }\n\n        // Start background tasks\n        let rotation_engine = self.clone();\n        let rotation_task = tokio::spawn(async move {\n            rotation_engine.rotation_loop(shutdown_rx).await;\n        });\n\n        let health_engine = self.clone();\n        let health_task = tokio::spawn(async move {\n            health_engine.health_check_loop().await;\n        });\n\n        // Wait for completion\n        tokio::select! {\n            result = rotation_task =\u003e {\n                if let Err(e) = result {\n                    eprintln!(\"Rotation task failed: {:?}\", e);\n                }\n            }\n            result = health_task =\u003e {\n                if let Err(e) = result {\n                    eprintln!(\"Health check task failed: {:?}\", e);\n                }\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Stop the rotation engine gracefully\n    pub async fn stop(\u0026self) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut shutdown_tx = self.shutdown_tx.lock().await;\n        if let Some(tx) = shutdown_tx.take() {\n            let _ = tx.send(());\n        }\n        Ok(())\n    }\n\n    /// Clone with arc references\n    fn clone(\u0026self) -\u003e Self {\n        Self {\n            credential_provider: self.credential_provider.clone(),\n            tasks: self.tasks.clone(),\n            config: self.config.clone(),\n            statistics: self.statistics.clone(),\n            audit_log: self.audit_log.clone(),\n            rotation_strategies: self.rotation_strategies.clone(),\n            event_handlers: self.event_handlers.clone(),\n            health_status: self.health_status.clone(),\n            shutdown_tx: self.shutdown_tx.clone(),\n        }\n    }\n\n    /// Main rotation loop\n    async fn rotation_loop(\u0026self, shutdown_rx: tokio::sync::oneshot::Receiver\u003c()\u003e) {\n        let mut time_based_timer = interval(Duration::from_secs(60)); // Check every minute\n        let mut shutdown_signal = shutdown_rx;\n\n        loop {\n            tokio::select! {\n                _ = time_based_timer.tick() =\u003e {\n                    self.process_time_based_rotations().await;\n                    self.process_pending_tasks().await;\n                }\n                _ = \u0026mut shutdown_signal =\u003e {\n                    break;\n                }\n            }\n        }\n\n        // Graceful shutdown: wait for active tasks to complete\n        let timeout = tokio::time::sleep(Duration::from_secs(60));\n        tokio::select! {\n            _ = timeout =\u003e {\n                // Force shutdown\n            }\n            _ = self.wait_for_active_tasks() =\u003e {\n                // All tasks completed\n            }\n        }\n    }\n\n    /// Health check loop\n    async fn health_check_loop(\u0026self) {\n        let mut timer = interval(self.config.health_check_interval);\n        \n        loop {\n            timer.tick().await;\n            \n            // Check engine health\n            let active_tasks = self.get_active_task_count().await;\n            let mut issues = Vec::new();\n            \n            if active_tasks \u003e self.config.max_concurrent_rotations {\n                issues.push(format!(\"Too many active tasks: {}\", active_tasks));\n            }\n            \n            // Update health status\n            {\n                let mut status = self.health_status.write().await;\n                status.is_healthy = issues.is_empty();\n                status.last_health_check = chrono::Utc::now();\n                status.issues = issues.clone();\n                status.running_tasks = active_tasks;\n            }\n        }\n    }\n\n    /// Process time-based rotations\n    async fn process_time_based_rotations(\u0026self) {\n        let strategies = self.rotation_strategies.read().await;\n        let now = chrono::Utc::now();\n\n        for (credential_name, strategy) in strategies.iter() {\n            if let RotationStrategy::TimeBased(time_based) = strategy {\n                // Check if rotation is due\n                if let Ok(current_credential) = self.credential_provider.get_credential(credential_name, None).await {\n                    let next_rotation = current_credential.credential.updated_at + time_based.rotation_interval;\n                    \n                    if now \u003e= next_rotation {\n                        let _ = self.schedule_rotation(credential_name, strategy.clone()).await;\n                    }\n                }\n            }\n        }\n    }\n\n    /// Process pending rotation tasks\n    async fn process_pending_tasks(\u0026self) {\n        let mut tasks = self.tasks.lock().await;\n        \n        // Collect pending tasks\n        let pending_tasks: Vec\u003cuuid::Uuid\u003e = tasks.values()\n            .filter(|task| matches!(task.state, RotationTaskState::Pending))\n            .map(|task| task.id)\n            .collect();\n\n        // Process each pending task\n        for task_id in pending_tasks {\n            let active_count = tasks.values().filter(|t| matches!(t.state, RotationTaskState::InProgress)).count();\n            \n            if active_count \u003c self.config.max_concurrent_rotations {\n                if let Some(task) = tasks.get_mut(\u0026task_id) {\n                    task.state = RotationTaskState::InProgress;\n                    task.started_at = Some(chrono::Utc::now());\n                    \n                    // Start task execution in background\n                    let engine_clone = self.clone();\n                    let task_clone = task.clone();\n                    tokio::spawn(async move {\n                        engine_clone.execute_rotation_task(task_clone).await;\n                    });\n                }\n            }\n        }\n    }\n\n    /// Execute rotation task\n    async fn execute_rotation_task(\u0026self, task: RotationTask) {\n        let start_time = chrono::Utc::now();\n        \n        // Update task state\n        {\n            let mut tasks = self.tasks.lock().await;\n            if let Some(existing_task) = tasks.get_mut(\u0026task.id) {\n                existing_task.state = RotationTaskState::InProgress;\n                existing_task.started_at = Some(start_time);\n            }\n        }\n\n        // Audit task start\n        self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskStarted).await;\n\n        let result = self.perform_rotation(\u0026task.credential_name, \u0026task.strategy).await;\n\n        // Update task state and statistics\n        {\n            let mut tasks = self.tasks.lock().await;\n            let mut task_update = tasks.get_mut(\u0026task.id).unwrap();\n            \n            match result {\n                Ok(rotation_result) =\u003e {\n                    task_update.state = RotationTaskState::Completed;\n                    task_update.completed_at = Some(chrono::Utc::now());\n                    task_update.result = Some(rotation_result);\n                    \n                    // Update statistics\n                    self.update_statistics(true, start_time).await;\n                    \n                    // Audit completion\n                    self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskCompleted).await;\n                }\n                Err(error) =\u003e {\n                    task_update.retry_count += 1;\n                    \n                    if task_update.retry_count \u003c task_update.max_retries {\n                        task_update.state = RotationTaskState::Pending;\n                        self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskRetried).await;\n                    } else {\n                        task_update.state = RotationTaskState::Failed;\n                        task_update.completed_at = Some(chrono::Utc::now());\n                        self.update_statistics(false, start_time).await;\n                        self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskFailed).await;\n                    }\n                }\n            }\n        }\n    }\n\n    /// Perform actual rotation\n    async fn perform_rotation(\u0026self, credential_name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        match self.credential_provider.rotate_credential(credential_name, strategy).await {\n            Ok(result) =\u003e {\n                // Trigger event handlers\n                let event = RotationEvent {\n                    event_type: RotationEventType::ScheduledRotation,\n                    credential_name: Some(credential_name.to_string()),\n                    metadata: HashMap::new(),\n                    timestamp: chrono::Utc::now(),\n                };\n                \n                self.trigger_event_handlers(\u0026event).await;\n                \n                Ok(result)\n            }\n            Err(error) =\u003e {\n                // Trigger failure event handlers\n                let event = RotationEvent {\n                    event_type: RotationEventType::SecurityBreach,\n                    credential_name: Some(credential_name.to_string()),\n                    metadata: HashMap::from([\n                        (\"error\".to_string(), error.to_string()),\n                        (\"failure_type\".to_string(), \"rotation_failed\".to_string()),\n                    ]),\n                    timestamp: chrono::Utc::now(),\n                };\n                \n                self.trigger_event_handlers(\u0026event).await;\n                \n                Err(error)\n            }\n        }\n    }\n\n    /// Schedule rotation task\n    pub async fn schedule_rotation(\u0026self, credential_name: \u0026str, strategy: RotationStrategy) -\u003e Result\u003cuuid::Uuid, CredentialError\u003e {\n        let task = RotationTask {\n            id: uuid::Uuid::new_v4(),\n            credential_name: credential_name.to_string(),\n            strategy,\n            state: RotationTaskState::Pending,\n            created_at: chrono::Utc::now(),\n            started_at: None,\n            completed_at: None,\n            result: None,\n            retry_count: 0,\n            max_retries: self.config.default_retry_count,\n        };\n\n        {\n            let mut tasks = self.tasks.lock().await;\n            tasks.insert(task.id, task.clone());\n        }\n\n        // Audit task creation\n        self.audit_event(task.id.clone(), credential_name, RotationAuditEventType::TaskCreated).await;\n\n        Ok(task.id)\n    }\n\n    /// Trigger event-based rotation\n    pub async fn trigger_event_rotation(\u0026self, event: RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // Audit event\n        if let Some(ref credential_name) = event.credential_name {\n            self.audit_event(uuid::Uuid::new_v4(), credential_name, RotationAuditEventType::EventTriggered).await;\n        }\n\n        // Trigger event handlers\n        self.trigger_event_handlers(\u0026event).await;\n\n        // Schedule rotation if it's an event that requires it\n        if matches!(event.event_type, RotationEventType::UnauthorizedAccess | \n                   RotationEventType::SecurityBreach | \n                   RotationEventType::ManualTrigger) {\n            if let Some(credential_name) = \u0026event.credential_name {\n                // Use event-based strategy\n                let strategy = RotationStrategy::EventBased(EventBasedRotation {\n                    triggers: vec![event.event_type],\n                    max_concurrent_rotations: 1,\n                });\n                \n                let _ = self.schedule_rotation(credential_name, strategy).await;\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Update rotation strategy for a credential\n    pub async fn update_rotation_strategy(\u0026self, credential_name: \u0026str, strategy: RotationStrategy) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut strategies = self.rotation_strategies.write().await;\n        strategies.insert(credential_name.to_string(), strategy.clone());\n        \n        // Audit strategy update\n        self.audit_event(uuid::Uuid::new_v4(), credential_name, RotationAuditEventType::StrategyUpdated).await;\n\n        Ok(())\n    }\n\n    /// Add event handler\n    pub async fn add_event_handler(\u0026self, handler: Box\u003cdyn RotationEventHandler + Send + Sync\u003e) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut handlers = self.event_handlers.write().await;\n        handlers.push(handler);\n        Ok(())\n    }\n\n    /// Get rotation status\n    pub async fn get_status(\u0026self) -\u003e RotationEngineStatus {\n        let tasks = self.tasks.lock().await;\n        let statistics = self.statistics.read().await;\n        let health_status = self.health_status.read().await;\n\n        RotationEngineStatus {\n            total_tasks: tasks.len() as u32,\n            pending_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::Pending)).count() as u32,\n            active_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::InProgress)).count() as u32,\n            completed_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::Completed)).count() as u32,\n            failed_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::Failed)).count() as u32,\n            statistics: statistics.clone(),\n            health_status: health_status.clone(),\n            configured_credentials: self.rotation_strategies.read().await.len() as u32,\n        }\n    }\n\n    /// Get task details\n    pub async fn get_task(\u0026self, task_id: \u0026uuid::Uuid) -\u003e Option\u003cRotationTask\u003e {\n        let tasks = self.tasks.lock().await;\n        tasks.get(task_id).cloned()\n    }\n\n    /// List all tasks\n    pub async fn list_tasks(\u0026self, filter: Option\u003cRotationTaskState\u003e) -\u003e Vec\u003cRotationTask\u003e {\n        let tasks = self.tasks.lock().await;\n        let mut task_list: Vec\u003cRotationTask\u003e = tasks.values().cloned().collect();\n        \n        if let Some(state) = filter {\n            task_list.retain(|task| matches!(task.state, state));\n        }\n        \n        task_list\n    }\n\n    /// Cancel rotation task\n    pub async fn cancel_task(\u0026self, task_id: \u0026uuid::Uuid) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut tasks = self.tasks.lock().await;\n        if let Some(task) = tasks.get_mut(task_id) {\n            match task.state {\n                RotationTaskState::Pending | RotationTaskState::InProgress =\u003e {\n                    task.state = RotationTaskState::Cancelled;\n                    self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskCancelled).await;\n                    Ok(())\n                }\n                RotationTaskState::Completed | RotationTaskState::Failed | RotationTaskState::Cancelled =\u003e {\n                    Err(CredentialError::PermissionDenied {\n                        name: \"Cannot cancel completed or failed task\".to_string()\n                    })\n                }\n            }\n        } else {\n            Err(CredentialError::NotFound {\n                name: format!(\"Task {}\", task_id)\n            })\n        }\n    }\n\n    // Helper methods\n    async fn get_active_task_count(\u0026self) -\u003e u32 {\n        let tasks = self.tasks.lock().await;\n        tasks.values().filter(|t| matches!(t.state, RotationTaskState::InProgress)).count() as u32\n    }\n\n    async fn wait_for_active_tasks(\u0026self) {\n        loop {\n            let active_count = self.get_active_task_count().await;\n            if active_count == 0 {\n                break;\n            }\n            tokio::time::sleep(Duration::from_secs(1)).await;\n        }\n    }\n\n    async fn update_statistics(\u0026self, success: bool, start_time: chrono::DateTime\u003cchrono::Utc\u003e) {\n        let mut stats = self.statistics.write().await;\n        let duration = chrono::Utc::now() - start_time;\n        let duration_secs = duration.num_seconds() as u64;\n\n        stats.total_rotations += 1;\n        if success {\n            stats.successful_rotations += 1;\n        } else {\n            stats.failed_rotations += 1;\n        }\n        stats.last_rotation = Some(chrono::Utc::now());\n\n        // Update average time (simple moving average)\n        let current_avg = stats.average_rotation_time.as_secs();\n        let new_avg = (current_avg * (stats.total_rotations - 1) + duration_secs) / stats.total_rotations;\n        stats.average_rotation_time = Duration::from_secs(new_avg);\n    }\n\n    async fn audit_event(\u0026self, task_id: uuid::Uuid, credential_name: \u0026str, event_type: RotationAuditEventType) {\n        let audit_event = RotationAuditEvent {\n            task_id,\n            credential_name: credential_name.to_string(),\n            event_type,\n            details: HashMap::new(),\n            timestamp: chrono::Utc::now(),\n        };\n\n        let mut log = self.audit_log.lock().await;\n        log.push(audit_event);\n\n        // Limit audit log size\n        if log.len() \u003e 10000 {\n            log.drain(0..5000);\n        }\n    }\n\n    async fn trigger_event_handlers(\u0026self, event: \u0026RotationEvent) {\n        let handlers = self.event_handlers.read().await;\n        for handler in handlers.iter() {\n            let _ = handler.handle_event(event).await;\n        }\n    }\n}\n\n/// Rotation event handler trait\n#[async_trait]\npub trait RotationEventHandler: Send + Sync {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e;\n}\n\n/// Rotation engine status\n#[derive(Debug, Clone)]\npub struct RotationEngineStatus {\n    pub total_tasks: u32,\n    pub pending_tasks: u32,\n    pub active_tasks: u32,\n    pub completed_tasks: u32,\n    pub failed_tasks: u32,\n    pub statistics: RotationStatistics,\n    pub health_status: RotationHealthStatus,\n    pub configured_credentials: u32,\n}\n\n/// Built-in rotation event handlers\n\n/// Keycloak rotation event handler\npub struct KeycloakRotationHandler {\n    keycloak_client: Arc\u003csuper::keycloak::KeycloakClient\u003e,\n}\n\nimpl KeycloakRotationHandler {\n    pub fn new(keycloak_client: Arc\u003csuper::keycloak::KeycloakClient\u003e) -\u003e Self {\n        Self { keycloak_client }\n    }\n}\n\n#[async_trait::async_trait]\nimpl RotationEventHandler for KeycloakRotationHandler {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        match event.event_type {\n            RotationEventType::UnauthorizedAccess =\u003e {\n                // Trigger immediate token refresh\n                {\n                    let mut cache = self.keycloak_client.token_cache.write().map_err(|_| CredentialError::InternalError)?;\n                    cache.clear();\n                }\n                Ok(())\n            }\n            _ =\u003e Ok(()),\n        }\n    }\n}\n\n/// AWS Secrets Manager rotation event handler\npub struct AWSRotationHandler {\n    // Would include AWS-specific configuration\n    #[allow(dead_code)]\n    region: String,\n}\n\nimpl AWSRotationHandler {\n    pub fn new(region: String) -\u003e Self {\n        Self { region }\n    }\n}\n\n#[async_trait::async_trait]\nimpl RotationEventHandler for AWSRotationHandler {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        match event.event_type {\n            RotationEventType::ComplianceCheck =\u003e {\n                // Log compliance check event\n                println!(\"AWS compliance check triggered for credential: {:?}\", event.credential_name);\n                Ok(())\n            }\n            _ =\u003e Ok(()),\n        }\n    }\n}\n\n/// Vault rotation event handler\npub struct VaultRotationHandler {\n    // Would include Vault-specific configuration\n    #[allow(dead_code)]\n    vault_url: String,\n}\n\nimpl VaultRotationHandler {\n    pub fn new(vault_url: String) -\u003e Self {\n        Self { vault_url }\n    }\n}\n\n#[async_trait::async_trait]\nimpl RotationEventHandler for VaultRotationHandler {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        match event.event_type {\n            RotationEventType::KeyRotation =\u003e {\n                // Trigger key rotation in Vault transit engine\n                println!(\"Vault key rotation triggered for credential: {:?}\", event.credential_name);\n                Ok(())\n            }\n            _ =\u003e Ok(()),\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","vault.rs"],"content":"//! HashiCorp Vault Provider Implementation\n//! \n//! This module provides a complete integration with HashiCorp Vault for\n//! secure credential management and storage.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::time::Duration;\n\n/// Vault-specific configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct VaultConfig {\n    pub vault_url: String,\n    pub auth_method: VaultAuthMethod,\n    pub timeout: Duration,\n    pub max_retries: u32,\n    pub secrets_engine: String,\n    pub transit_engine: Option\u003cString\u003e,\n    pub kv_version: KVVersion,\n}\n\n/// Vault authentication methods\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum VaultAuthMethod {\n    Token { token: String },\n    AppRole { role_id: String, secret_id: String },\n    Kubernetes { jwt_path: String, role: String },\n    Azure { client_id: String, client_secret: String, tenant_id: String },\n}\n\n/// KV engine version\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum KVVersion {\n    V1,\n    V2,\n}\n\n/// Vault API client wrapper\n#[derive(Debug)]\npub struct VaultClient {\n    client: reqwest::Client,\n    config: VaultConfig,\n    token: Option\u003cString\u003e,\n    token_expiry: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\nimpl VaultClient {\n    /// Create new Vault client\n    pub async fn new(config: VaultConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = reqwest::Client::builder()\n            .timeout(config.timeout)\n            .danger_accept_invalid_certs(false)\n            .build()\n            .map_err(|e| CredentialError::Network { \n                message: format!(\"Failed to create HTTP client: {}\", e) \n            })?;\n\n        let mut vault_client = Self {\n            client,\n            config: config.clone(),\n            token: None,\n            token_expiry: None,\n        };\n\n        // Authenticate using configured method\n        vault_client.authenticate().await?;\n\n        Ok(vault_client)\n    }\n\n    /// Authenticate with Vault\n    async fn authenticate(\u0026mut self) -\u003e Result\u003c(), CredentialError\u003e {\n        match \u0026self.config.auth_method {\n            VaultAuthMethod::Token { token } =\u003e {\n                self.token = Some(token.clone());\n                self.token_expiry = Some(chrono::Utc::now() + chrono::Duration::hours(24));\n            }\n            VaultAuthMethod::AppRole { role_id, secret_id } =\u003e {\n                let auth_data = HashMap::from([\n                    (\"role_id\", role_id.clone()),\n                    (\"secret_id\", secret_id.clone()),\n                ]);\n                \n                let response = self.client\n                    .post(\u0026format!(\"{}/v1/auth/approle/login\", self.config.vault_url))\n                    .json(\u0026auth_data)\n                    .send()\n                    .await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Vault authentication failed: {}\", e)\n                    })?;\n\n                if !response.status().is_success() {\n                    return Err(CredentialError::Authentication {\n                        message: \"Failed to authenticate with Vault\".to_string(),\n                    });\n                }\n\n                let auth_response: serde_json::Value = response.json().await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Failed to parse Vault auth response: {}\", e)\n                    })?;\n\n                if let Some(auth) = auth_response.get(\"auth\") {\n                    if let Some(token) = auth.get(\"client_token\").and_then(|t| t.as_str()) {\n                        if let Some(lease_duration) = auth.get(\"lease_duration\").and_then(|d| d.as_u64()) {\n                            self.token_expiry = Some(chrono::Utc::now() + chrono::Duration::seconds(lease_duration as i64));\n                        }\n                        self.token = Some(token.to_string());\n                    }\n                }\n            }\n            VaultAuthMethod::Kubernetes { jwt_path, role } =\u003e {\n                let jwt = tokio::fs::read_to_string(jwt_path).await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Failed to read JWT file: {}\", e)\n                    })?;\n\n                let auth_data = HashMap::from([\n                    (\"role\", role.clone()),\n                    (\"jwt\", jwt.trim().to_string()),\n                ]);\n\n                let response = self.client\n                    .post(\u0026format!(\"{}/v1/auth/kubernetes/login\", self.config.vault_url))\n                    .json(\u0026auth_data)\n                    .send()\n                    .await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Vault Kubernetes authentication failed: {}\", e)\n                    })?;\n\n                if !response.status().is_success() {\n                    return Err(CredentialError::Authentication {\n                        message: \"Failed to authenticate with Vault via Kubernetes\".to_string(),\n                    });\n                }\n\n                let auth_response: serde_json::Value = response.json().await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Failed to parse Vault auth response: {}\", e)\n                    })?;\n\n                if let Some(auth) = auth_response.get(\"auth\") {\n                    if let Some(token) = auth.get(\"client_token\").and_then(|t| t.as_str()) {\n                        if let Some(lease_duration) = auth.get(\"lease_duration\").and_then(|d| d.as_u64()) {\n                            self.token_expiry = Some(chrono::Utc::now() + chrono::Duration::seconds(lease_duration as i64));\n                        }\n                        self.token = Some(token.to_string());\n                    }\n                }\n            }\n            _ =\u003e {\n                return Err(CredentialError::Configuration {\n                    message: \"Unsupported authentication method\".to_string(),\n                });\n            }\n        }\n\n        if self.token.is_none() {\n            return Err(CredentialError::Authentication {\n                message: \"Failed to obtain Vault token\".to_string(),\n            });\n        }\n\n        Ok(())\n    }\n\n    /// Ensure we have a valid token\n    async fn ensure_authenticated(\u0026mut self) -\u003e Result\u003c(), CredentialError\u003e {\n        if let Some(expiry) = self.token_expiry {\n            if chrono::Utc::now() \u003e expiry - chrono::Duration::minutes(5) {\n                self.authenticate().await?;\n            }\n        }\n        Ok(())\n    }\n\n    /// Make authenticated request to Vault\n    async fn request(\u0026mut self, method: reqwest::Method, path: \u0026str, body: Option\u003cserde_json::Value\u003e) -\u003e Result\u003creqwest::Response, CredentialError\u003e {\n        self.ensure_authenticated().await?;\n\n        let url = if path.starts_with(\"/v1/\") {\n            format!(\"{}{}\", self.config.vault_url, path)\n        } else {\n            format!(\"{}/v1/{}\", self.config.vault_url, path)\n        };\n\n        let mut request = self.client.request(method, \u0026url);\n        \n        if let Some(token) = \u0026self.token {\n            request = request.bearer_auth(token);\n        }\n\n        if let Some(body) = body {\n            request = request.json(\u0026body);\n        }\n\n        request.send().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Vault API request failed: {}\", e)\n            })\n    }\n\n    /// Get secret from Vault\n    pub async fn get_secret(\u0026mut self, path: \u0026str) -\u003e Result\u003cserde_json::Value, CredentialError\u003e {\n        let response = self.request(reqwest::Method::GET, path, None).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to get secret from Vault: HTTP {}\", response.status())\n            });\n        }\n\n        response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Vault response: {}\", e)\n            })\n    }\n\n    /// Put secret to Vault\n    pub async fn put_secret(\u0026mut self, path: \u0026str, secret_data: \u0026HashMap\u003cString, String\u003e) -\u003e Result\u003c(), CredentialError\u003e {\n        let secret_payload = match self.config.kv_version {\n            KVVersion::V1 =\u003e {\n                serde_json::to_value(secret_data).map_err(|e| CredentialError::Storage {\n                    message: format!(\"Failed to serialize secret data: {}\", e)\n                })?\n            }\n            KVVersion::V2 =\u003e {\n                serde_json::json!({ \"data\": secret_data })\n            }\n        };\n\n        let response = self.request(reqwest::Method::PUT, path, Some(secret_payload)).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to put secret to Vault: HTTP {}\", response.status())\n            });\n        }\n\n        Ok(())\n    }\n\n    /// List secrets at path\n    pub async fn list_secrets(\u0026mut self, path: \u0026str) -\u003e Result\u003cVec\u003cString\u003e, CredentialError\u003e {\n        let list_path = if self.config.kv_version == KVVersion::V2 {\n            format!(\"{}/metadata\", path)\n        } else {\n            path.to_string()\n        };\n\n        let response = self.request(reqwest::Method::GET, \u0026format!(\"{}?list=1\", list_path), None).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to list secrets from Vault: HTTP {}\", response.status())\n            });\n        }\n\n        let response_data: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Vault list response: {}\", e)\n            })?;\n\n        let mut keys = Vec::new();\n        if let Some(data) = response_data.get(\"data\") {\n            if let Some(key_list) = data.get(\"keys\").and_then(|k| k.as_array()) {\n                for key in key_list {\n                    if let Some(key_str) = key.as_str() {\n                        keys.push(key_str.to_string());\n                    }\n                }\n            }\n        }\n\n        Ok(keys)\n    }\n\n    /// Delete secret from Vault\n    pub async fn delete_secret(\u0026mut self, path: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let delete_path = match self.config.kv_version {\n            KVVersion::V1 =\u003e path.to_string(),\n            KVVersion::V2 =\u003e format!(\"{}/metadata\", path),\n        };\n\n        let response = self.request(reqwest::Method::DELETE, \u0026delete_path, None).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to delete secret from Vault: HTTP {}\", response.status())\n            });\n        }\n\n        Ok(())\n    }\n}\n\n/// HashiCorp Vault Credential Provider\n#[derive(Debug)]\npub struct HashiCorpVaultProvider {\n    client: Arc\u003cstd::sync::Mutex\u003cVaultClient\u003e\u003e,\n    config: VaultConfig,\n}\n\nimpl HashiCorpVaultProvider {\n    /// Create new HashiCorp Vault Provider\n    pub async fn new(config: VaultConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = VaultClient::new(config.clone()).await?;\n        Ok(Self {\n            client: Arc::new(std::sync::Mutex::new(client)),\n            config,\n        })\n    }\n\n    /// Convert secret data from Vault format to Credential format\n    fn vault_data_to_credential(\u0026self, vault_data: \u0026serde_json::Value) -\u003e Result\u003cHashMap\u003cString, String\u003e, CredentialError\u003e {\n        let mut values = HashMap::new();\n\n        if let Some(data) = vault_data.get(\"data\") {\n            if data.is_object() {\n                for (key, value) in data.as_object().unwrap() {\n                    if let Some(value_str) = value.as_str() {\n                        values.insert(key.clone(), value_str.to_string());\n                    } else if value.is_number() {\n                        values.insert(key.clone(), value.to_string());\n                    } else if value.is_boolean() {\n                        values.insert(key.clone(), value.to_string());\n                    } else {\n                        values.insert(key.clone(), serde_json::to_string(value).unwrap_or_else(|_| \"null\".to_string()));\n                    }\n                }\n            }\n        } else {\n            // For V1, the root object is the data\n            if vault_data.is_object() {\n                for (key, value) in vault_data.as_object().unwrap() {\n                    if let Some(value_str) = value.as_str() {\n                        values.insert(key.clone(), value_str.to_string());\n                    } else if value.is_number() {\n                        values.insert(key.clone(), value.to_string());\n                    } else if value.is_boolean() {\n                        values.insert(key.clone(), value.to_string());\n                    } else {\n                        values.insert(key.clone(), serde_json::to_string(value).unwrap_or_else(|_| \"null\".to_string()));\n                    }\n                }\n            }\n        }\n\n        Ok(values)\n    }\n\n    /// Convert Credential to Vault format\n    fn credential_to_vault_data(\u0026self, credential: \u0026Credential) -\u003e HashMap\u003cString, String\u003e {\n        let mut vault_data = HashMap::new();\n        \n        // Copy all values\n        for (key, value) in \u0026credential.values {\n            vault_data.insert(key.clone(), value.clone());\n        }\n        \n        // Add metadata\n        if let Some(policy) = \u0026credential.access_policy {\n            vault_data.insert(\"rotation_enabled\".to_string(), credential.rotation_enabled.to_string());\n            vault_data.insert(\"expires_at\".to_string(), \n                credential.expires_at.map(|e| e.to_rfc3339()).unwrap_or_else(|| \"null\".to_string()));\n            \n            // Store access policy as JSON\n            let policy_json = serde_json::to_string(policy)\n                .unwrap_or_else(|_| \"{}\".to_string());\n            vault_data.insert(\"access_policy\".to_string(), policy_json);\n        }\n        \n        vault_data\n    }\n\n    /// Get secret path in Vault\n    fn get_secret_path(\u0026self, name: \u0026str) -\u003e String {\n        format!(\"{}/{}\", self.config.secrets_engine, name)\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for HashiCorpVaultProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let path = self.get_secret_path(name);\n        let vault_data = client.get_secret(\u0026path).await?;\n        \n        let values = self.vault_data_to_credential(\u0026vault_data)?;\n        \n        // Extract metadata\n        let mut credential_values = HashMap::new();\n        let mut metadata = HashMap::new();\n        let mut rotation_enabled = false;\n        let mut expires_at = None;\n        let mut access_policy = None;\n\n        for (key, value) in values {\n            match key.as_str() {\n                \"rotation_enabled\" =\u003e rotation_enabled = value.parse().unwrap_or(false),\n                \"expires_at\" =\u003e {\n                    if value != \"null\" {\n                        expires_at = chrono::DateTime::parse_from_rfc3339(\u0026value)\n                            .ok()\n                            .map(|dt| dt.with_timezone(\u0026chrono::Utc));\n                    }\n                }\n                \"access_policy\" =\u003e {\n                    if value != \"{}\" {\n                        access_policy = serde_json::from_str(\u0026value).ok();\n                    }\n                }\n                _ =\u003e credential_values.insert(key.clone(), value),\n            }\n        }\n\n        let credential = Credential {\n            name: name.to_string(),\n            values: credential_values,\n            metadata,\n            created_at: chrono::Utc::now(),\n            updated_at: chrono::Utc::now(),\n            expires_at,\n            rotation_enabled,\n            access_policy,\n        };\n\n        let version = version.unwrap_or(\"current\").to_string();\n        \n        Ok(VersionedCredential {\n            credential,\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let path = self.get_secret_path(\u0026credential.name);\n        let vault_data = self.credential_to_vault_data(credential);\n        \n        client.put_secret(\u0026path, \u0026vault_data).await?;\n        \n        let version = format!(\"v{}_{}\", chrono::Utc::now().timestamp_millis(), chrono::Utc::now().timestamp_nanos());\n        \n        Ok(VersionedCredential {\n            credential: credential.clone(),\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        // Note: Vault V1 doesn't support version listing\n        // This would need to be implemented differently for V1 vs V2\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        // For simplicity, return current version only\n        let current = self.get_credential(name, None).await?;\n        Ok(vec![current])\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let secrets = client.list_secrets(\u0026self.config.secrets_engine).await?;\n        \n        let mut credentials = Vec::new();\n        for secret_name in secrets {\n            if let Ok(version) = self.get_credential(\u0026secret_name, None).await {\n                credentials.push(version);\n            }\n        }\n        \n        Ok(credentials)\n    }\n\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let path = self.get_secret_path(name);\n        client.delete_secret(\u0026path).await?;\n        \n        Ok(())\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        // Simple existence check by attempting to read\n        match self.get_credential(name, None).await {\n            Ok(_) =\u003e Ok(true),\n            Err(CredentialError::NotFound { .. }) =\u003e Ok(false),\n            Err(e) =\u003e Err(e),\n        }\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        // Get current credential\n        let current = self.get_credential(name, None).await?;\n        \n        // Create rotated version\n        let mut rotated_credential = current.credential.clone();\n        rotated_credential.updated_at = chrono::Utc::now();\n        \n        match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                rotated_credential.expires_at = Some(chrono::Utc::now() + time_based.rotation_interval);\n            }\n            RotationStrategy::EventBased(_) =\u003e {\n                // For event-based, rotate immediately\n                rotated_credential.expires_at = Some(chrono::Utc::now() + chrono::Duration::days(30));\n            }\n            RotationStrategy::Manual =\u003e {\n                // Keep current expiry\n            }\n        }\n\n        // Store rotated credential\n        let rotated_version = self.put_credential(\u0026rotated_credential).await?;\n        \n        Ok(RotationResult {\n            success: true,\n            rotated_credential: Some(name.to_string()),\n            old_credential_version: Some(current.version),\n            new_credential_version: Some(rotated_version.version),\n            rotated_at: chrono::Utc::now(),\n            error_message: None,\n        })\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        let version = self.get_credential(name, None).await?;\n        Ok(version.credential.access_policy)\n    }\n\n    async fn set_access_policy(\u0026self, name: \u0026str, policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut version = self.get_credential(name, None).await?;\n        version.credential.access_policy = Some(policy.clone());\n        version.credential.updated_at = chrono::Utc::now();\n        \n        let _ = self.put_credential(\u0026version.credential).await?;\n        \n        Ok(())\n    }\n\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let version = self.get_credential(name, None).await?;\n        \n        if let Some(policy) = \u0026version.credential.access_policy {\n            Ok(policy.allowed_subjects.contains(subject))\n        } else {\n            // Default policy: allow all access\n            Ok(true)\n        }\n    }\n\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // In a production environment, you would log this to a secure audit system\n        // For now, just return success\n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        let credentials = self.list_credentials().await?;\n        \n        Ok(CredentialStatistics {\n            total_credentials: credentials.len() as u64,\n            active_credentials: credentials.len() as u64,\n            credential_versions: credentials.len() as u64,\n            rotation_statistics: RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                next_scheduled_rotation: None,\n            },\n            last_accessed: None,\n        })\n    }\n}\n\n/// Transit encryption provider for Vault\npub struct VaultTransitProvider {\n    client: Arc\u003cstd::sync::Mutex\u003cVaultClient\u003e\u003e,\n    key_name: String,\n}\n\nimpl VaultTransitProvider {\n    /// Create new Transit provider\n    pub fn new(client: Arc\u003cstd::sync::Mutex\u003cVaultClient\u003e\u003e, key_name: String) -\u003e Self {\n        Self { client, key_name }\n    }\n\n    /// Encrypt data using Vault Transit\n    pub async fn encrypt(\u0026self, plaintext: \u0026[u8]) -\u003e Result\u003cVec\u003cu8\u003e, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let payload = HashMap::from([\n            (\"plaintext\", base64::Engine::encode(\u0026base64::engine::general_purpose::STANDARD, plaintext))\n        ]);\n\n        let response = client.request(\n            reqwest::Method::POST,\n            \u0026format!(\"v1/transit/encrypt/{}\", self.key_name),\n            Some(serde_json::to_value(\u0026payload).unwrap())\n        ).await?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Encryption {\n                message: \"Failed to encrypt data\".to_string()\n            });\n        }\n\n        let response_data: serde_json::Value = response.json().await\n            .map_err(|_| CredentialError::Encryption {\n                message: \"Failed to parse encryption response\".to_string()\n            })?;\n\n        if let Some(ciphertext) = response_data.get(\"data\").and_then(|d| d.get(\"ciphertext\")) {\n            if let Some(ciphertext_str) = ciphertext.as_str() {\n                return Ok(base64::Engine::decode(\u0026base64::engine::general_purpose::STANDARD, ciphertext_str).unwrap());\n            }\n        }\n\n        Err(CredentialError::Encryption {\n            message: \"Invalid encryption response format\".to_string()\n        })\n    }\n\n    /// Decrypt data using Vault Transit\n    pub async fn decrypt(\u0026self, ciphertext: \u0026[u8]) -\u003e Result\u003cVec\u003cu8\u003e, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let ciphertext_b64 = base64::Engine::encode(\u0026base64::engine::general_purpose::STANDARD, ciphertext);\n        let payload = HashMap::from([\n            (\"ciphertext\", ciphertext_b64)\n        ]);\n\n        let response = client.request(\n            reqwest::Method::POST,\n            \u0026format!(\"v1/transit/decrypt/{}\", self.key_name),\n            Some(serde_json::to_value(\u0026payload).unwrap())\n        ).await?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Decryption {\n                message: \"Failed to decrypt data\".to_string()\n            });\n        }\n\n        let response_data: serde_json::Value = response.json().await\n            .map_err(|_| CredentialError::Decryption {\n                message: \"Failed to parse decryption response\".to_string()\n            })?;\n\n        if let Some(plaintext) = response_data.get(\"data\").and_then(|d| d.get(\"plaintext\")) {\n            if let Some(plaintext_str) = plaintext.as_str() {\n                return Ok(base64::Engine::decode(\u0026base64::engine::general_purpose::STANDARD, plaintext_str).unwrap());\n            }\n        }\n\n        Err(CredentialError::Decryption {\n            message: \"Invalid decryption response format\".to_string()\n        })\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","examples","advanced_usage.rs"],"content":"//! Advanced Usage Example\n//! \n//! This example demonstrates advanced Worker Manager features including:\n//! - Multiple providers (Kubernetes and Docker)\n//! - Complex credential management with rotation\n//! - Integration with NATS for event streaming\n//! - Keycloak service account integration\n//! - Health checks and monitoring\n\nuse worker_manager::*;\nuse std::collections::HashMap;\nuse tokio::time::{Duration, sleep};\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Initialize tracing\n    tracing_subscriber::fmt::init();\n\n    println!(\"Worker Manager Advanced Usage Example\");\n    println!(\"=====================================\");\n\n    // 1. Set up comprehensive credential providers\n    println!(\"\\n--- Setting Up Credential Providers ---\");\n    \n    // Simple in-memory provider for quick access\n    let simple_provider = Arc::new(credentials::SimpleCredentialProvider::new());\n    println!(\" Created SimpleCredentialProvider\");\n\n    // Set up sample credentials\n    setup_sample_credentials(\u0026simple_provider).await?;\n    println!(\" Configured sample credentials\");\n\n    // 2. Create Worker Manager with multiple providers\n    println!(\"\\n--- Creating Multi-Provider Worker Manager ---\");\n    \n    let mut worker_manager = WorkerManager::new(simple_provider.clone());\n\n    // Kubernetes Provider (if available)\n    match setup_kubernetes_provider(\u0026simple_provider).await {\n        Ok(provider) =\u003e {\n            worker_manager.register_provider(\"kubernetes\".to_string(), provider);\n            println!(\" Configured Kubernetes provider\");\n        }\n        Err(e) =\u003e {\n            println!(\" Kubernetes provider not available: {}\", e);\n        }\n    }\n\n    // Docker Provider\n    let docker_provider = Arc::new(providers::docker::DockerProvider::from_provider_config(\n        \u0026ProviderConfig::docker(),\n        simple_provider.clone()\n    )?);\n    worker_manager.register_provider(\"docker\".to_string(), docker_provider.clone());\n    worker_manager.set_default_provider(\"docker\".to_string());\n    println!(\" Configured Docker provider\");\n\n    // 3. Start rotation engine with advanced configuration\n    println!(\"\\n--- Starting Advanced Rotation Engine ---\");\n    \n    let mut rotation_config = credentials::rotation::RotationEngineConfig::default();\n    rotation_config.max_concurrent_rotations = 10;\n    rotation_config.rotation_timeout = Duration::from_secs(120);\n    rotation_config.enable_metrics = true;\n\n    worker_manager.start_rotation_engine(rotation_config).await?;\n    println!(\" Started advanced rotation engine\");\n\n    // 4. Set up rotation strategies for different credentials\n    println!(\"\\n--- Configuring Rotation Strategies ---\");\n    \n    // Time-based rotation for database credentials\n    let db_rotation_strategy = credentials::rotation::RotationStrategy::TimeBased(\n        credentials::rotation::TimeBasedRotation {\n            rotation_interval: Duration::from_secs(600), // 10 minutes\n            rotation_time: None,\n            grace_period: Duration::from_secs(30),\n        }\n    );\n\n    // Event-based rotation for API keys (triggered by security events)\n    let api_rotation_strategy = credentials::rotation::RotationStrategy::EventBased(\n        credentials::rotation::EventBasedRotation {\n            triggers: vec![\n                credentials::rotation::RotationTrigger::CredentialAccessed,\n                credentials::rotation::RotationTrigger::UnauthorizedAccessAttempt,\n            ],\n            max_concurrent_rotations: 3,\n        }\n    );\n\n    // Schedule rotations\n    let db_rotation_task = worker_manager.schedule_rotation(\"database-credentials\", db_rotation_strategy).await?;\n    let api_rotation_task = worker_manager.schedule_rotation(\"api-keys\", api_rotation_strategy).await?;\n    \n    println!(\" Scheduled rotation tasks:\");\n    println!(\"  Database credentials: {}\", db_rotation_task);\n    println!(\"  API keys: {}\", api_rotation_task);\n\n    // 5. Create workers with different providers\n    println!(\"\\n--- Creating Workers with Different Providers ---\");\n    \n    let mut workers = Vec::new();\n\n    // Worker 1: Database worker with secrets\n    let db_worker = create_database_worker().await?;\n    let db_provider_config = ProviderConfig::docker();\n    let db_worker_result = worker_manager.create_worker(\u0026db_worker, \u0026db_provider_config).await?;\n    workers.push(db_worker_result);\n    println!(\" Created database worker: {}\", workers[0].id);\n\n    // Worker 2: Web worker with API keys\n    let api_worker = create_api_worker().await?;\n    let api_provider_config = ProviderConfig::docker();\n    let api_worker_result = worker_manager.create_worker(\u0026api_worker, \u0026api_provider_config).await?;\n    workers.push(api_worker_result);\n    println!(\" Created API worker: {}\", workers[1].id);\n\n    // Worker 3: Kubernetes worker (if available)\n    if worker_manager.get_provider_by_name(\"kubernetes\").is_some() {\n        let k8s_worker = create_kubernetes_worker().await?;\n        let k8s_provider_config = ProviderConfig::kubernetes(\"default\".to_string());\n        let k8s_worker_result = worker_manager.create_worker(\u0026k8s_worker, \u0026k8s_provider_config).await?;\n        workers.push(k8s_worker_result);\n        println!(\" Created Kubernetes worker: {}\", workers[2].id);\n    }\n\n    // 6. Monitor worker lifecycle\n    println!(\"\\n--- Monitoring Worker Lifecycle ---\");\n    \n    for worker in \u0026workers {\n        println!(\"  Monitoring worker: {}\", worker.id);\n        \n        // Monitor for 30 seconds\n        for i in 1..=15 {\n            sleep(Duration::from_secs(2)).await;\n            \n            let status = worker_manager.get_worker_status(\u0026worker.id).await?;\n            println!(\"    Status {}: {:?}\", i, status);\n\n            // Test operations during monitoring\n            if i == 5 {\n                match worker_manager.get_provider().unwrap().get_logs(\u0026worker.id).await {\n                    Ok(logs) =\u003e {\n                        println!(\"     Got logs: {}\", logs.stream_id);\n                    }\n                    Err(e) =\u003e {\n                        println!(\"     Failed to get logs: {}\", e);\n                    }\n                }\n            }\n\n            if i == 10 {\n                match worker_manager.get_provider().unwrap().execute_command(\n                    \u0026worker.id, \n                    vec![\"echo\".to_string(), \"health-check\".to_string()],\n                    Some(Duration::from_secs(5))\n                ).await {\n                    Ok(result) =\u003e {\n                        println!(\"     Health check passed\");\n                    }\n                    Err(e) =\u003e {\n                        println!(\"     Health check failed: {}\", e);\n                    }\n                }\n            }\n\n            if matches!(status, WorkerState::Terminated) {\n                println!(\"     Worker completed\");\n                break;\n            }\n        }\n    }\n\n    // 7. Demonstrate event-driven rotations\n    println!(\"\\n--- Triggering Event-Based Rotations ---\");\n    \n    // Simulate security event\n    let security_event = credentials::rotation::RotationEvent {\n        event_type: credentials::rotation::RotationEventType::UnauthorizedAccessAttempt,\n        credential_name: Some(\"api-keys\".to_string()),\n        metadata: HashMap::from([\n            (\"source_ip\".to_string(), \"192.168.1.100\".to_string()),\n            (\"attempt_count\".to_string(), \"5\".to_string()),\n            (\"severity\".to_string(), \"high\".to_string()),\n        ]),\n        timestamp: chrono::Utc::now(),\n    };\n\n    // Trigger event through rotation engine\n    let rotation_engine = worker_manager.rotation_engine.read().await;\n    if let Some(engine) = rotation_engine.as_ref() {\n        engine.trigger_event_rotation(security_event).await?;\n        println!(\" Triggered event-based rotation\");\n    }\n\n    // 8. Test capacity management\n    println!(\"\\n--- Capacity Management ---\");\n    \n    let capacity = worker_manager.get_capacity().await?;\n    println!(\"  System capacity:\");\n    println!(\"    Active workers: {}\", capacity.active_workers);\n    println!(\"    CPU usage: {}m/{}m\", \n             capacity.used_resources.cpu_m, \n             capacity.total_resources.cpu_m);\n    println!(\"    Memory usage: {}MB/{}MB\", \n             capacity.used_resources.memory_mb, \n             capacity.total_resources.memory_mb);\n\n    // 9. Health checks and system status\n    println!(\"\\n--- Health Check and System Status ---\");\n    \n    let health_status = worker_manager.health_check().await?;\n    println!(\"  System health: {}\", if health_status.is_healthy { \"Healthy\" } else { \"Unhealthy\" });\n    println!(\"  Provider status:\");\n    for (name, healthy, error) in \u0026health_status.provider_details {\n        println!(\"    {}: {}\", name, if *healthy { \" Healthy\" } else { \" Failed\" });\n        if let Some(err) = error {\n            println!(\"      Error: {}\", err);\n        }\n    }\n\n    // 10. Rotation engine status\n    println!(\"\\n--- Rotation Engine Status ---\");\n    \n    let rotation_status = worker_manager.get_rotation_status().await?;\n    println!(\"  Rotation tasks:\");\n    println!(\"    Total: {}\", rotation_status.total_tasks);\n    println!(\"    Pending: {}\", rotation_status.pending_tasks);\n    println!(\"    Active: {}\", rotation_status.active_tasks);\n    println!(\"    Completed: {}\", rotation_status.completed_tasks);\n    println!(\"    Failed: {}\", rotation_status.failed_tasks);\n    println!(\"  Rotation statistics:\");\n    println!(\"    Total rotations: {}\", rotation_status.statistics.total_rotations);\n    println!(\"    Successful: {}\", rotation_status.statistics.successful_rotations);\n    println!(\"    Failed: {}\", rotation_status.statistics.failed_rotations);\n\n    // 11. Test manual rotation\n    println!(\"\\n--- Manual Rotation Test ---\");\n    \n    let manual_strategy = credentials::rotation::RotationStrategy::Manual;\n    let manual_task = worker_manager.schedule_rotation(\"database-credentials\", manual_strategy).await?;\n    println!(\" Scheduled manual rotation: {}\", manual_task);\n\n    // 12. Cleanup and shutdown\n    println!(\"\\n--- Cleanup and Shutdown ---\");\n    \n    // Wait a bit for rotations to complete\n    sleep(Duration::from_secs(5)).await;\n\n    // Terminate all workers\n    for worker in \u0026workers {\n        match worker_manager.terminate_worker(\u0026worker.id).await {\n            Ok(_) =\u003e println!(\" Terminated worker: {}\", worker.id),\n            Err(e) =\u003e println!(\" Failed to terminate worker {}: {}\", worker.id, e),\n        }\n    }\n\n    // Stop rotation engine\n    worker_manager.stop_rotation_engine().await?;\n    println!(\" Stopped rotation engine\");\n\n    println!(\"\\n--- Advanced Example Completed ---\");\n    println!(\" Demonstrated multi-provider support\");\n    println!(\" Showed advanced rotation strategies\");\n    println!(\" Tested event-driven rotations\");\n    println!(\" Validated system health monitoring\");\n    println!(\" Performed manual operations and cleanup\");\n\n    Ok(())\n}\n\nasync fn setup_sample_credentials(\n    provider: \u0026Arc\u003ccredentials::SimpleCredentialProvider\u003e\n) -\u003e Result\u003c(), CredentialError\u003e {\n    // Database credentials\n    let db_credential = credentials::Credential {\n        name: \"database-credentials\".to_string(),\n        values: HashMap::from([\n            (\"DB_HOST\".to_string(), \"prod-db.example.com\".to_string()),\n            (\"DB_PORT\".to_string(), \"5432\".to_string()),\n            (\"DB_USER\".to_string(), \"app_user\".to_string()),\n            (\"DB_PASSWORD\".to_string(), \"secure_password_123\".to_string()),\n            (\"DB_NAME\".to_string(), \"production_db\".to_string()),\n        ]),\n        metadata: HashMap::from([\n            (\"environment\".to_string(), \"production\".to_string()),\n            (\"team\".to_string(), \"backend\".to_string()),\n            (\"compliance_level\".to_string(), \"high\".to_string()),\n        ]),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(90)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"web-service\".to_string(), \"api-service\".to_string()],\n            read_permissions: vec![\"read\".to_string()],\n            write_permissions: vec![],\n            rotation_permissions: vec![\"rotate\".to_string(), \"manual-rotate\".to_string()],\n        }),\n    };\n\n    provider.put_credential(\u0026db_credential).await?;\n\n    // API keys\n    let api_credential = credentials::Credential {\n        name: \"api-keys\".to_string(),\n        values: HashMap::from([\n            (\"GOOGLE_API_KEY\".to_string(), \"AIzaSyD-9tSrke72PouQMnMX-a7u8Lm5Oz9kSu4\".to_string()),\n            (\"STRIPE_PUBLISHABLE_KEY\".to_string(), \"pk_test_51H...xyz\".to_string()),\n            (\"STRIPE_SECRET_KEY\".to_string(), \"sk_test_51H...abc\".to_string()),\n            (\"TWITTER_BEARER_TOKEN\".to_string(), \"Bearer_token_here\".to_string()),\n        ]),\n        metadata: HashMap::from([\n            (\"environment\".to_string(), \"production\".to_string()),\n            (\"team\".to_string(), \"frontend\".to_string()),\n            (\"sensitivity\".to_string(), \"high\".to_string()),\n        ]),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(60)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"frontend-service\".to_string(), \"mobile-app\".to_string()],\n            read_permissions: vec![\"read\".to_string()],\n            write_permissions: vec![],\n            rotation_permissions: vec![\"rotate\".to_string()],\n        }),\n    };\n\n    provider.put_credential(\u0026api_credential).await?;\n\n    // Service account credentials\n    let service_credential = credentials::Credential {\n        name: \"service-accounts\".to_string(),\n        values: HashMap::from([\n            (\"KAFKA_BROKER_USER\".to_string(), \"kafka-service\".to_string()),\n            (\"KAFKA_BROKER_PASSWORD\".to_string(), \"kafka_password_456\".to_string()),\n            (\"REDIS_USER\".to_string(), \"redis-admin\".to_string()),\n            (\"REDIS_PASSWORD\".to_string(), \"redis_secret_789\".to_string()),\n        ]),\n        metadata: HashMap::from([\n            (\"environment\".to_string(), \"production\".to_string()),\n            (\"team\".to_string(), \"infrastructure\".to_string()),\n            (\"sensitivity\".to_string(), \"critical\".to_string()),\n        ]),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(120)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"worker-service\".to_string(), \"scheduler\".to_string()],\n            read_permissions: vec![\"read\".to_string(), \"write\".to_string()],\n            write_permissions: vec![\"write\".to_string()],\n            rotation_permissions: vec![\"rotate\".to_string()],\n        }),\n    };\n\n    provider.put_credential(\u0026service_credential).await?;\n\n    Ok(())\n}\n\nasync fn setup_kubernetes_provider(\n    credential_provider: \u0026Arc\u003ccredentials::SimpleCredentialProvider\u003e\n) -\u003e Result\u003cArc\u003cproviders::kubernetes::KubernetesProvider\u003e, ProviderError\u003e {\n    // This would require actual Kubernetes cluster access in production\n    // For demo purposes, we'll try to create it but expect it to fail\n    let k8s_config = providers::kubernetes::KubernetesConfig {\n        namespace: \"default\".to_string(),\n        service_account: None,\n        node_selector: None,\n        tolerations: None,\n        node_affinity: None,\n        pod_disruption_budget: None,\n        termination_grace_period_seconds: Some(30),\n        image_pull_secrets: None,\n        priority_class: None,\n        security_context: None,\n    };\n\n    providers::kubernetes::KubernetesProvider::new(k8s_config, credential_provider.clone()).await\n        .map(|p| Arc::new(p))\n}\n\nasync fn create_database_worker() -\u003e Result\u003cRuntimeSpec, ProviderError\u003e {\n    let mut spec = RuntimeSpec::basic(\"postgres:15\".to_string());\n    spec.command = Some(vec![\n        \"postgres\".to_string()\n    ]);\n    spec.secret_refs = vec![\"database-credentials\".to_string()];\n    spec.ports = vec![5432];\n    spec.resources = ResourceQuota::basic(1000, 1024);\n    spec.labels.insert(\"type\".to_string(), \"database\".to_string());\n    spec.labels.insert(\"environment\".to_string(), \"production\".to_string());\n    \n    // Add environment variables\n    spec.env.insert(\"POSTGRES_DB\".to_string(), \"production_db\".to_string());\n    spec.env.insert(\"PGDATA\".to_string(), \"/var/lib/postgresql/data/pgdata\".to_string());\n\n    Ok(spec)\n}\n\nasync fn create_api_worker() -\u003e Result\u003cRuntimeSpec, ProviderError\u003e {\n    let mut spec = RuntimeSpec::basic(\"nginx:alpine\".to_string());\n    spec.command = Some(vec![\n        \"nginx\".to_string(), \"-g\".to_string(), \"daemon off;\".to_string()\n    ]);\n    spec.secret_refs = vec![\"api-keys\".to_string()];\n    spec.ports = vec![80, 443];\n    spec.resources = ResourceQuota::basic(500, 512);\n    spec.labels.insert(\"type\".to_string(), \"api\".to_string());\n    spec.labels.insert(\"environment\".to_string(), \"production\".to_string());\n    \n    // Add API-specific configuration\n    spec.env.insert(\"API_TIMEOUT\".to_string(), \"30\".to_string());\n    spec.env.insert(\"RATE_LIMIT\".to_string(), \"100\".to_string());\n\n    Ok(spec)\n}\n\nasync fn create_kubernetes_worker() -\u003e Result\u003cRuntimeSpec, ProviderError\u003e {\n    let mut spec = RuntimeSpec::basic(\"busybox:1.36\".to_string());\n    spec.command = Some(vec![\n        \"sh\".to_string(), \"-c\".to_string(), \n        \"echo 'Kubernetes worker started' \u0026\u0026 sleep 60\".to_string()\n    ]);\n    spec.secret_refs = vec![\"service-accounts\".to_string()];\n    spec.resources = ResourceQuota::basic(250, 256);\n    spec.labels.insert(\"type\".to_string(), \"k8s-demo\".to_string());\n    spec.labels.insert(\"environment\".to_string(), \"production\".to_string());\n\n    Ok(spec)\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","examples","basic_usage.rs"],"content":"//! Basic Usage Example\n//! \n//! This example demonstrates how to use the Worker Manager with basic configurations.\n//! Shows the complete lifecycle: creation, monitoring, and termination of workers.\n\nuse worker_manager::*;\nuse std::collections::HashMap;\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Initialize logging\n    tracing_subscriber::fmt::init();\n\n    println!(\"Worker Manager Basic Usage Example\");\n    println!(\"==================================\");\n\n    // 1. Create a simple credential provider (in-memory for demo)\n    let credential_provider = Arc::new(credentials::SimpleCredentialProvider::new());\n    println!(\" Created simple credential provider\");\n\n    // 2. Create Worker Manager\n    let mut worker_manager = WorkerManager::new(credential_provider.clone());\n    println!(\" Created Worker Manager\");\n\n    // 3. Set up Docker Provider\n    let docker_provider = Arc::new(providers::docker::DockerProvider::from_provider_config(\n        \u0026ProviderConfig::docker(),\n        credential_provider.clone()\n    )?);\n    worker_manager.register_provider(\"docker\".to_string(), docker_provider.clone());\n    worker_manager.set_default_provider(\"docker\".to_string());\n    println!(\" Configured Docker provider\");\n\n    // 4. Create sample credential\n    let mut sample_credential = credentials::Credential {\n        name: \"database-credentials\".to_string(),\n        values: HashMap::from([\n            (\"DB_HOST\".to_string(), \"localhost\".to_string()),\n            (\"DB_PORT\".to_string(), \"5432\".to_string()),\n            (\"DB_USER\".to_string(), \"admin\".to_string()),\n            (\"DB_PASSWORD\".to_string(), \"secret123\".to_string()),\n        ]),\n        metadata: HashMap::new(),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(30)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"worker-service\".to_string()],\n            read_permissions: vec![\"read\".to_string()],\n            write_permissions: vec![],\n            rotation_permissions: vec![\"rotate\".to_string()],\n        }),\n    };\n\n    // Store the credential\n    let stored_credential = credential_provider.put_credential(\u0026sample_credential).await?;\n    println!(\" Created sample credential: {}\", stored_credential.credential.name);\n\n    // 5. Create a RuntimeSpec for a worker\n    let mut runtime_spec = RuntimeSpec::basic(\"alpine:3.18\".to_string());\n    runtime_spec.command = Some(vec![\"/bin/sh\".to_string(), \"-lc\".to_string(), \n                                  \"echo 'Hello from Worker Manager!' \u0026\u0026 sleep 10\".to_string()]);\n    runtime_spec.secret_refs = vec![\"database-credentials\".to_string()];\n    runtime_spec.ports = vec![8080];\n    runtime_spec.resources = ResourceQuota::basic(500, 256);\n    runtime_spec.labels.insert(\"environment\".to_string(), \"development\".to_string());\n    runtime_spec.labels.insert(\"service\".to_string(), \"worker-demo\".to_string());\n\n    println!(\" Created RuntimeSpec for worker\");\n\n    // 6. Create ProviderConfig\n    let provider_config = ProviderConfig::docker();\n    println!(\" Created ProviderConfig\");\n\n    // 7. Create worker\n    println!(\"\\n--- Creating Worker ---\");\n    let worker = worker_manager.create_worker(\u0026runtime_spec, \u0026provider_config).await?;\n    println!(\" Worker created successfully\");\n    println!(\"  Worker ID: {}\", worker.id);\n    println!(\"  State: {:?}\", worker.state);\n    println!(\"  Provider: {}\", worker.provider_config.provider_name);\n\n    // 8. Monitor worker status\n    println!(\"\\n--- Monitoring Worker Status ---\");\n    for i in 1..=5 {\n        tokio::time::sleep(std::time::Duration::from_secs(2)).await;\n        \n        let status = worker_manager.get_worker_status(\u0026worker.id).await?;\n        println!(\"  Status check {}: {:?}\", i, status);\n\n        if matches!(status, WorkerState::Terminated) {\n            println!(\"   Worker completed successfully\");\n            break;\n        }\n    }\n\n    // 9. Test capacity information\n    println!(\"\\n--- System Capacity ---\");\n    let capacity = worker_manager.get_capacity().await?;\n    println!(\"  Active workers: {}\", capacity.active_workers);\n    println!(\"  Total CPU: {}m\", capacity.total_resources.cpu_m);\n    println!(\"  Total Memory: {}MB\", capacity.total_resources.memory_mb);\n    println!(\"  Available CPU: {}m\", capacity.available_resources.cpu_m);\n    println!(\"  Available Memory: {}MB\", capacity.available_resources.memory_mb);\n\n    // 10. Demonstrate rotation setup\n    println!(\"\\n--- Setting Up Credential Rotation ---\");\n    \n    // Start rotation engine\n    let rotation_config = credentials::rotation::RotationEngineConfig::default();\n    worker_manager.start_rotation_engine(rotation_config).await?;\n    println!(\" Started rotation engine\");\n\n    // Schedule time-based rotation\n    let rotation_strategy = credentials::rotation::RotationStrategy::TimeBased(\n        credentials::rotation::TimeBasedRotation {\n            rotation_interval: std::time::Duration::from_secs(300), // 5 minutes for demo\n            rotation_time: None,\n            grace_period: std::time::Duration::from_secs(60),\n        }\n    );\n\n    let rotation_task_id = worker_manager.schedule_rotation(\"database-credentials\", rotation_strategy).await?;\n    println!(\" Scheduled rotation task: {}\", rotation_task_id);\n\n    // 11. Get rotation status\n    tokio::time::sleep(std::time::Duration::from_secs(1)).await;\n    let rotation_status = worker_manager.get_rotation_status().await?;\n    println!(\"  Total tasks: {}\", rotation_status.total_tasks);\n    println!(\"  Active tasks: {}\", rotation_status.active_tasks);\n    println!(\"  Healthy providers: {}/{}\", rotation_status.health_status.running_tasks, \n             rotation_status.health_status.is_healthy);\n\n    // 12. Demonstrate provider switching\n    println!(\"\\n--- Provider Switching ---\");\n    let providers = worker_manager.list_providers();\n    println!(\"  Available providers: {:?}\", providers);\n\n    // 13. Health check\n    println!(\"\\n--- System Health Check ---\");\n    let health_status = worker_manager.health_check().await?;\n    println!(\"  Overall health: {}\", if health_status.is_healthy { \"Healthy\" } else { \"Unhealthy\" });\n    println!(\"  Healthy providers: {}/{}\", health_status.healthy_providers, health_status.total_providers);\n    println!(\"  Rotation engine: {}\", if health_status.rotation_engine_healthy { \"Running\" } else { \"Not running\" });\n\n    // 14. Clean up\n    println!(\"\\n--- Cleanup ---\");\n    worker_manager.stop_rotation_engine().await?;\n    println!(\" Stopped rotation engine\");\n    println!(\" Example completed successfully\");\n\n    println!(\"\\n--- Summary ---\");\n    println!(\" Created and managed worker lifecycle\");\n    println!(\" Configured credential storage and rotation\");\n    println!(\" Monitored system capacity and health\");\n    println!(\" Tested provider abstraction and switching\");\n    println!(\" Demonstrated production-ready patterns\");\n\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","lib.rs"],"content":"//! Worker Manager Abstraction Layer\n//! \n//! A comprehensive Rust-based worker management system providing abstractions\n//! for multiple infrastructure providers and credential management with automatic rotation.\n\n#![warn(missing_docs)]\n\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n// Re-export main traits and types\npub use crate::traits::*;\npub use crate::providers::{WorkerManagerProvider, KubernetesProvider, DockerProvider};\npub use crate::credentials::{CredentialProvider, RotationEngine, RotationEvent, RotationStrategy};\n\n// Provider modules\npub mod traits {\n    pub use worker_manager_traits::*;\n}\n\npub mod providers {\n    pub mod kubernetes {\n        pub use worker_manager_providers_kubernetes::*;\n    }\n    pub mod docker {\n        pub use worker_manager_providers_docker::*;\n    }\n    pub mod plugin {\n        pub mod registry {\n            pub use worker_manager_providers_plugin_registry::*;\n        }\n    }\n}\n\n// Credential management modules\npub mod credentials {\n    pub mod mod_rs {\n        pub use worker_manager_credentials_mod::*;\n    }\n    pub mod vault {\n        pub use worker_manager_credentials_vault::*;\n    }\n    pub mod aws_secrets {\n        pub use worker_manager_credentials_aws_secrets::*;\n    }\n    pub mod keycloak {\n        pub use worker_manager_credentials_keycloak::*;\n    }\n    pub mod rotation {\n        pub use worker_manager_credentials_rotation::*;\n    }\n}\n\n// Ephemeral worker management\npub mod ephemeral {\n    pub mod auto_scaling {\n        pub use worker_manager_ephemeral_auto_scaling::*;\n    }\n    pub mod health_checks {\n        pub use worker_manager_ephemeral_health_checks::*;\n    }\n    pub mod cost_optimization {\n        pub use worker_manager_ephemeral_cost_optimization::*;\n    }\n}\n\n// Security and networking\npub mod security {\n    pub mod rbac {\n        pub use worker_manager_security_rbac::*;\n    }\n    pub mod network_policies {\n        pub use worker_manager_security_network_policies::*;\n    }\n}\n\n/// Main Worker Manager\n#[derive(Debug)]\npub struct WorkerManager {\n    providers: HashMap\u003cString, Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e,\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    rotation_engine: Arc\u003cRwLock\u003cOption\u003crotation::RotationEngine\u003e\u003e\u003e,\n    current_provider: String,\n}\n\nimpl WorkerManager {\n    /// Create new Worker Manager\n    pub fn new(\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Self {\n        Self {\n            providers: HashMap::new(),\n            credential_provider,\n            rotation_engine: Arc::new(RwLock::new(None)),\n            current_provider: \"kubernetes\".to_string(),\n        }\n    }\n\n    /// Register a provider\n    pub fn register_provider(\n        \u0026mut self,\n        name: String,\n        provider: Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e,\n    ) {\n        self.providers.insert(name, provider);\n    }\n\n    /// Set default provider\n    pub fn set_default_provider(\u0026mut self, provider_name: String) {\n        if self.providers.contains_key(\u0026provider_name) {\n            self.current_provider = provider_name;\n        }\n    }\n\n    /// Get current provider\n    pub fn get_provider(\u0026self) -\u003e Option\u003cArc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e\u003e {\n        self.providers.get(\u0026self.current_provider).cloned()\n    }\n\n    /// Get specific provider\n    pub fn get_provider_by_name(\n        \u0026self,\n        name: \u0026str,\n    ) -\u003e Option\u003cArc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e {\n        self.providers.get(name).cloned()\n    }\n\n    /// Start rotation engine\n    pub async fn start_rotation_engine(\n        \u0026self,\n        config: credentials::rotation::RotationEngineConfig,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        let mut rotation_engine = self.rotation_engine.write().await;\n        \n        let engine = credentials::rotation::RotationEngine::new(\n            self.credential_provider.clone(),\n            config,\n        );\n        \n        // Start the rotation engine\n        let engine_clone = engine.clone();\n        tokio::spawn(async move {\n            if let Err(e) = engine_clone.start().await {\n                eprintln!(\"Rotation engine failed to start: {:?}\", e);\n            }\n        });\n        \n        *rotation_engine = Some(engine);\n        Ok(())\n    }\n\n    /// Stop rotation engine\n    pub async fn stop_rotation_engine(\u0026self) -\u003e Result\u003c(), ProviderError\u003e {\n        let rotation_engine = self.rotation_engine.read().await;\n        if let Some(engine) = rotation_engine.as_ref() {\n            engine.stop().await.map_err(|e| ProviderError::internal(\n                format!(\"Failed to stop rotation engine: {}\", e),\n                Some(\"rotation_engine\".to_string()),\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: Some(\"rotation_engine\".to_string()),\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        }\n        Ok(())\n    }\n\n    /// Create worker with default provider\n    pub async fn create_worker(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        config: \u0026ProviderConfig,\n    ) -\u003e Result\u003cWorker, ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.create_worker(spec, config).await\n    }\n\n    /// Terminate worker\n    pub async fn terminate_worker(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.terminate_worker(worker_id).await\n    }\n\n    /// Get worker status\n    pub async fn get_worker_status(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003cWorkerState, ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.get_worker_status(worker_id).await\n    }\n\n    /// Get system capacity\n    pub async fn get_capacity(\u0026self) -\u003e Result\u003cCapacityInfo, ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.get_capacity().await\n    }\n\n    /// Schedule rotation\n    pub async fn schedule_rotation(\n        \u0026self,\n        credential_name: \u0026str,\n        strategy: credentials::rotation::RotationStrategy,\n    ) -\u003e Result\u003cuuid::Uuid, ProviderError\u003e {\n        let rotation_engine = self.rotation_engine.read().await;\n        if let Some(engine) = rotation_engine.as_ref() {\n            engine.schedule_rotation(credential_name, strategy)\n                .await\n                .map_err(|e| ProviderError::credentials(\n                    format!(\"Failed to schedule rotation: {}\", e),\n                    \"rotation_engine\".to_string()\n                ))\n        } else {\n            Err(ProviderError::configuration(\n                \"Rotation engine not started\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: Some(\"rotation_engine\".to_string()),\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))\n        }\n    }\n\n    /// Get rotation status\n    pub async fn get_rotation_status(\u0026self) -\u003e Result\u003ccredentials::rotation::RotationEngineStatus, ProviderError\u003e {\n        let rotation_engine = self.rotation_engine.read().await;\n        if let Some(engine) = rotation_engine.as_ref() {\n            Ok(engine.get_status().await)\n        } else {\n            Err(ProviderError::configuration(\n                \"Rotation engine not started\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: Some(\"rotation_engine\".to_string()),\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))\n        }\n    }\n\n    /// Get available providers\n    pub fn list_providers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.providers.keys().cloned().collect()\n    }\n\n    /// Health check\n    pub async fn health_check(\u0026self) -\u003e Result\u003cWorkerManagerHealthStatus, ProviderError\u003e {\n        let providers_status = futures::future::join_all(\n            self.providers.values().map(|provider| {\n                let name = self.providers.iter()\n                    .find_map(|(k, v)| if v.as_ref() == provider.as_ref() { Some(k.clone()) } else { None })\n                    .unwrap_or_else(|| \"unknown\".to_string());\n                \n                async move {\n                    match provider.get_capacity().await {\n                        Ok(_) =\u003e (name, true, None),\n                        Err(e) =\u003e (name, false, Some(e.to_string())),\n                    }\n                }\n            })\n        ).await;\n\n        let healthy_providers = providers_status.iter().filter(|(_, healthy, _)| *healthy).count();\n        let is_healthy = healthy_providers \u003e 0;\n\n        Ok(WorkerManagerHealthStatus {\n            is_healthy,\n            healthy_providers,\n            total_providers: self.providers.len(),\n            provider_details: providers_status.into_iter().map(|(name, healthy, error)| {\n                (name, healthy, error)\n            }).collect(),\n            rotation_engine_healthy: self.rotation_engine.read().await.is_some(),\n            timestamp: chrono::Utc::now(),\n        })\n    }\n}\n\n/// Worker Manager health status\n#[derive(Debug, Clone)]\npub struct WorkerManagerHealthStatus {\n    pub is_healthy: bool,\n    pub healthy_providers: usize,\n    pub total_providers: usize,\n    pub provider_details: Vec\u003c(String, bool, Option\u003cString\u003e)\u003e,\n    pub rotation_engine_healthy: bool,\n    pub timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Worker Manager Builder for easier setup\n#[derive(Debug)]\npub struct WorkerManagerBuilder {\n    credential_provider: Option\u003cArc\u003cdyn CredentialProvider + Send + Sync\u003e\u003e,\n    providers: HashMap\u003cString, Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e,\n    default_provider: Option\u003cString\u003e,\n}\n\nimpl WorkerManagerBuilder {\n    /// Create new builder\n    pub fn new() -\u003e Self {\n        Self {\n            credential_provider: None,\n            providers: HashMap::new(),\n            default_provider: None,\n        }\n    }\n\n    /// Set credential provider\n    pub fn credential_provider(mut self, provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e) -\u003e Self {\n        self.credential_provider = Some(provider);\n        self\n    }\n\n    /// Add provider\n    pub fn add_provider(\n        mut self,\n        name: String,\n        provider: Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e,\n    ) -\u003e Self {\n        self.providers.insert(name, provider);\n        if self.default_provider.is_none() {\n            self.default_provider = Some(name);\n        }\n        self\n    }\n\n    /// Set default provider\n    pub fn default_provider(mut self, name: String) -\u003e Self {\n        self.default_provider = Some(name);\n        self\n    }\n\n    /// Build Worker Manager\n    pub fn build(self) -\u003e Result\u003cWorkerManager, ProviderError\u003e {\n        let credential_provider = self.credential_provider\n            .ok_or_else(|| ProviderError::configuration(\n                \"Credential provider is required\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n\n        let mut manager = WorkerManager::new(credential_provider);\n\n        for (name, provider) in self.providers {\n            manager.register_provider(name, provider);\n        }\n\n        if let Some(default) = self.default_provider {\n            manager.set_default_provider(default);\n        }\n\n        Ok(manager)\n    }\n}\n\nimpl Default for WorkerManagerBuilder {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Helper functions for creating common configurations\npub mod builders {\n    use super::*;\n\n    /// Create a simple in-memory provider setup for testing\n    pub async fn create_simple_setup() -\u003e Result\u003c(WorkerManager, Vec\u003cArc\u003cdyn CredentialProvider + Send + Sync\u003e\u003e), ProviderError\u003e {\n        use crate::providers::docker::DockerProvider;\n        use crate::providers::kubernetes::KubernetesProvider;\n        use crate::credentials::SimpleCredentialProvider;\n\n        let simple_credential = Arc::new(SimpleCredentialProvider::new());\n        \n        let mut k8s_config = crate::providers::kubernetes::KubernetesConfig::default();\n        k8s_config.namespace = \"default\".to_string();\n        \n        let k8s_provider = Arc::new(KubernetesProvider::from_provider_config(\n            \u0026ProviderConfig::kubernetes(\"default\".to_string()),\n            simple_credential.clone()\n        ).await?);\n\n        let docker_provider = Arc::new(DockerProvider::from_provider_config(\n            \u0026ProviderConfig::docker(),\n            simple_credential.clone()\n        ).await?);\n\n        let mut builder = WorkerManagerBuilder::new()\n            .credential_provider(simple_credential.clone())\n            .add_provider(\"kubernetes\".to_string(), k8s_provider)\n            .add_provider(\"docker\".to_string(), docker_provider)\n            .default_provider(\"docker\".to_string());\n\n        let manager = builder.build()?;\n        Ok((manager, vec![simple_credential]))\n    }\n\n    /// Create a production setup with multiple credential providers\n    pub async fn create_production_setup() -\u003e Result\u003c(WorkerManager, Vec\u003cArc\u003cdyn CredentialProvider + Send + Sync\u003e\u003e), ProviderError\u003e {\n        use crate::credentials::vault::HashiCorpVaultProvider;\n        use crate::credentials::aws_secrets::AWSSecretsManagerProvider;\n        use crate::credentials::keycloak::KeycloakServiceAccountProvider;\n\n        // This would require proper configuration in production\n        // For now, return a simple setup\n        create_simple_setup().await\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","providers","docker","mod.rs"],"content":"//! Docker Provider Implementation\n//! \n//! This module provides a complete implementation of the WorkerManagerProvider trait\n//! using Docker as the underlying infrastructure provider.\n\nuse crate::traits::*;\nuse crate::credentials::CredentialProvider;\nuse async_trait::async_trait;\nuse chrono::Utc;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\nuse uuid::Uuid;\n\n/// Docker-specific configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DockerConfig {\n    pub host: Option\u003cString\u003e,\n    pub network_mode: String,\n    pub registry_auth: Option\u003cString\u003e,\n    pub cpu_shares: Option\u003cu64\u003e,\n    pub memory_limit: Option\u003cu64\u003e,\n    pub ulimits: Option\u003cVec\u003cString\u003e\u003e,\n    pub log_driver: Option\u003cString\u003e,\n    pub log_options: Option\u003cHashMap\u003cString, String\u003e\u003e,\n}\n\nimpl Default for DockerConfig {\n    fn default() -\u003e Self {\n        Self {\n            host: None,\n            network_mode: \"bridge\".to_string(),\n            registry_auth: None,\n            cpu_shares: Some(1024),\n            memory_limit: Some(1024 * 1024 * 1024), // 1GB\n            ulimits: None,\n            log_driver: Some(\"json-file\".to_string()),\n            log_options: Some(HashMap::from([\n                (\"max-size\".to_string(), \"10m\".to_string()),\n                (\"max-file\".to_string(), \"3\".to_string()),\n            ])),\n        }\n    }\n}\n\n/// Container tracking information\n#[derive(Debug, Clone)]\nstruct DockerWorker {\n    id: WorkerId,\n    container_id: String,\n    network_name: Option\u003cString\u003e,\n    volume_mounts: Vec\u003cString\u003e,\n    created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    last_health_check: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Docker Provider Implementation\npub struct DockerProvider {\n    client: docker::Docker,\n    config: DockerConfig,\n    workers: Arc\u003cRwLock\u003cHashMap\u003cWorkerId, DockerWorker\u003e\u003e\u003e,\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n}\n\nimpl DockerProvider {\n    /// Create a new Docker Provider\n    pub async fn new(\n        config: DockerConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let client = docker::Docker::connect_with_unix_socket(\u0026config.host.unwrap_or_else(|| \"/var/run/docker.sock\".to_string()), 115, docker::Docker::default_timeout())\n            .map_err(|e| ProviderError::infrastructure(\n                format!(\"Failed to create Docker client: {}\", e),\n                \"docker\".to_string()\n            ))?;\n\n        Ok(Self {\n            client,\n            config,\n            workers: Arc::new(RwLock::new(HashMap::new())),\n            credential_provider,\n        })\n    }\n\n    /// Get Docker configuration from ProviderConfig\n    pub fn from_provider_config(\n        provider_config: \u0026ProviderConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let mut config = DockerConfig::default();\n        \n        if let serde_json::Value::Object(obj) = \u0026provider_config.specific_config {\n            if let Some(host) = obj.get(\"host\").and_then(|v| v.as_str()) {\n                config.host = Some(host.to_string());\n            }\n            if let Some(network_mode) = obj.get(\"network_mode\").and_then(|v| v.as_str()) {\n                config.network_mode = network_mode.to_string();\n            }\n            if let Some(registry_auth) = obj.get(\"registry_auth\").and_then(|v| v.as_str()) {\n                config.registry_auth = Some(registry_auth.to_string());\n            }\n        }\n\n        Self::new(config, credential_provider)\n    }\n\n    /// Create Docker network for isolation\n    async fn create_network(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cString, ProviderError\u003e {\n        let network_name = format!(\"worker-network-{}\", worker_id);\n        \n        let network_config = docker::models::NetworkCreateRequest::builder()\n            .name(\u0026network_name)\n            .driver(\"bridge\")\n            .internal(false)\n            .ipam(docker::models::IPAMConfig::builder().build())\n            .options({\n                let mut options = HashMap::new();\n                options.insert(\"com.docker.network.bridge.enable_icc\".to_string(), \"true\".to_string());\n                options.insert(\"com.docker.network.bridge.enable_ip_masquerade\".to_string(), \"true\".to_string());\n                options\n            })\n            .build();\n\n        match self.client.create_network(network_config).await {\n            Ok(_) =\u003e Ok(network_name),\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to create Docker network: {}\", e),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    /// Get container logs\n    async fn get_container_logs(\u0026self, container_id: \u0026str) -\u003e Result\u003cString, ProviderError\u003e {\n        let mut logs = String::new();\n        \n        match self.client.logs(container_id, \u0026docker::Docker::default_timeout()).await {\n            Ok(log_stream) =\u003e {\n                for result in log_stream {\n                    match result {\n                        Ok(log) =\u003e {\n                            match log {\n                                docker::LogOutput::StdOut { message } =\u003e {\n                                    logs.push_str(\u0026String::from_utf8_lossy(\u0026message));\n                                }\n                                docker::LogOutput::StdErr { message } =\u003e {\n                                    logs.push_str(\u0026format!(\"STDERR: {}\", String::from_utf8_lossy(\u0026message)));\n                                }\n                                docker::LogOutput::StdIn { message } =\u003e {\n                                    logs.push_str(\u0026format!(\"STDIN: {}\", String::from_utf8_lossy(\u0026message)));\n                                }\n                                docker::LogOutput::Unknown { data } =\u003e {\n                                    logs.push_str(\u0026format!(\"UNKNOWN: {}\", String::from_utf8_lossy(\u0026data)));\n                                }\n                            }\n                        }\n                        Err(e) =\u003e return Err(ProviderError::infrastructure(\n                            format!(\"Failed to read log: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n                Ok(logs)\n            }\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to get container logs: {}\", e),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    /// Create volume mount configuration\n    async fn create_volume_mounts(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n    ) -\u003e Result\u003cVec\u003cdocker::models::Mount\u003e, ProviderError\u003e {\n        let mut mounts = Vec::new();\n\n        for volume_mount in \u0026spec.volumes {\n            let mount = match \u0026volume_mount.source {\n                VolumeSource::EmptyDir =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::TMPFS)\n                        .source(None)\n                        .target(\u0026volume_mount.mount_path)\n                        .tmpfs_options(Some(docker::models::TmpfsOptions {\n                            size_bytes: None,\n                            mode: Some(0o755),\n                        }))\n                        .build()\n                }\n                VolumeSource::HostPath(path) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::BIND)\n                        .source(Some(path.clone()))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n                VolumeSource::Secret(secret_name) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::VOLUME)\n                        .source(Some(format!(\"secret-{}\", secret_name)))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n                VolumeSource::ConfigMap(config_name) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::VOLUME)\n                        .source(Some(format!(\"config-{}\", config_name)))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n                VolumeSource::PersistentVolume(pv_name) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::VOLUME)\n                        .source(Some(pv_name.clone()))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n            };\n            mounts.push(mount);\n        }\n\n        Ok(mounts)\n    }\n\n    /// Create health check configuration\n    async fn create_health_check(\u0026self, spec: \u0026RuntimeSpec) -\u003e Option\u003cdocker::models::HealthConfig\u003e {\n        Some(docker::models::HealthConfig {\n            test: Some(vec![\"CMD\".to_string(), \"curl\".to_string(), \"-f\".to_string(), \"http://localhost:8080/health\".to_string()]),\n            interval: Some(30_000_000_000), // 30 seconds in nanoseconds\n            timeout: Some(5_000_000_000),  // 5 seconds\n            retries: Some(3),\n            start_period: Some(0),\n        })\n    }\n}\n\n#[async_trait]\nimpl WorkerManagerProvider for DockerProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"docker\"\n    }\n\n    async fn create_worker(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        config: \u0026ProviderConfig,\n    ) -\u003e Result\u003cWorker, ProviderError\u003e {\n        let worker_id = WorkerId::new();\n        \n        // Create network for isolation\n        let network_name = self.create_network(\u0026worker_id).await?;\n        \n        // Create volume mounts\n        let mounts = self.create_volume_mounts(spec).await?;\n\n        // Prepare container creation options\n        let mut create_options = docker::models::CreateContainerOptions {\n            name: Some(format!(\"worker-{}\", worker_id)),\n            ..Default::default()\n        };\n\n        // Prepare host configuration\n        let mut host_config = docker::models::HostConfig::builder()\n            .network_mode(Some(network_name.clone()))\n            .memory(self.config.memory_limit)\n            .cpu_shares(self.config.cpu_shares)\n            .restart_policy(Some(docker::models::RestartPolicy {\n                name: Some(docker::models::RestartPolicyNameEnum::ALWAYS),\n                maximum_retry_count: None,\n            }))\n            .build();\n\n        // Add volume mounts\n        host_config.mounts = Some(mounts);\n\n        // Add log configuration\n        if let Some(log_driver) = \u0026self.config.log_driver {\n            let mut log_config = docker::models::LogConfig {\n                type_: Some(log_driver.clone()),\n                config: self.config.log_options.clone(),\n            };\n            host_config.log_config = Some(log_config);\n        }\n\n        // Add environment variables\n        let mut env_vars = Vec::new();\n        for (key, value) in \u0026spec.env {\n            env_vars.push(format!(\"{}={}\", key, value));\n        }\n\n        // Add secrets as environment variables if not using volume mounts\n        for secret_ref in \u0026spec.secret_refs {\n            match self.credential_provider.get_secret(secret_ref, None).await {\n                Ok(secret) =\u003e {\n                    for (key, value) in \u0026secret.values {\n                        env_vars.push(format!(\"{}_{}={}\", secret_ref.to_uppercase(), key, value));\n                    }\n                }\n                Err(e) =\u003e {\n                    return Err(ProviderError::credentials(\n                        format!(\"Failed to get secret {}: {}\", secret_ref, e),\n                        \"docker\".to_string()\n                    ));\n                }\n            }\n        }\n\n        // Create container\n        let container_config = docker::models::ContainerCreateBody {\n            image: spec.image.clone(),\n            cmd: spec.command.clone(),\n            env: Some(env_vars),\n            host_config: Some(host_config),\n            healthcheck: self.create_health_check(spec).await,\n            ..Default::default()\n        };\n\n        match self.client.create_container(\u0026create_options, \u0026container_config).await {\n            Ok(container_info) =\u003e {\n                // Start the container\n                match self.client.start_container(\u0026container_info.id).await {\n                    Ok(_) =\u003e {\n                        let worker = Worker {\n                            id: worker_id.clone(),\n                            state: WorkerState::Running,\n                            runtime_spec: spec.clone(),\n                            provider_config: config.clone(),\n                            metadata: HashMap::new(),\n                            created_at: Utc::now(),\n                            last_update: Utc::now(),\n                        };\n\n                        // Track the worker\n                        {\n                            let mut workers = self.workers.write().await;\n                            workers.insert(worker_id.clone(), DockerWorker {\n                                id: worker_id,\n                                container_id: container_info.id.clone(),\n                                network_name: Some(network_name),\n                                volume_mounts: Vec::new(),\n                                created_at: Utc::now(),\n                                last_health_check: Utc::now(),\n                            });\n                        }\n\n                        Ok(worker)\n                    }\n                    Err(e) =\u003e {\n                        // Clean up container if start failed\n                        let _ = self.client.remove_container(\u0026container_info.id, \u0026docker::Docker::default_timeout()).await;\n                        Err(ProviderError::infrastructure(\n                            format!(\"Failed to start Docker container: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n            }\n            Err(e) =\u003e {\n                // Clean up network if container creation failed\n                let _ = self.client.remove_network(\u0026network_name).await;\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to create Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn terminate_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            let network_name = worker.network_name.clone();\n            drop(workers);\n\n            // Stop and remove container\n            let remove_options = docker::models::RemoveContainerOptions {\n                force: true,\n                v: false,\n                link: false,\n            };\n\n            match self.client.stop_container(\u0026container_id, \u0026docker::Docker::default_timeout()).await {\n                Ok(_) =\u003e {\n                    match self.client.remove_container(\u0026container_id, \u0026remove_options).await {\n                        Ok(_) =\u003e {\n                            // Remove network if it exists\n                            if let Some(network) = network_name {\n                                let _ = self.client.remove_network(\u0026network).await;\n                            }\n\n                            // Remove from tracking\n                            {\n                                let mut workers = self.workers.write().await;\n                                workers.remove(worker_id);\n                            }\n\n                            Ok(())\n                        }\n                        Err(e) =\u003e Err(ProviderError::infrastructure(\n                            format!(\"Failed to remove Docker container: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to stop Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerState, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            match self.client.inspect_container(\u0026container_id).await {\n                Ok(inspect_info) =\u003e {\n                    // Update last health check\n                    {\n                        let mut workers = self.workers.write().await;\n                        if let Some(w) = workers.get_mut(worker_id) {\n                            w.last_health_check = Utc::now();\n                        }\n                    }\n\n                    match inspect_info.state {\n                        Some(state) =\u003e {\n                            match state.status {\n                                Some(docker::models::ContainerStateStatusEnum::RUNNING) =\u003e {\n                                    // Check if container has a health check and if it's healthy\n                                    if let Some(health) = \u0026inspect_info.health {\n                                        match health.status {\n                                            Some(docker::models::HealthStatus::HEALTHY) =\u003e Ok(WorkerState::Running),\n                                            Some(docker::models::HealthStatus::UNHEALTHY) =\u003e Ok(WorkerState::Failed {\n                                                reason: \"Health check failed\".to_string()\n                                            }),\n                                            _ =\u003e Ok(WorkerState::Running),\n                                        }\n                                    } else {\n                                        Ok(WorkerState::Running)\n                                    }\n                                }\n                                Some(docker::models::ContainerStateStatusEnum::EXITED) =\u003e {\n                                    Ok(WorkerState::Terminated)\n                                }\n                                Some(docker::models::ContainerStateStatusEnum::PAUSED) =\u003e {\n                                    Ok(WorkerState::Paused)\n                                }\n                                Some(docker::models::ContainerStateStatusEnum::DEAD) =\u003e {\n                                    Ok(WorkerState::Failed {\n                                        reason: \"Container is dead\".to_string()\n                                    })\n                                }\n                                _ =\u003e Ok(WorkerState::Unknown),\n                            }\n                        }\n                        None =\u003e Ok(WorkerState::Unknown),\n                    }\n                }\n                Err(e) =\u003e {\n                    if e.is_not_found() {\n                        Ok(WorkerState::Terminated)\n                    } else {\n                        Err(ProviderError::infrastructure(\n                            format!(\"Failed to inspect Docker container: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn get_logs(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cLogStreamRef, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Get logs through Docker API\n            let logs = self.get_container_logs(\u0026container_id).await?;\n\n            Ok(LogStreamRef {\n                stream_id: Uuid::new_v4(),\n                worker_id: worker_id.clone(),\n                format: LogFormat::Json,\n                endpoints: vec![\n                    format!(\"docker://logs/{}\", container_id),\n                ],\n            })\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn port_forward(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        local_port: u16,\n        remote_port: u16,\n    ) -\u003e Result\u003cString, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            let network_name = worker.network_name.clone().unwrap_or_default();\n            drop(workers);\n\n            // Docker port forwarding through exec or attach\n            let endpoint = format!(\"localhost:{} (container:{})\", local_port, container_id);\n            \n            // In practice, you would use Docker's port forwarding mechanisms\n            // This is a simplified implementation\n            Ok(endpoint)\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn get_capacity(\u0026self) -\u003e Result\u003cCapacityInfo, ProviderError\u003e {\n        match self.client.info().await {\n            Ok(info) =\u003e {\n                let active_workers = {\n                    let workers = self.workers.read().await;\n                    workers.len() as u32\n                };\n\n                Ok(CapacityInfo {\n                    total_resources: ResourceQuota {\n                        cpu_m: (info.cpu_count.unwrap_or(1) * 1000) as u64,\n                        memory_mb: (info.mem_total.unwrap_or(1024 * 1024 * 1024) / 1024 / 1024) as u64,\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    used_resources: ResourceQuota {\n                        cpu_m: active_workers as u64 * 500, // Estimate\n                        memory_mb: active_workers as u64 * 256, // Estimate\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    available_resources: ResourceQuota {\n                        cpu_m: ((info.cpu_count.unwrap_or(1) * 1000) as u64).saturating_sub(active_workers as u64 * 500),\n                        memory_mb: ((info.mem_total.unwrap_or(1024 * 1024 * 1024) / 1024 / 1024) as u64).saturating_sub(active_workers as u64 * 256),\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    active_workers,\n                    last_updated: Utc::now(),\n                })\n            }\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to get Docker capacity: {}\", e),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn execute_command(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        command: Vec\u003cString\u003e,\n        timeout: Option\u003cstd::time::Duration\u003e,\n    ) -\u003e Result\u003cExecutionResult, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            let start_time = std::time::Instant::now();\n            \n            // Execute command in container\n            let exec_config = docker::models::ExecCreateBody {\n                cmd: Some(command),\n                attach_stdout: Some(true),\n                attach_stderr: Some(true),\n                attach_stdin: Some(false),\n                working_dir: None,\n                user: None,\n                env: None,\n            };\n\n            match self.client.exec_create(\u0026container_id, \u0026exec_config).await {\n                Ok(exec_info) =\u003e {\n                    match self.client.exec_start(\u0026exec_info.id, \u0026docker::Docker::default_timeout()).await {\n                        Ok(output) =\u003e {\n                            let duration = start_time.elapsed();\n                            let finished_at = Utc::now();\n                            let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n\n                            let mut stdout = String::new();\n                            let mut stderr = String::new();\n\n                            for result in output {\n                                match result {\n                                    Ok(output) =\u003e {\n                                        match output {\n                                            docker::LogOutput::StdOut { message } =\u003e {\n                                                stdout.push_str(\u0026String::from_utf8_lossy(\u0026message));\n                                            }\n                                            docker::LogOutput::StdErr { message } =\u003e {\n                                                stderr.push_str(\u0026String::from_utf8_lossy(\u0026message));\n                                            }\n                                            _ =\u003e {}\n                                        }\n                                    }\n                                    Err(e) =\u003e {\n                                        stderr.push_str(\u0026format!(\"Error reading output: {}\", e));\n                                    }\n                                }\n                            }\n\n                            Ok(ExecutionResult {\n                                exit_code: 0,\n                                stdout,\n                                stderr,\n                                duration,\n                                started_at,\n                                finished_at,\n                            })\n                        }\n                        Err(e) =\u003e {\n                            let duration = start_time.elapsed();\n                            let finished_at = Utc::now();\n                            let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n\n                            Ok(ExecutionResult {\n                                exit_code: -1,\n                                stdout: String::new(),\n                                stderr: format!(\"Command execution failed: {}\", e),\n                                duration,\n                                started_at,\n                                finished_at,\n                            })\n                        }\n                    }\n                }\n                Err(e) =\u003e {\n                    let duration = start_time.elapsed();\n                    let finished_at = Utc::now();\n                    let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n\n                    Ok(ExecutionResult {\n                        exit_code: -1,\n                        stdout: String::new(),\n                        stderr: format!(\"Failed to create exec: {}\", e),\n                        duration,\n                        started_at,\n                        finished_at,\n                    })\n                }\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn restart_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Restart container\n            match self.client.restart_container(\u0026container_id, \u0026docker::Docker::default_timeout()).await {\n                Ok(_) =\u003e Ok(()),\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to restart Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn pause_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Pause container\n            match self.client.pause_container(\u0026container_id).await {\n                Ok(_) =\u003e Ok(()),\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to pause Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn resume_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Unpause container\n            match self.client.unpause_container(\u0026container_id).await {\n                Ok(_) =\u003e Ok(()),\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to resume Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    fn stream_worker_events(\u0026self) -\u003e tokio_stream::wrappers::IntervalStream {\n        let interval = tokio::time::interval(std::time::Duration::from_secs(30));\n        tokio_stream::wrappers::IntervalStream::new(interval)\n    }\n}\n\nimpl Drop for DockerProvider {\n    fn drop(\u0026mut self) {\n        // Clean up any remaining networks on drop\n        // In practice, you might want to implement a proper cleanup mechanism\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","providers","kubernetes","mod.rs"],"content":"//! Kubernetes Provider Implementation\n//! \n//! This module provides a complete implementation of the WorkerManagerProvider trait\n//! using Kubernetes as the underlying infrastructure provider.\n\npub mod client;\npub mod crds;\npub mod security;\n\nuse crate::traits::*;\nuse crate::credentials::CredentialProvider;\nuse async_trait::async_trait;\nuse chrono::Utc;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\nuse uuid::Uuid;\n\n/// Kubernetes-specific configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KubernetesConfig {\n    pub namespace: String,\n    pub service_account: Option\u003cString\u003e,\n    pub node_selector: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    pub tolerations: Option\u003cVec\u003cToleration\u003e\u003e,\n    pub node_affinity: Option\u003cString\u003e,\n    pub pod_disruption_budget: Option\u003cu32\u003e,\n    pub termination_grace_period_seconds: Option\u003ci64\u003e,\n    pub image_pull_secrets: Option\u003cVec\u003cString\u003e\u003e,\n    pub priority_class: Option\u003cString\u003e,\n    pub security_context: Option\u003cContainerSecurity\u003e,\n}\n\nimpl Default for KubernetesConfig {\n    fn default() -\u003e Self {\n        Self {\n            namespace: \"default\".to_string(),\n            service_account: None,\n            node_selector: None,\n            tolerations: None,\n            node_affinity: None,\n            pod_disruption_budget: None,\n            termination_grace_period_seconds: Some(30),\n            image_pull_secrets: None,\n            priority_class: None,\n            security_context: None,\n        }\n    }\n}\n\n/// Worker tracking in Kubernetes\n#[derive(Debug, Clone)]\nstruct KubernetesWorker {\n    id: WorkerId,\n    pod_name: String,\n    node_name: Option\u003cString\u003e,\n    created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    last_heartbeat: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Kubernetes Provider Implementation\npub struct KubernetesProvider {\n    client: kube::Client,\n    config: KubernetesConfig,\n    workers: Arc\u003cRwLock\u003cHashMap\u003cWorkerId, KubernetesWorker\u003e\u003e\u003e,\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n}\n\nimpl KubernetesProvider {\n    /// Create a new Kubernetes Provider\n    pub async fn new(\n        config: KubernetesConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let client = kube::Client::try_default()\n            .await\n            .map_err(|e| ProviderError::infrastructure(\n                format!(\"Failed to create Kubernetes client: {}\", e),\n                \"kubernetes\".to_string()\n            ))?;\n\n        Ok(Self {\n            client,\n            config,\n            workers: Arc::new(RwLock::new(HashMap::new())),\n            credential_provider,\n        })\n    }\n\n    /// Get Kubernetes configuration from ProviderConfig\n    pub fn from_provider_config(\n        provider_config: \u0026ProviderConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let mut config = KubernetesConfig::default();\n        \n        if let serde_json::Value::Object(obj) = \u0026provider_config.specific_config {\n            if let Some(namespace) = obj.get(\"namespace\").and_then(|v| v.as_str()) {\n                config.namespace = namespace.to_string();\n            }\n            if let Some(service_account) = obj.get(\"service_account\").and_then(|v| v.as_str()) {\n                config.service_account = Some(service_account.to_string());\n            }\n            // Additional Kubernetes-specific configurations...\n        }\n\n        Self::new(config, credential_provider)\n    }\n}\n\n#[async_trait]\nimpl WorkerManagerProvider for KubernetesProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"kubernetes\"\n    }\n\n    async fn create_worker(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        config: \u0026ProviderConfig,\n    ) -\u003e Result\u003cWorker, ProviderError\u003e {\n        let worker_id = WorkerId::new();\n        let mut labels = spec.labels.clone();\n        labels.insert(\"worker-id\".to_string(), worker_id.to_string());\n        labels.insert(\"provider\".to_string(), \"kubernetes\".to_string());\n        labels.insert(\"created-at\".to_string(), Utc::now().to_rfc3339());\n\n        // Create pod specification\n        let mut pod_spec = k8s_openapi::api::core::v1::PodSpec::default();\n        pod_spec.containers = vec![self.create_container_spec(spec, \u0026labels).await?];\n        pod_spec.service_account_name = self.config.service_account.clone().unwrap_or_default();\n        pod_spec.node_selector = self.config.node_selector.clone();\n        pod_spec.tolerations = self.config.tolerations.clone();\n        pod_spec.termination_grace_period_seconds = self.config.termination_grace_period_seconds;\n\n        if let Some(pdb) = self.config.pod_disruption_budget {\n            pod_spec.min_ready_seconds = Some(5);\n        }\n\n        // Configure security context\n        if let Some(security_ctx) = \u0026self.config.security_context {\n            pod_spec.security_context = Some(k8s_openapi::api::core::v1::PodSecurityContext {\n                run_as_user: security_ctx.user,\n                se_linux_options: security_ctx.se_linux_options.clone().map(|opts| k8s_openapi::api::core::v1::SELinuxOptions {\n                    user: opts.get(\"user\").cloned(),\n                    role: opts.get(\"role\").cloned(),\n                    type_: opts.get(\"type\").cloned(),\n                    level: opts.get(\"level\").cloned(),\n                }),\n                ..Default::default()\n            });\n        }\n\n        let mut pod = k8s_openapi::api::core::v1::Pod {\n            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {\n                name: Some(format!(\"worker-{}\", worker_id)),\n                namespace: Some(self.config.namespace.clone()),\n                labels: Some(labels),\n                annotations: Some(HashMap::from([\n                    (\"created-by\".to_string(), \"worker-manager\".to_string()),\n                    (\"runtime-spec-hash\".to_string(), format!(\"{:?}\", std::collections::hash_map::DefaultHasher::new().finish())),\n                ])),\n                ..Default::default()\n            },\n            spec: Some(pod_spec),\n            status: None,\n        };\n\n        // Add volumes for secrets and configmaps\n        let volumes = self.create_volumes(spec).await?;\n        if !volumes.is_empty() {\n            pod.spec.as_mut().unwrap().volumes = Some(volumes);\n        }\n\n        // Add resource limits\n        self.apply_resource_limits(\u0026mut pod, \u0026spec.resources)?;\n\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        // Create the pod\n        match api.create(\u0026kube::PostParams::default(), \u0026pod).await {\n            Ok(created_pod) =\u003e {\n                let worker = Worker {\n                    id: worker_id.clone(),\n                    state: WorkerState::Creating,\n                    runtime_spec: spec.clone(),\n                    provider_config: config.clone(),\n                    metadata: HashMap::new(),\n                    created_at: Utc::now(),\n                    last_update: Utc::now(),\n                };\n\n                // Track the worker\n                {\n                    let mut workers = self.workers.write().await;\n                    workers.insert(worker_id.clone(), KubernetesWorker {\n                        id: worker_id,\n                        pod_name: format!(\"worker-{}\", worker_id),\n                        node_name: None,\n                        created_at: Utc::now(),\n                        last_heartbeat: Utc::now(),\n                    });\n                }\n\n                Ok(worker)\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to create Kubernetes pod: {}\", e),\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn terminate_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.delete(\u0026pod_name, \u0026kube::DeleteParams::default()).await {\n            Ok(_) =\u003e {\n                // Remove from tracking\n                {\n                    let mut workers = self.workers.write().await;\n                    workers.remove(worker_id);\n                }\n                Ok(())\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to terminate Kubernetes pod {}: {}\", pod_name, e),\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerState, ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.get(\u0026pod_name).await {\n            Ok(pod) =\u003e {\n                // Update heartbeat\n                {\n                    let mut workers = self.workers.write().await;\n                    if let Some(worker) = workers.get_mut(worker_id) {\n                        worker.last_heartbeat = Utc::now();\n                    }\n                }\n\n                match pod.status {\n                    Some(status) =\u003e {\n                        match status.phase {\n                            Some(k8s_openapi::api::core::v1::PodPhase::Running) =\u003e Ok(WorkerState::Running),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Pending) =\u003e Ok(WorkerState::Creating),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Failed) =\u003e Ok(WorkerState::Failed {\n                                reason: status.message.clone().unwrap_or_default()\n                            }),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Succeeded) =\u003e Ok(WorkerState::Terminated),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Unknown) =\u003e Ok(WorkerState::Unknown),\n                            _ =\u003e Ok(WorkerState::Unknown),\n                        }\n                    }\n                    None =\u003e Ok(WorkerState::Creating),\n                }\n            }\n            Err(e) =\u003e {\n                if e.is_not_found() {\n                    Ok(WorkerState::Terminated)\n                } else {\n                    Err(ProviderError::infrastructure(\n                        format!(\"Failed to get Kubernetes pod status: {}\", e),\n                        \"kubernetes\".to_string()\n                    ))\n                }\n            }\n        }\n    }\n\n    async fn get_logs(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cLogStreamRef, ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.get(\u0026pod_name).await {\n            Ok(pod) =\u003e {\n                // Note: This is a simplified log reference - in practice, you would\n                // typically use a log aggregation system or the Kubernetes log API\n                let log_ref = LogStreamRef {\n                    stream_id: Uuid::new_v4(),\n                    worker_id: worker_id.clone(),\n                    format: LogFormat::Json,\n                    endpoints: vec![\n                        format!(\"https://{}/api/v1/namespaces/{}/pods/{}/log?container={}\", \n                               \"kubernetes-api\", self.config.namespace, pod_name),\n                    ],\n                };\n                Ok(log_ref)\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::not_found(\n                    format!(\"Pod not found: {}\", e),\n                    \"pod\".to_string(),\n                    pod_name,\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn port_forward(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        local_port: u16,\n        remote_port: u16,\n    ) -\u003e Result\u003cString, ProviderError\u003e {\n        // Kubernetes port forwarding implementation\n        // This would typically involve using kubectl port-forward or a similar mechanism\n        // For now, return a placeholder endpoint\n        let endpoint = format!(\"localhost:{}\", local_port);\n        Ok(endpoint)\n    }\n\n    async fn get_capacity(\u0026self) -\u003e Result\u003cCapacityInfo, ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Node\u003e = kube::Api::all(self.client.clone());\n        \n        match api.list(\u0026kube::ListParams::default()).await {\n            Ok(nodes) =\u003e {\n                let mut total_cpu = 0u64;\n                let mut total_memory = 0u64;\n                let mut used_cpu = 0u64;\n                let mut used_memory = 0u64;\n                let mut active_workers = 0u32;\n\n                for node in nodes.items {\n                    if let Some(status) = \u0026node.status {\n                        if let Some(capacity) = \u0026status.capacity {\n                            if let Some(cpu) = capacity.get(\"cpu\").and_then(|c| c.as_str()) {\n                                total_cpu += cpu.parse::\u003cu64\u003e().unwrap_or(0) * 1000; // Convert to milli-cores\n                            }\n                            if let Some(memory) = capacity.get(\"memory\").and_then(|m| m.as_str()) {\n                                total_memory += memory.parse::\u003cu64\u003e().unwrap_or(0) / 1024 / 1024; // Convert to MB\n                            }\n                        }\n                        \n                        if let Some(allocatable) = \u0026status.allocatable {\n                            if let Some(cpu) = allocatable.get(\"cpu\").and_then(|c| c.as_str()) {\n                                used_cpu += cpu.parse::\u003cu64\u003e().unwrap_or(0) * 1000;\n                            }\n                            if let Some(memory) = allocatable.get(\"memory\").and_then(|m| m.as_str()) {\n                                used_memory += memory.parse::\u003cu64\u003e().unwrap_or(0) / 1024 / 1024;\n                            }\n                        }\n                    }\n                }\n\n                // Count active workers\n                {\n                    let workers = self.workers.read().await;\n                    active_workers = workers.len() as u32;\n                }\n\n                Ok(CapacityInfo {\n                    total_resources: ResourceQuota {\n                        cpu_m: total_cpu,\n                        memory_mb: total_memory,\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    used_resources: ResourceQuota {\n                        cpu_m: used_cpu,\n                        memory_mb: used_memory,\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    available_resources: ResourceQuota {\n                        cpu_m: total_cpu.saturating_sub(used_cpu),\n                        memory_mb: total_memory.saturating_sub(used_memory),\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    active_workers,\n                    last_updated: Utc::now(),\n                })\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to get Kubernetes capacity: {}\", e),\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn execute_command(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        command: Vec\u003cString\u003e,\n        timeout: Option\u003cstd::time::Duration\u003e,\n    ) -\u003e Result\u003cExecutionResult, ProviderError\u003e {\n        // Kubernetes command execution via exec API\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        let timeout_duration = timeout.unwrap_or(std::time::Duration::from_secs(30));\n        \n        let start_time = std::time::Instant::now();\n        \n        match api.exec(\u0026pod_name, \u0026command, \u0026kube::ExecParams::default()).await {\n            Ok(output) =\u003e {\n                let duration = start_time.elapsed();\n                let finished_at = Utc::now();\n                let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n                \n                Ok(ExecutionResult {\n                    exit_code: 0,\n                    stdout: output.stdout,\n                    stderr: output.stderr,\n                    duration,\n                    started_at,\n                    finished_at,\n                })\n            }\n            Err(e) =\u003e {\n                let duration = start_time.elapsed();\n                let finished_at = Utc::now();\n                let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n                \n                Ok(ExecutionResult {\n                    exit_code: -1,\n                    stdout: String::new(),\n                    stderr: format!(\"Command execution failed: {}\", e),\n                    duration,\n                    started_at,\n                    finished_at,\n                })\n            }\n        }\n    }\n\n    async fn restart_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.get(\u0026pod_name).await {\n            Ok(mut pod) =\u003e {\n                // Update the pod's generation to force a restart\n                if let Some(metadata) = \u0026mut pod.metadata {\n                    if let Some(annotations) = \u0026mut metadata.annotations {\n                        annotations.insert(\n                            \"restart-timestamp\".to_string(),\n                            Utc::now().to_rfc3339(),\n                        );\n                    }\n                }\n                \n                match api.patch(\u0026pod_name, \u0026kube::PatchParams::default(), \n                             \u0026kube::Patch::\u003ckube::json::Json\u003e::Merge(pod)).await {\n                    Ok(_) =\u003e Ok(()),\n                    Err(e) =\u003e Err(ProviderError::infrastructure(\n                        format!(\"Failed to restart Kubernetes pod: {}\", e),\n                        \"kubernetes\".to_string()\n                    ))\n                }\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::not_found(\n                    format!(\"Pod not found for restart: {}\", e),\n                    \"pod\".to_string(),\n                    pod_name,\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn pause_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        // Implement pause by updating pod annotations\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        let patch = serde_json::json!({\n            \"metadata\": {\n                \"annotations\": {\n                    \"worker-state\": \"paused\",\n                    \"paused-at\": Utc::now().to_rfc3339()\n                }\n            }\n        });\n        \n        match api.patch(\u0026pod_name, \u0026kube::PatchParams::default(), \n                      \u0026kube::Patch::\u003ckube::json::Json\u003e::Merge(patch)).await {\n            Ok(_) =\u003e Ok(()),\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to pause Kubernetes pod: {}\", e),\n                \"kubernetes\".to_string()\n            ))\n        }\n    }\n\n    async fn resume_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        // Remove pause annotations\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        let patch = serde_json::json!({\n            \"metadata\": {\n                \"annotations\": {\n                    \"worker-state\": \"running\"\n                }\n            }\n        });\n        \n        match api.patch(\u0026pod_name, \u0026kube::PatchParams::default(), \n                      \u0026kube::Patch::\u003ckube::json::Json\u003e::Merge(patch)).await {\n            Ok(_) =\u003e Ok(()),\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to resume Kubernetes pod: {}\", e),\n                \"kubernetes\".to_string()\n            ))\n        }\n    }\n\n    fn stream_worker_events(\u0026self) -\u003e tokio_stream::wrappers::IntervalStream {\n        let interval = tokio::time::interval(std::time::Duration::from_secs(30));\n        tokio_stream::wrappers::IntervalStream::new(interval)\n    }\n}\n\nimpl KubernetesProvider {\n    /// Create container specification from runtime spec\n    async fn create_container_spec(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        labels: \u0026std::collections::HashMap\u003cString, String\u003e,\n    ) -\u003e Result\u003ck8s_openapi::api::core::v1::Container, ProviderError\u003e {\n        let mut container = k8s_openapi::api::core::v1::Container {\n            name: \"worker\".to_string(),\n            image: Some(spec.image.clone()),\n            command: spec.command.clone(),\n            args: None,\n            working_dir: None,\n            ports: Some(spec.ports.iter().map(|port| k8s_openapi::api::core::v1::ContainerPort {\n                container_port: *port as i32,\n                protocol: Some(k8s_openapi::api::core::v1::Protocol::TCP),\n                ..Default::default()\n            }).collect()),\n            env_from: None,\n            env: Some(spec.env.iter().map(|(k, v)| k8s_openapi::api::core::v1::EnvVar {\n                name: k.clone(),\n                value: Some(v.clone()),\n                value_from: None,\n            }).collect()),\n            resources: Some(k8s_openapi::api::core::v1::ResourceRequirements {\n                requests: Some(std::collections::HashMap::from([\n                    (\"cpu\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity(spec.resources.cpu_m.to_string() + \"m\")),\n                    (\"memory\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity((spec.resources.memory_mb * 1024 * 1024).to_string())),\n                ])),\n                limits: Some(std::collections::HashMap::from([\n                    (\"cpu\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity(spec.resources.cpu_m.to_string() + \"m\")),\n                    (\"memory\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity((spec.resources.memory_mb * 1024 * 1024).to_string())),\n                ])),\n                ..Default::default()\n            }),\n            volume_mounts: Some(Vec::new()), // Will be filled by create_volumes\n            liveness_probe: None,\n            readiness_probe: None,\n            startup_probe: None,\n            ..Default::default()\n        };\n\n        // Add health checks\n        let health_check = k8s_openapi::api::core::v1::Probe {\n            http_get: Some(k8s_openapi::api::core::v1::HTTPGetAction {\n                path: Some(\"/health\".to_string()),\n                port: 8080.try_into().unwrap(),\n                host: None,\n                scheme: Some(k8s_openapi::api::core::v1::URIScheme::HTTP),\n            }),\n            tcp_socket: None,\n            exec: None,\n            period_seconds: Some(30),\n            timeout_seconds: Some(5),\n            success_threshold: Some(1),\n            failure_threshold: Some(3),\n            termination_grace_period_seconds: None,\n        };\n\n        container.liveness_probe = Some(health_check.clone());\n        container.readiness_probe = Some(health_check);\n\n        Ok(container)\n    }\n\n    /// Create volumes for secrets and configmaps\n    async fn create_volumes(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n    ) -\u003e Result\u003cVec\u003ck8s_openapi::api::core::v1::Volume\u003e, ProviderError\u003e {\n        let mut volumes = Vec::new();\n\n        for volume_mount in \u0026spec.volumes {\n            let volume = match \u0026volume_mount.source {\n                VolumeSource::Secret(secret_name) =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: format!(\"secret-{}\", secret_name),\n                        config_map: Some(k8s_openapi::api::core::v1::ConfigMapVolumeSource {\n                            default_mode: Some(0o444),\n                            items: None,\n                            name: Some(secret_name.clone()),\n                            optional: None,\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::ConfigMap(config_name) =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: format!(\"config-{}\", config_name),\n                        config_map: Some(k8s_openapi::api::core::v1::ConfigMapVolumeSource {\n                            default_mode: Some(0o444),\n                            items: None,\n                            name: Some(config_name.clone()),\n                            optional: None,\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::EmptyDir =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: \"empty-dir\".to_string(),\n                        empty_dir: Some(k8s_openapi::api::core::v1::EmptyDirVolumeSource {\n                            medium: Some(k8s_openapi::api::core::v1::StorageMedium::Memory),\n                            size_limit: None,\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::HostPath(path) =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: format!(\"host-path-{}\", path.replace(\"/\", \"-\")),\n                        host_path: Some(k8s_openapi::api::core::v1::HostPathVolumeSource {\n                            path: path.clone(),\n                            type_: Some(k8s_openapi::api::core::v1::HostPathType::DirectoryOrCreate),\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::PersistentVolume(_) =\u003e {\n                    // For persistent volumes, you would typically reference a PVC\n                    k8s_openapi::api::core::v1::Volume {\n                        name: \"persistent-volume\".to_string(),\n                        persistent_volume_claim: Some(k8s_openapi::api::core::v1::PersistentVolumeClaimVolumeSource {\n                            claim_name: \"placeholder-pvc\".to_string(),\n                            read_only: Some(volume_mount.read_only),\n                        }),\n                        ..Default::default()\n                    }\n                }\n            };\n\n            volumes.push(volume);\n        }\n\n        Ok(volumes)\n    }\n\n    /// Apply resource limits to pod\n    fn apply_resource_limits(\n        \u0026self,\n        pod: \u0026mut k8s_openapi::api::core::v1::Pod,\n        resources: \u0026ResourceQuota,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        if let Some(spec) = pod.spec.as_mut() {\n            if let Some(container) = spec.containers.first_mut() {\n                if let Some(resources) = \u0026mut container.resources {\n                    resources.limits = Some(std::collections::HashMap::from([\n                        (\"cpu\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity(resources.cpu_m.to_string() + \"m\")),\n                        (\"memory\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity((resources.memory_mb * 1024 * 1024).to_string())),\n                    ]));\n                    \n                    if let Some(storage_mb) = resources.storage_mb {\n                        resources.limits.as_mut().unwrap().insert(\n                            \"ephemeral-storage\".to_string(),\n                            k8s_openapi::apimachinery::pkg::api::resource::Quantity((storage_mb * 1024 * 1024).to_string()),\n                        );\n                    }\n\n                    if let Some(gpu) = resources.gpu {\n                        resources.limits.as_mut().unwrap().insert(\n                            \"nvidia.com/gpu\".to_string(),\n                            k8s_openapi::apimachinery::pkg::api::resource::Quantity(gpu.to_string()),\n                        );\n                    }\n                }\n            }\n        }\n\n        Ok(())\n    }\n}\n","traces":[],"covered":0,"coverable":0}]};
        var previousData = {"files":[{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","concurrency-patterns","src","lib.rs"],"content":"//! Concurrency patterns module\n\npub mod worker_pools;\n\npub use worker_pools::{BackpressureController, CircuitBreaker, DynamicWorkerPool};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","concurrency-patterns","src","worker_pools.rs"],"content":"//! Optimized Tokio worker pools for high concurrency\n\nuse parking_lot::Mutex;\nuse std::collections::HashMap;\nuse std::sync::{\n    Arc,\n    atomic::{AtomicUsize, Ordering},\n};\n\n/// Worker handle (optimized)\ntype WorkerHandle = tokio::task::JoinHandle\u003c()\u003e;\n\n/// Dynamic worker pool that auto-scales based on load\npub struct DynamicWorkerPool {\n    workers: Arc\u003cMutex\u003cHashMap\u003cString, WorkerHandle\u003e\u003e\u003e,\n    max_workers: usize,\n    min_workers: usize,\n    active_tasks: Arc\u003cAtomicUsize\u003e,\n    queue: Arc\u003ccrossbeam::queue::SegQueue\u003ctokio::task::JoinHandle\u003c()\u003e\u003e\u003e,\n}\n\nimpl DynamicWorkerPool {\n    pub fn new(min_workers: usize, max_workers: usize) -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n            min_workers,\n            max_workers,\n            active_tasks: Arc::new(AtomicUsize::new(0)),\n            queue: Arc::new(crossbeam::queue::SegQueue::new()),\n        }\n    }\n\n    /// Submit work to the pool (optimized)\n    pub async fn submit\u003cF, Fut\u003e(\u0026self, worker_id: \u0026str, work: F) -\u003e Result\u003c(), WorkerPoolError\u003e\n    where\n        F: FnOnce() -\u003e Fut + Send + 'static,\n        Fut: std::future::Future\u003cOutput = ()\u003e + Send,\n    {\n        let workers = self.workers.lock();\n        if workers.contains_key(worker_id) {\n            drop(workers);\n\n            // Increment active task counter\n            self.active_tasks.fetch_add(1, Ordering::SeqCst);\n\n            let active_tasks = Arc::clone(\u0026self.active_tasks);\n\n            // Spawn optimized task\n            let handle = tokio::spawn(async move {\n                work().await;\n                active_tasks.fetch_sub(1, Ordering::SeqCst);\n            });\n\n            self.queue.push(handle);\n            Ok(())\n        } else {\n            Err(WorkerPoolError::WorkerNotFound)\n        }\n    }\n\n    /// Get current load metric\n    pub fn current_load(\u0026self) -\u003e f64 {\n        let active = self.active_tasks.load(Ordering::SeqCst);\n        let total = self.workers.lock().len().max(1);\n        active as f64 / total as f64\n    }\n\n    /// Auto-scale workers based on load\n    pub fn auto_scale(\u0026self) {\n        let load = self.current_load();\n        let mut workers = self.workers.lock();\n\n        if load \u003e 0.8 \u0026\u0026 workers.len() \u003c self.max_workers {\n            let new_worker_id = format!(\"worker-{}\", workers.len());\n            // Spawn new worker (simplified)\n            let handle = tokio::spawn(async move {\n                tokio::time::sleep(std::time::Duration::from_secs(3600)).await;\n            });\n            workers.insert(new_worker_id, handle);\n        } else if load \u003c 0.3 \u0026\u0026 workers.len() \u003e self.min_workers {\n            // Remove excess worker (simplified)\n            if let Some(key) = workers.keys().next().cloned() {\n                if let Some(handle) = workers.remove(\u0026key) {\n                    handle.abort();\n                }\n            }\n        }\n    }\n}\n\nimpl Drop for DynamicWorkerPool {\n    fn drop(\u0026mut self) {\n        // Graceful shutdown of all workers\n        let workers = self.workers.lock();\n        for handle in workers.values() {\n            handle.abort();\n        }\n    }\n}\n\n/// Optimized Circuit breaker for resilience\npub struct CircuitBreaker {\n    state: Arc\u003cparking_lot::Mutex\u003cCircuitBreakerState\u003e\u003e,\n    threshold: u32,\n    timeout: std::time::Duration,\n    failures: Arc\u003cAtomicUsize\u003e,\n}\n\n#[derive(Debug, Clone, PartialEq)]\nenum CircuitBreakerState {\n    Closed,\n    Open,\n    HalfOpen,\n    LastFailure(std::time::Instant),\n}\n\nimpl CircuitBreaker {\n    pub fn new(threshold: u32, timeout: std::time::Duration) -\u003e Self {\n        Self {\n            state: Arc::new(parking_lot::Mutex::new(CircuitBreakerState::Closed)),\n            threshold,\n            timeout,\n            failures: Arc::new(AtomicUsize::new(0)),\n        }\n    }\n\n    pub async fn execute\u003cF, Fut, T\u003e(\u0026self, operation: F) -\u003e Result\u003cT, CircuitBreakerError\u003e\n    where\n        F: FnOnce() -\u003e Fut + Send,\n        Fut: std::future::Future\u003cOutput = Result\u003cT, Box\u003cdyn std::error::Error + Send + Sync\u003e\u003e\u003e\n            + Send,\n    {\n        let state = self.state.lock();\n\n        match *state {\n            CircuitBreakerState::Open =\u003e {\n                drop(state);\n                Err(CircuitBreakerError::Open)\n            }\n            CircuitBreakerState::HalfOpen =\u003e {\n                drop(state);\n                match operation().await {\n                    Ok(result) =\u003e {\n                        self.reset();\n                        Ok(result)\n                    }\n                    Err(_) =\u003e {\n                        self.record_failure();\n                        Err(CircuitBreakerError::ExecutionFailed)\n                    }\n                }\n            }\n            _ =\u003e {\n                drop(state);\n                match operation().await {\n                    Ok(result) =\u003e {\n                        self.reset();\n                        Ok(result)\n                    }\n                    Err(_) =\u003e {\n                        self.record_failure();\n                        Err(CircuitBreakerError::ExecutionFailed)\n                    }\n                }\n            }\n        }\n    }\n\n    fn record_failure(\u0026self) {\n        let failures = self.failures.fetch_add(1, Ordering::SeqCst) + 1;\n        if failures \u003e= self.threshold as usize {\n            let mut state = self.state.lock();\n            *state = CircuitBreakerState::Open;\n        }\n    }\n\n    fn reset(\u0026self) {\n        self.failures.store(0, Ordering::SeqCst);\n        let mut state = self.state.lock();\n        *state = CircuitBreakerState::Closed;\n    }\n\n    pub fn half_open(\u0026self) {\n        let mut state = self.state.lock();\n        *state = CircuitBreakerState::HalfOpen;\n    }\n}\n\n/// Optimized Backpressure controller\npub struct BackpressureController {\n    max_queue_size: usize,\n    current_queue_size: Arc\u003cAtomicUsize\u003e,\n    rejection_count: Arc\u003cAtomicUsize\u003e,\n}\n\nimpl BackpressureController {\n    pub fn new(max_queue_size: usize) -\u003e Self {\n        Self {\n            max_queue_size,\n            current_queue_size: Arc::new(AtomicUsize::new(0)),\n            rejection_count: Arc::new(AtomicUsize::new(0)),\n        }\n    }\n\n    pub fn can_accept(\u0026self) -\u003e bool {\n        let size = self.current_queue_size.load(Ordering::SeqCst);\n        size \u003c self.max_queue_size\n    }\n\n    pub fn record_enqueue(\u0026self) -\u003e Result\u003c(), BackpressureError\u003e {\n        let size = self.current_queue_size.fetch_add(1, Ordering::SeqCst) + 1;\n        if size \u003e self.max_queue_size {\n            self.current_queue_size.fetch_sub(1, Ordering::SeqCst);\n            self.rejection_count.fetch_add(1, Ordering::SeqCst);\n            return Err(BackpressureError::QueueFull);\n        }\n        Ok(())\n    }\n\n    pub fn record_dequeue(\u0026self) {\n        self.current_queue_size.fetch_sub(1, Ordering::SeqCst);\n    }\n\n    pub fn rejection_rate(\u0026self) -\u003e f64 {\n        let rejections = self.rejection_count.load(Ordering::SeqCst) as f64;\n        let total_attempts = rejections + self.current_queue_size.load(Ordering::SeqCst) as f64;\n        if total_attempts \u003e 0.0 {\n            rejections / total_attempts\n        } else {\n            0.0\n        }\n    }\n}\n\n/// Rate limiter with token bucket\npub struct RateLimiter {\n    capacity: usize,\n    tokens: Arc\u003cAtomicUsize\u003e,\n    refill_rate: std::time::Duration,\n    last_refill: Arc\u003cMutex\u003cstd::time::Instant\u003e\u003e,\n}\n\nimpl RateLimiter {\n    pub fn new(capacity: usize, refill_rate: std::time::Duration) -\u003e Self {\n        let tokens = Arc::new(AtomicUsize::new(capacity));\n        let last_refill = Arc::new(Mutex::new(std::time::Instant::now()));\n\n        // Start refill task\n        let tokens_ref = Arc::clone(\u0026tokens);\n        let last_refill_ref = Arc::clone(\u0026last_refill);\n        let refill_rate_clone = refill_rate;\n        tokio::spawn(async move {\n            let mut interval = tokio::time::interval(refill_rate_clone);\n            loop {\n                interval.tick().await;\n                let mut last = last_refill_ref.lock();\n                *last = std::time::Instant::now();\n                tokens_ref.store(capacity, Ordering::SeqCst);\n            }\n        });\n\n        Self {\n            capacity,\n            tokens,\n            refill_rate,\n            last_refill,\n        }\n    }\n\n    pub async fn acquire(\u0026self) -\u003e Result\u003c(), RateLimitError\u003e {\n        let current = self.tokens.fetch_sub(1, Ordering::SeqCst);\n        if current \u003e 0 {\n            Ok(())\n        } else {\n            self.tokens.fetch_add(1, Ordering::SeqCst); // Restore token\n            Err(RateLimitError::RateExceeded)\n        }\n    }\n\n    pub fn remaining_tokens(\u0026self) -\u003e usize {\n        self.tokens.load(Ordering::SeqCst)\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum WorkerPoolError {\n    #[error(\"worker not found\")]\n    WorkerNotFound,\n\n    #[error(\"pool at capacity\")]\n    AtCapacity,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum CircuitBreakerError {\n    #[error(\"circuit breaker is open\")]\n    Open,\n\n    #[error(\"execution failed\")]\n    ExecutionFailed,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum BackpressureError {\n    #[error(\"queue is full\")]\n    QueueFull,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum RateLimitError {\n    #[error(\"rate limit exceeded\")]\n    RateExceeded,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_worker_pool_load() {\n        let pool = DynamicWorkerPool::new(1, 5);\n\n        // Check load calculation with no active tasks\n        let load = pool.current_load();\n        assert_eq!(load, 0.0);\n\n        // Test auto-scaling\n        pool.auto_scale();\n    }\n\n    #[tokio::test]\n    async fn test_circuit_breaker() {\n        let breaker = CircuitBreaker::new(2, std::time::Duration::from_secs(1));\n\n        // First failure\n        assert!(\n            breaker\n                .execute(|| async { Err::\u003c(), _\u003e(\"fail\".into()) })\n                .await\n                .is_err()\n        );\n\n        // Second failure - should open\n        assert!(\n            breaker\n                .execute(|| async { Err::\u003c(), _\u003e(\"fail\".into()) })\n                .await\n                .is_err()\n        );\n\n        // Should be open now\n        assert!(matches!(\n            breaker.execute(|| async { Ok::\u003c(), _\u003e(()) }).await,\n            Err(CircuitBreakerError::Open)\n        ));\n    }\n\n    #[tokio::test]\n    async fn test_backpressure() {\n        let controller = BackpressureController::new(2);\n\n        assert!(controller.record_enqueue().is_ok());\n        assert!(controller.record_enqueue().is_ok());\n\n        // Should fail on third\n        assert!(matches!(\n            controller.record_enqueue(),\n            Err(BackpressureError::QueueFull)\n        ));\n    }\n}\n","traces":[{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":34},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","coordinator","src","lib.rs"],"content":"//! Distributed Coordinator implementation (US-003)\n//!\n//! This module provides job coordination with load balancing and failure detection.\n\nuse async_trait::async_trait;\nuse hodei_distributed_comm::EventBus;\nuse hodei_shared_types::job_definitions::{JobId, JobSpec};\nuse hodei_shared_types::{CorrelationId, DomainError, TenantId, Uuid};\nuse parking_lot::Mutex;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n/// Worker information for load balancing\n#[derive(Debug, Clone)]\npub struct WorkerInfo {\n    pub id: Uuid,\n    pub load: f64,\n    pub capabilities: Vec\u003cString\u003e,\n    pub is_healthy: bool,\n}\n\n/// Load balancer for selecting optimal workers\npub struct LoadBalancer {\n    workers: Arc\u003cMutex\u003cHashMap\u003cUuid, WorkerInfo\u003e\u003e\u003e,\n}\n\nimpl LoadBalancer {\n    pub fn new() -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    pub fn add_worker(\u0026self, worker: WorkerInfo) {\n        let mut workers = self.workers.lock();\n        workers.insert(worker.id, worker);\n    }\n\n    pub fn select_worker(\u0026self, required_capabilities: \u0026[String]) -\u003e Option\u003cUuid\u003e {\n        let workers = self.workers.lock();\n\n        let mut candidates: Vec\u003c_\u003e = workers\n            .values()\n            .filter(|w| {\n                w.is_healthy\n                    \u0026\u0026 required_capabilities\n                        .iter()\n                        .all(|cap| w.capabilities.contains(cap))\n            })\n            .collect();\n\n        if candidates.is_empty() {\n            return None;\n        }\n\n        // Select worker with minimum load\n        candidates.sort_by(|a, b| {\n            a.load\n                .partial_cmp(\u0026b.load)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n        Some(candidates[0].id)\n    }\n\n    pub fn update_load(\u0026self, worker_id: Uuid, new_load: f64) {\n        let mut workers = self.workers.lock();\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.load = new_load;\n        }\n    }\n\n    pub fn mark_unhealthy(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.is_healthy = false;\n        }\n    }\n}\n\n/// Failure detector based on heartbeat\npub struct FailureDetector {\n    workers: Arc\u003cMutex\u003cHashMap\u003cUuid, std::time::Instant\u003e\u003e\u003e,\n    timeout: std::time::Duration,\n}\n\nimpl FailureDetector {\n    pub fn new(timeout: std::time::Duration) -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n            timeout,\n        }\n    }\n\n    pub fn record_heartbeat(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        workers.insert(worker_id, std::time::Instant::now());\n    }\n\n    pub fn detect_failures(\u0026self) -\u003e Vec\u003cUuid\u003e {\n        let workers = self.workers.lock();\n        let now = std::time::Instant::now();\n\n        workers\n            .iter()\n            .filter(|(_, last_seen)| now.duration_since(**last_seen) \u003e self.timeout)\n            .map(|(id, _)| *id)\n            .collect()\n    }\n\n    pub fn remove_worker(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        workers.remove(\u0026worker_id);\n    }\n}\n\n/// Job Coordinator\npub struct JobCoordinator {\n    load_balancer: Arc\u003cLoadBalancer\u003e,\n    failure_detector: Arc\u003cFailureDetector\u003e,\n    event_bus: Option\u003cArc\u003chodei_distributed_comm::NatsEventBus\u003e\u003e,\n}\n\nimpl JobCoordinator {\n    pub fn new() -\u003e Self {\n        Self {\n            load_balancer: Arc::new(LoadBalancer::new()),\n            failure_detector: Arc::new(FailureDetector::new(std::time::Duration::from_secs(30))),\n            event_bus: None,\n        }\n    }\n\n    pub fn with_event_bus(mut self, event_bus: Arc\u003chodei_distributed_comm::NatsEventBus\u003e) -\u003e Self {\n        self.event_bus = Some(event_bus);\n        self\n    }\n\n    /// Register a new worker\n    pub fn register_worker(\u0026self, worker: WorkerInfo) {\n        self.load_balancer.add_worker(worker.clone());\n        self.failure_detector.record_heartbeat(worker.id);\n    }\n\n    /// Schedule a job to an available worker\n    pub async fn schedule_job(\n        \u0026self,\n        job_id: JobId,\n        job_spec: \u0026JobSpec,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003cUuid, DomainError\u003e {\n        // Extract required capabilities from job spec (simplified)\n        let required_capabilities = vec![\"default\".to_string()];\n\n        // Select optimal worker\n        let worker_id = self\n            .load_balancer\n            .select_worker(\u0026required_capabilities)\n            .ok_or_else(|| DomainError::Infrastructure(\"no suitable worker found\".to_string()))?;\n\n        // Record heartbeat\n        self.failure_detector.record_heartbeat(worker_id);\n\n        // Update worker load (simulate job assignment)\n        self.load_balancer.update_load(worker_id, 1.0);\n\n        // Publish job scheduled event if event bus is available\n        if let Some(event_bus) = \u0026self.event_bus {\n            let event = JobScheduledEvent {\n                job_id: job_id.clone(),\n                worker_id,\n                correlation_id: correlation_id.clone(),\n                tenant_id: tenant_id.clone(),\n            };\n\n            if let Err(e) = event_bus\n                .publish(\n                    \"jobs.scheduled\",\n                    \u0026event,\n                    correlation_id.clone(),\n                    tenant_id.clone(),\n                )\n                .await\n            {\n                tracing::warn!(\"Failed to publish job scheduled event: {}\", e);\n            }\n        }\n\n        tracing::info!(\"Scheduled job {} to worker {}\", job_id.as_uuid(), worker_id);\n\n        Ok(worker_id)\n    }\n\n    /// Handle worker heartbeat\n    pub fn handle_heartbeat(\u0026self, worker_id: Uuid) {\n        self.failure_detector.record_heartbeat(worker_id);\n        self.load_balancer.update_load(worker_id, 0.5); // Active but not at full capacity\n    }\n\n    /// Handle worker failure\n    pub fn handle_worker_failure(\u0026self, worker_id: Uuid) {\n        self.load_balancer.mark_unhealthy(worker_id);\n        self.failure_detector.remove_worker(worker_id);\n        tracing::warn!(\"Worker {} marked as unhealthy\", worker_id);\n    }\n\n    /// Check for failed workers\n    pub fn check_failures(\u0026self) -\u003e Vec\u003cUuid\u003e {\n        let failed_workers = self.failure_detector.detect_failures();\n        for worker_id in \u0026failed_workers {\n            self.handle_worker_failure(*worker_id);\n        }\n        failed_workers\n    }\n}\n\n/// Event for job scheduled\n#[derive(Debug, serde::Serialize, serde::Deserialize)]\npub struct JobScheduledEvent {\n    pub job_id: JobId,\n    pub worker_id: Uuid,\n    pub correlation_id: CorrelationId,\n    pub tenant_id: TenantId,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use hodei_shared_types::job_definitions::ResourceQuota;\n    use std::collections::HashMap;\n\n    #[test]\n    fn test_load_balancer_worker_selection() {\n        let balancer = LoadBalancer::new();\n\n        // Add workers with different loads\n        balancer.add_worker(WorkerInfo {\n            id: Uuid::new_v4(),\n            load: 0.5,\n            capabilities: vec![\"default\".to_string()],\n            is_healthy: true,\n        });\n\n        let worker_id = balancer.select_worker(\u0026[\"default\".to_string()]);\n        assert!(worker_id.is_some());\n    }\n\n    #[test]\n    fn test_failure_detection() {\n        let detector = FailureDetector::new(std::time::Duration::from_millis(100));\n\n        let worker_id = Uuid::new_v4();\n        detector.record_heartbeat(worker_id);\n\n        // Should not detect failure immediately\n        assert_eq!(detector.detect_failures().len(), 0);\n\n        std::thread::sleep(std::time::Duration::from_millis(150));\n\n        // Should detect failure after timeout\n        let failures = detector.detect_failures();\n        assert_eq!(failures.len(), 1);\n        assert_eq!(failures[0], worker_id);\n    }\n\n    #[test]\n    fn test_coordinator_job_scheduling() {\n        let coordinator = JobCoordinator::new();\n\n        // Register a worker\n        coordinator.register_worker(WorkerInfo {\n            id: Uuid::new_v4(),\n            load: 0.0,\n            capabilities: vec![\"default\".to_string()],\n            is_healthy: true,\n        });\n\n        let spec = JobSpec {\n            name: \"test-job\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        };\n\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test\".to_string());\n\n        // This would be async in real usage\n        // but we're just testing the coordination logic here\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","error_handling.rs"],"content":"//! Error handling for distributed communication\n\n#[derive(Debug, thiserror::Error)]\npub enum DistributedError {\n    #[error(\"connection error: {0}\")]\n    Connection(String),\n\n    #[error(\"publish error: {0}\")]\n    Publish(String),\n\n    #[error(\"subscribe error: {0}\")]\n    Subscribe(String),\n\n    #[error(\"timeout: {0}\")]\n    Timeout(String),\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","lib.rs"],"content":"//! Distributed communication module\n\npub mod error_handling;\npub mod nats_adapter;\npub mod tests;\n\npub use error_handling::DistributedError;\npub use nats_adapter::{EventBus, NatsClient, NatsError, NatsEventBus, TopicManager};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","nats_adapter.rs"],"content":"//! NATS JetStream adapter for distributed communication (Mock Implementation)\n\nuse async_trait::async_trait;\nuse hodei_shared_types::{CorrelationId, TenantId};\nuse serde::{Deserialize, Serialize};\nuse std::sync::Arc;\n\n/// Mock NATS connection wrapper\npub struct NatsClient {\n    _phantom: std::marker::PhantomData\u003c()\u003e,\n}\n\nimpl NatsClient {\n    pub fn new() -\u003e Self {\n        Self {\n            _phantom: std::marker::PhantomData,\n        }\n    }\n\n    /// Publish a message (mock)\n    pub async fn publish(\u0026self, _subject: \u0026str, _data: \u0026[u8]) -\u003e Result\u003c(), NatsError\u003e {\n        tracing::debug!(\"Mock publish to subject\");\n        Ok(())\n    }\n\n    /// Subscribe to a subject (mock)\n    pub fn subscribe(\n        \u0026self,\n        _subject: \u0026str,\n    ) -\u003e Result\u003cstd::sync::mpsc::Receiver\u003cVec\u003cu8\u003e\u003e, NatsError\u003e {\n        let (_tx, rx) = std::sync::mpsc::channel();\n        tracing::debug!(\"Mock subscribe to subject\");\n        // Return a mock receiver\n        Ok(rx)\n    }\n}\n\n/// NATS error types\n#[derive(Debug, thiserror::Error)]\npub enum NatsError {\n    #[error(\"publish error: {0}\")]\n    PublishError(String),\n\n    #[error(\"subscribe error: {0}\")]\n    SubscribeError(String),\n\n    #[error(\"connection error: {0}\")]\n    ConnectionError(String),\n}\n\n/// Message types for distributed communication\n#[derive(Debug, Serialize, Deserialize)]\npub struct Message {\n    pub subject: String,\n    pub payload: Vec\u003cu8\u003e,\n    pub correlation_id: CorrelationId,\n    pub tenant_id: TenantId,\n}\n\n/// NATS topics manager\npub struct TopicManager {\n    nats_client: Arc\u003cNatsClient\u003e,\n}\n\nimpl TopicManager {\n    pub fn new(nats_client: Arc\u003cNatsClient\u003e) -\u003e Self {\n        Self { nats_client }\n    }\n\n    /// Create a JetStream stream (mock)\n    pub async fn create_stream(\u0026self, name: \u0026str, subjects: \u0026[\u0026str]) -\u003e Result\u003c(), NatsError\u003e {\n        tracing::info!(\n            \"Mock: Creating stream {} with subjects {:?}\",\n            name,\n            subjects\n        );\n        Ok(())\n    }\n\n    /// Publish event to topic\n    pub async fn publish_event\u003cT: Serialize\u003e(\n        \u0026self,\n        topic: \u0026str,\n        event: \u0026T,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003c(), NatsError\u003e {\n        let payload =\n            bincode::serialize(event).map_err(|e| NatsError::PublishError(e.to_string()))?;\n\n        let message = Message {\n            subject: topic.to_string(),\n            payload,\n            correlation_id,\n            tenant_id,\n        };\n\n        let message_bytes =\n            bincode::serialize(\u0026message).map_err(|e| NatsError::PublishError(e.to_string()))?;\n\n        self.nats_client.publish(topic, \u0026message_bytes).await?;\n        Ok(())\n    }\n}\n\n/// EventBus trait for publishing events\n#[async_trait]\npub trait EventBus: Send + Sync {\n    async fn publish\u003cT: Serialize + Sync\u003e(\n        \u0026self,\n        topic: \u0026str,\n        event: \u0026T,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003c(), NatsError\u003e;\n}\n\n/// NATS EventBus implementation\npub struct NatsEventBus {\n    topic_manager: Arc\u003cTopicManager\u003e,\n}\n\nimpl NatsEventBus {\n    pub fn new(topic_manager: Arc\u003cTopicManager\u003e) -\u003e Self {\n        Self { topic_manager }\n    }\n}\n\n#[async_trait]\nimpl EventBus for NatsEventBus {\n    async fn publish\u003cT: Serialize + Sync\u003e(\n        \u0026self,\n        topic: \u0026str,\n        event: \u0026T,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003c(), NatsError\u003e {\n        self.topic_manager\n            .publish_event(topic, event, correlation_id, tenant_id)\n            .await\n    }\n}\n","traces":[{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":10},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","distributed-comm","src","tests.rs"],"content":"#[cfg(test)]\nmod tests {\n    use crate::{EventBus, NatsClient, NatsEventBus, TopicManager};\n    use hodei_shared_types::{CorrelationId, TenantId};\n    use std::sync::Arc;\n\n    #[tokio::test]\n    async fn test_nats_client_creation() {\n        let client = NatsClient::new();\n        assert!(client.publish(\"test.subject\", b\"test data\").await.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_event_publishing() {\n        let client = Arc::new(NatsClient::new());\n        let topic_manager = Arc::new(TopicManager::new(client));\n        let event_bus = Arc::new(NatsEventBus::new(topic_manager));\n\n        let test_event = \"test event\";\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n\n        let result = event_bus\n            .publish(\"test.topic\", \u0026test_event, correlation_id, tenant_id)\n            .await;\n\n        assert!(result.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_stream_creation() {\n        let client = Arc::new(NatsClient::new());\n        let topic_manager = Arc::new(TopicManager::new(client));\n\n        let result = topic_manager\n            .create_stream(\"test-stream\", \u0026[\"test.subject.*\"])\n            .await;\n\n        assert!(result.is_ok());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","monitoring","src","lib.rs"],"content":"//! Performance benchmarking and monitoring (US-006)\n//!\n//! This module provides comprehensive performance monitoring including:\n//! - Metrics collection (counters, gauges, histograms)\n//! - Benchmark runner with automated metrics\n//! - Prometheus metrics exporter\n//! - SLA monitoring and alerting\n\nuse hodei_shared_types::{CorrelationId, JobId, TenantId, Uuid};\nuse parking_lot::Mutex;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\n\n/// Performance metrics collector\n#[derive(Debug, Clone)]\npub struct MetricsCollector {\n    pub metrics: HashMap\u003cString, MetricValue\u003e,\n    start_time: Instant,\n}\n\n#[derive(Debug, Clone)]\npub struct MetricValue {\n    pub name: String,\n    pub value: f64,\n    pub metric_type: MetricType,\n    pub labels: HashMap\u003cString, String\u003e,\n    pub timestamp: Instant,\n}\n\n#[derive(Debug, Clone)]\npub enum MetricType {\n    Counter,\n    Gauge,\n    Histogram,\n}\n\nimpl MetricsCollector {\n    pub fn new() -\u003e Self {\n        Self {\n            metrics: HashMap::new(),\n            start_time: Instant::now(),\n        }\n    }\n\n    /// Record a counter metric\n    pub fn increment_counter(\n        \u0026mut self,\n        name: \u0026str,\n        value: f64,\n        labels: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    ) {\n        let labels = labels.unwrap_or_default();\n        let key = format!(\"{}_{:?}\", name, labels);\n\n        if let Some(existing) = self.metrics.get_mut(\u0026key) {\n            existing.value += value;\n            existing.timestamp = Instant::now();\n        } else {\n            self.metrics.insert(\n                key,\n                MetricValue {\n                    name: name.to_string(),\n                    value,\n                    metric_type: MetricType::Counter,\n                    labels,\n                    timestamp: Instant::now(),\n                },\n            );\n        }\n    }\n\n    /// Record a gauge metric\n    pub fn set_gauge(\u0026mut self, name: \u0026str, value: f64, labels: Option\u003cHashMap\u003cString, String\u003e\u003e) {\n        let labels = labels.unwrap_or_default();\n        let key = format!(\"{}_{:?}\", name, labels);\n\n        self.metrics.insert(\n            key,\n            MetricValue {\n                name: name.to_string(),\n                value,\n                metric_type: MetricType::Gauge,\n                labels,\n                timestamp: Instant::now(),\n            },\n        );\n    }\n\n    /// Record a histogram metric\n    pub fn record_histogram(\n        \u0026mut self,\n        name: \u0026str,\n        value: f64,\n        labels: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    ) {\n        let labels = labels.unwrap_or_default();\n        let key = format!(\"{}_{:?}\", name, labels);\n\n        self.metrics.insert(\n            key,\n            MetricValue {\n                name: name.to_string(),\n                value,\n                metric_type: MetricType::Histogram,\n                labels,\n                timestamp: Instant::now(),\n            },\n        );\n    }\n\n    /// Get all metrics\n    pub fn get_metrics(\u0026self) -\u003e Vec\u003c\u0026MetricValue\u003e {\n        self.metrics.values().collect()\n    }\n\n    /// Get uptime\n    pub fn uptime(\u0026self) -\u003e Duration {\n        self.start_time.elapsed()\n    }\n\n    /// Clear all metrics\n    pub fn clear(\u0026mut self) {\n        self.metrics.clear();\n    }\n}\n\n/// Benchmark runner\npub struct BenchmarkRunner {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n}\n\nimpl BenchmarkRunner {\n    pub fn new() -\u003e Self {\n        Self {\n            metrics: Arc::new(Mutex::new(MetricsCollector::new())),\n        }\n    }\n\n    /// Run a benchmark and record results\n    pub async fn run_benchmark\u003cF, Fut\u003e(\u0026self, name: \u0026str, operation: F) -\u003e BenchmarkResult\n    where\n        F: FnOnce() -\u003e Fut,\n        Fut: std::future::Future,\n    {\n        let start = Instant::now();\n\n        operation().await;\n\n        let duration = start.elapsed();\n        let mut metrics = self.metrics.lock();\n\n        metrics.record_histogram(\n            \u0026format!(\"{}_duration_ms\", name),\n            duration.as_millis() as f64,\n            None,\n        );\n\n        BenchmarkResult {\n            name: name.to_string(),\n            duration,\n            success: true,\n        }\n    }\n\n    /// Get reference to metrics collector\n    pub fn metrics(\u0026self) -\u003e \u0026Arc\u003cMutex\u003cMetricsCollector\u003e\u003e {\n        \u0026self.metrics\n    }\n}\n\n/// Prometheus metrics exporter\npub struct PrometheusExporter {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n}\n\nimpl PrometheusExporter {\n    pub fn new(metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e) -\u003e Self {\n        Self { metrics }\n    }\n\n    /// Export metrics in Prometheus format\n    pub fn export(\u0026self) -\u003e String {\n        let metrics = self.metrics.lock();\n        let mut output = String::new();\n\n        for metric in metrics.get_metrics() {\n            let labels: Vec\u003cString\u003e = metric\n                .labels\n                .iter()\n                .map(|(k, v)| format!(\"{}=\\\"{}\\\"\", k, v))\n                .collect();\n\n            let labels_str = if !labels.is_empty() {\n                format!(\"{{{}}}\", labels.join(\",\"))\n            } else {\n                String::new()\n            };\n\n            output.push_str(\u0026format!(\n                \"# TYPE {} {}\\n{}{} {}\\n\\n\",\n                metric.name,\n                format!(\"{:?}\", metric.metric_type).to_lowercase(),\n                metric.name,\n                labels_str,\n                metric.value\n            ));\n        }\n\n        output\n    }\n}\n\n/// SLA monitor for tracking service level objectives\npub struct SLAMonitor {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n    sla_thresholds: HashMap\u003cString, Threshold\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct Threshold {\n    pub target_value: f64,\n    pub comparison: Comparison,\n}\n\n#[derive(Debug, Clone)]\npub enum Comparison {\n    LessThan,\n    GreaterThan,\n    Equal,\n}\n\nimpl SLAMonitor {\n    pub fn new() -\u003e Self {\n        let mut sla_thresholds = HashMap::new();\n\n        // Define default SLAs\n        sla_thresholds.insert(\n            \"job_scheduling_latency_ms\".to_string(),\n            Threshold {\n                target_value: 50.0, // \u003c 50ms\n                comparison: Comparison::LessThan,\n            },\n        );\n\n        sla_thresholds.insert(\n            \"system_uptime_percent\".to_string(),\n            Threshold {\n                target_value: 99.9, // \u003e 99.9%\n                comparison: Comparison::GreaterThan,\n            },\n        );\n\n        Self {\n            metrics: Arc::new(Mutex::new(MetricsCollector::new())),\n            sla_thresholds,\n        }\n    }\n\n    /// Check if SLA is being met\n    pub fn check_sla(\u0026self, metric_name: \u0026str) -\u003e SLACheckResult {\n        let metrics = self.metrics.lock();\n\n        if let Some(threshold) = self.sla_thresholds.get(metric_name) {\n            // Find metric by name (keys may have labels suffix)\n            for metric in metrics.metrics.values() {\n                if metric.name == metric_name {\n                    let meets_sla = match threshold.comparison {\n                        Comparison::LessThan =\u003e metric.value \u003c= threshold.target_value,\n                        Comparison::GreaterThan =\u003e metric.value \u003e= threshold.target_value,\n                        Comparison::Equal =\u003e metric.value == threshold.target_value,\n                    };\n\n                    return SLACheckResult {\n                        metric_name: metric_name.to_string(),\n                        current_value: metric.value,\n                        threshold: threshold.target_value,\n                        meets_sla,\n                    };\n                }\n            }\n        }\n\n        SLACheckResult {\n            metric_name: metric_name.to_string(),\n            current_value: 0.0,\n            threshold: 0.0,\n            meets_sla: false,\n        }\n    }\n\n    /// Record job scheduling latency\n    pub fn record_job_scheduling_latency(\u0026self, latency_ms: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.record_histogram(\"job_scheduling_latency_ms\", latency_ms, None);\n    }\n\n    /// Record worker utilization\n    pub fn record_worker_utilization(\u0026self, worker_id: Uuid, utilization: f64) {\n        let mut metrics = self.metrics.lock();\n        let mut labels = HashMap::new();\n        labels.insert(\"worker_id\".to_string(), worker_id.to_string());\n        metrics.set_gauge(\"worker_utilization_percent\", utilization, Some(labels));\n    }\n\n    /// Record job throughput\n    pub fn record_job_throughput(\u0026self, jobs_per_minute: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.increment_counter(\"job_throughput_total\", jobs_per_minute, None);\n    }\n\n    /// Record system health\n    pub fn record_system_health(\u0026self, health_score: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.set_gauge(\"system_health_score\", health_score, None);\n    }\n\n    /// Get metrics collector reference\n    pub fn metrics(\u0026self) -\u003e \u0026Arc\u003cMutex\u003cMetricsCollector\u003e\u003e {\n        \u0026self.metrics\n    }\n}\n\n#[derive(Debug)]\npub struct SLACheckResult {\n    pub metric_name: String,\n    pub current_value: f64,\n    pub threshold: f64,\n    pub meets_sla: bool,\n}\n\n#[derive(Debug)]\npub struct BenchmarkResult {\n    pub name: String,\n    pub duration: Duration,\n    pub success: bool,\n}\n\n/// Performance profiler for detailed analysis\npub struct PerformanceProfiler {\n    metrics: Arc\u003cMutex\u003cMetricsCollector\u003e\u003e,\n    profiling_enabled: bool,\n}\n\nimpl PerformanceProfiler {\n    pub fn new() -\u003e Self {\n        Self {\n            metrics: Arc::new(Mutex::new(MetricsCollector::new())),\n            profiling_enabled: true,\n        }\n    }\n\n    /// Profile a function call\n    pub async fn profile\u003cF, Fut, T\u003e(\u0026self, name: \u0026str, operation: F) -\u003e T\n    where\n        F: FnOnce() -\u003e Fut,\n        Fut: std::future::Future\u003cOutput = T\u003e,\n    {\n        if !self.profiling_enabled {\n            return operation().await;\n        }\n\n        let start = Instant::now();\n        let result = operation().await;\n        let duration = start.elapsed();\n\n        let mut metrics = self.metrics.lock();\n        metrics.record_histogram(\n            \u0026format!(\"{}_profiled_duration_ms\", name),\n            duration.as_millis() as f64,\n            None,\n        );\n\n        result\n    }\n\n    /// Record memory usage\n    pub fn record_memory_usage(\u0026self, bytes: usize) {\n        let mut metrics = self.metrics.lock();\n        metrics.set_gauge(\"memory_usage_bytes\", bytes as f64, None);\n    }\n\n    /// Record CPU utilization\n    pub fn record_cpu_utilization(\u0026self, percentage: f64) {\n        let mut metrics = self.metrics.lock();\n        metrics.set_gauge(\"cpu_utilization_percent\", percentage, None);\n    }\n\n    /// Enable/disable profiling\n    pub fn set_profiling_enabled(\u0026self, _enabled: bool) {\n        // This would need interior mutability in a real implementation\n        // Simplified for example purposes\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_metrics_collection() {\n        let mut collector = MetricsCollector::new();\n\n        collector.increment_counter(\"requests_total\", 1.0, None);\n        collector.set_gauge(\"memory_usage_mb\", 512.0, None);\n        collector.record_histogram(\"response_time_ms\", 45.0, None);\n\n        let metrics = collector.get_metrics();\n        assert_eq!(metrics.len(), 3);\n\n        // Small delay to ensure uptime is measurable\n        std::thread::sleep(Duration::from_millis(10));\n\n        // Check uptime is set\n        assert!(collector.uptime().as_millis() \u003e 0);\n    }\n\n    #[test]\n    fn test_sla_monitoring() {\n        let monitor = SLAMonitor::new();\n\n        // Record good latency\n        monitor.record_job_scheduling_latency(30.0);\n\n        let result = monitor.check_sla(\"job_scheduling_latency_ms\");\n        assert!(result.meets_sla);\n        assert_eq!(result.current_value, 30.0);\n\n        // Record bad latency\n        monitor.record_job_scheduling_latency(100.0);\n\n        let result = monitor.check_sla(\"job_scheduling_latency_ms\");\n        assert!(!result.meets_sla);\n    }\n\n    #[test]\n    fn test_prometheus_export() {\n        let metrics = Arc::new(Mutex::new(MetricsCollector::new()));\n        {\n            let mut collector = metrics.lock();\n            collector.increment_counter(\"test_counter\", 5.0, None);\n            collector.set_gauge(\"test_gauge\", 42.0, None);\n        }\n\n        let exporter = PrometheusExporter::new(metrics);\n        let output = exporter.export();\n\n        assert!(output.contains(\"test_counter\"));\n        assert!(output.contains(\"test_gauge\"));\n        assert!(output.contains(\"counter\"));\n        assert!(output.contains(\"gauge\"));\n    }\n\n    #[tokio::test]\n    async fn test_benchmark_runner() {\n        let runner = BenchmarkRunner::new();\n\n        let result = runner\n            .run_benchmark(\"test_operation\", || async {\n                tokio::time::sleep(Duration::from_millis(10)).await;\n            })\n            .await;\n\n        assert!(result.success);\n        assert!(result.duration.as_millis() \u003e= 10);\n\n        let metrics = runner.metrics().lock();\n        assert!(metrics.get_metrics().len() \u003e 0);\n    }\n}\n","traces":[{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":360,"address":[],"length":0,"stats":{"Line":0}},{"line":361,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":0}},{"line":370,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":20},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","application","job_use_cases.rs"],"content":"//! Application layer - Use cases and Application Services\n//!\n//! This layer orchestrates domain entities and provides application-specific logic.\n\nuse crate::domain::{Job, JobEvent};\nuse async_trait::async_trait;\nuse hodei_shared_types::{CorrelationId, DomainError, JobId, JobSpec, TenantId};\n\n/// Data Transfer Objects\npub mod dtos {\n    use serde::{Deserialize, Serialize};\n\n    #[derive(Debug, Serialize, Deserialize)]\n    pub struct CreateJobRequest {\n        pub spec: hodei_shared_types::JobSpec,\n        pub correlation_id: Option\u003chodei_shared_types::CorrelationId\u003e,\n        pub tenant_id: hodei_shared_types::TenantId,\n    }\n\n    #[derive(Debug, Serialize, Deserialize)]\n    pub struct CreateJobResponse {\n        pub job_id: hodei_shared_types::JobId,\n    }\n\n    #[derive(Debug, Serialize, Deserialize)]\n    pub struct JobStatusResponse {\n        pub job_id: hodei_shared_types::JobId,\n        pub state: String,\n        pub attempts: u8,\n        pub created_at: Option\u003cString\u003e,\n        pub started_at: Option\u003cString\u003e,\n        pub completed_at: Option\u003cString\u003e,\n    }\n}\n\n/// Job Use Cases\n#[async_trait]\npub trait JobUseCases {\n    async fn create_job(\n        \u0026self,\n        request: dtos::CreateJobRequest,\n    ) -\u003e Result\u003cdtos::CreateJobResponse, DomainError\u003e;\n\n    async fn get_job_status(\u0026self, job_id: JobId) -\u003e Result\u003cdtos::JobStatusResponse, DomainError\u003e;\n\n    async fn cancel_job(\u0026self, job_id: JobId) -\u003e Result\u003c(), DomainError\u003e;\n}\n\n/// In-memory Job Repository (for testing)\npub struct InMemoryJobRepository {\n    jobs: std::sync::Arc\u003cstd::sync::Mutex\u003cstd::collections::HashMap\u003cJobId, Job\u003e\u003e\u003e,\n}\n\nimpl InMemoryJobRepository {\n    pub fn new() -\u003e Self {\n        Self {\n            jobs: std::sync::Arc::new(std::sync::Mutex::new(std::collections::HashMap::new())),\n        }\n    }\n\n    pub async fn save(\u0026self, job: Job) -\u003e Result\u003c(), DomainError\u003e {\n        let mut jobs = self.jobs.lock().unwrap();\n        jobs.insert(job.id.clone(), job);\n        Ok(())\n    }\n\n    pub async fn find(\u0026self, job_id: \u0026JobId) -\u003e Result\u003cOption\u003cJob\u003e, DomainError\u003e {\n        let jobs = self.jobs.lock().unwrap();\n        Ok(jobs.get(job_id).cloned())\n    }\n}\n\n/// Job Application Service\npub struct JobApplicationService {\n    repository: InMemoryJobRepository,\n}\n\nimpl JobApplicationService {\n    pub fn new(repository: InMemoryJobRepository) -\u003e Self {\n        Self { repository }\n    }\n}\n\n#[async_trait]\nimpl JobUseCases for JobApplicationService {\n    async fn create_job(\n        \u0026self,\n        request: dtos::CreateJobRequest,\n    ) -\u003e Result\u003cdtos::CreateJobResponse, DomainError\u003e {\n        let correlation_id = request.correlation_id.unwrap_or_default();\n\n        // Use domain service to create job\n        let job = Job::create(request.spec, correlation_id, request.tenant_id)?;\n\n        // Save to repository\n        self.repository.save(job.clone()).await?;\n\n        Ok(dtos::CreateJobResponse { job_id: job.id })\n    }\n\n    async fn get_job_status(\u0026self, job_id: JobId) -\u003e Result\u003cdtos::JobStatusResponse, DomainError\u003e {\n        let job =\n            self.repository.find(\u0026job_id).await?.ok_or_else(|| {\n                DomainError::NotFound(format!(\"job {} not found\", job_id.as_uuid()))\n            })?;\n\n        Ok(dtos::JobStatusResponse {\n            job_id: job.id,\n            state: job.state.as_str().to_string(),\n            attempts: job.attempts,\n            created_at: job.created_at.map(|t| t.to_rfc3339()),\n            started_at: job.started_at.map(|t| t.to_rfc3339()),\n            completed_at: job.completed_at.map(|t| t.to_rfc3339()),\n        })\n    }\n\n    async fn cancel_job(\u0026self, job_id: JobId) -\u003e Result\u003c(), DomainError\u003e {\n        let mut job =\n            self.repository.find(\u0026job_id).await?.ok_or_else(|| {\n                DomainError::NotFound(format!(\"job {} not found\", job_id.as_uuid()))\n            })?;\n\n        job.cancel()?;\n\n        self.repository.save(job).await?;\n        Ok(())\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","application","mod.rs"],"content":"//! Application layer module\n\npub mod job_use_cases;\n\npub use job_use_cases::{dtos, JobApplicationService, JobUseCases};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","domain","job_entity.rs"],"content":"//! Job Entity implementation\n//!\n//! This module contains the Job aggregate root with state management\n//! and business logic according to DDD principles.\n\nuse hodei_shared_types::job_definitions::{ExecResult, JobId, JobSpec, JobState};\nuse hodei_shared_types::{CorrelationId, DateTime, DomainError, TenantId, Utc};\n\n/// Job aggregate root\n#[derive(Debug, Clone)]\npub struct Job {\n    pub id: JobId,\n    pub spec: JobSpec,\n    pub state: JobState,\n    pub attempts: u8,\n    pub created_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub started_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub completed_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    pub correlation_id: CorrelationId,\n    pub tenant_id: TenantId,\n}\n\n#[derive(Debug)]\npub enum JobEvent {\n    JobRequested {\n        job_id: JobId,\n        spec: JobSpec,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    },\n    JobScheduled {\n        job_id: JobId,\n    },\n    JobStarted {\n        job_id: JobId,\n    },\n    JobCompleted {\n        job_id: JobId,\n        result: ExecResult,\n    },\n    JobFailed {\n        job_id: JobId,\n        error: String,\n        retryable: bool,\n    },\n    JobCancelled {\n        job_id: JobId,\n    },\n}\n\nimpl Job {\n    /// Create a new Job with PENDING state\n    pub fn create(\n        spec: JobSpec,\n        correlation_id: CorrelationId,\n        tenant_id: TenantId,\n    ) -\u003e Result\u003cSelf, DomainError\u003e {\n        // Validate spec before creation\n        spec.validate()?;\n\n        Ok(Self {\n            id: JobId::new(),\n            spec,\n            state: JobState::from(JobState::PENDING.to_string()),\n            attempts: 1,\n            created_at: Some(Utc::now()),\n            started_at: None,\n            completed_at: None,\n            correlation_id,\n            tenant_id,\n        })\n    }\n\n    /// Transition job to SCHEDULED state\n    pub fn schedule(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::SCHEDULED.to_string()))?;\n        self.state = JobState::from(JobState::SCHEDULED.to_string());\n\n        Ok(vec![JobEvent::JobScheduled {\n            job_id: self.id.clone(),\n        }])\n    }\n\n    /// Transition job to RUNNING state\n    pub fn start(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::RUNNING.to_string()))?;\n        self.state = JobState::from(JobState::RUNNING.to_string());\n        self.started_at = Some(Utc::now());\n\n        Ok(vec![JobEvent::JobStarted {\n            job_id: self.id.clone(),\n        }])\n    }\n\n    /// Transition job to SUCCESS state\n    pub fn complete(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::SUCCESS.to_string()))?;\n        self.state = JobState::from(JobState::SUCCESS.to_string());\n        self.completed_at = Some(Utc::now());\n\n        let result = ExecResult {\n            exit_code: 0,\n            stdout: None,\n            stderr: None,\n        };\n\n        Ok(vec![JobEvent::JobCompleted {\n            job_id: self.id.clone(),\n            result,\n        }])\n    }\n\n    /// Transition job to FAILED state\n    pub fn fail(\u0026mut self, error: String, retryable: bool) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::FAILED.to_string()))?;\n        self.state = JobState::from(JobState::FAILED.to_string());\n        self.completed_at = Some(Utc::now());\n\n        Ok(vec![JobEvent::JobFailed {\n            job_id: self.id.clone(),\n            error,\n            retryable,\n        }])\n    }\n\n    /// Transition job to CANCELLED state\n    pub fn cancel(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(\u0026JobState::from(JobState::CANCELLED.to_string()))?;\n        self.state = JobState::from(JobState::CANCELLED.to_string());\n\n        Ok(vec![JobEvent::JobCancelled {\n            job_id: self.id.clone(),\n        }])\n    }\n\n    /// Retry a failed job (transition to PENDING)\n    pub fn retry(\u0026mut self) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        // Check max retries\n        if self.attempts \u003e= self.spec.retries {\n            return Err(DomainError::Validation(format!(\n                \"max retries ({}) exceeded\",\n                self.spec.retries\n            )));\n        }\n\n        self.ensure_transition_valid(\u0026JobState::from(JobState::PENDING.to_string()))?;\n        self.state = JobState::from(JobState::PENDING.to_string());\n        self.attempts += 1;\n\n        Ok(vec![JobEvent::JobRequested {\n            job_id: self.id.clone(),\n            spec: self.spec.clone(),\n            correlation_id: self.correlation_id.clone(),\n            tenant_id: self.tenant_id.clone(),\n        }])\n    }\n\n    /// Manual state transition with validation\n    pub fn transition_to(\u0026mut self, target_state: \u0026JobState) -\u003e Result\u003cVec\u003cJobEvent\u003e, DomainError\u003e {\n        self.ensure_transition_valid(target_state)?;\n        self.state = target_state.clone();\n\n        match target_state.as_str() {\n            JobState::RUNNING =\u003e self.started_at = Some(Utc::now()),\n            JobState::SUCCESS | JobState::FAILED | JobState::CANCELLED =\u003e {\n                self.completed_at = Some(Utc::now())\n            }\n            _ =\u003e {}\n        }\n\n        Ok(vec![])\n    }\n\n    /// Validate state transition\n    fn ensure_transition_valid(\u0026self, target: \u0026JobState) -\u003e Result\u003c(), DomainError\u003e {\n        if !self.state.can_transition_to(target) {\n            return Err(DomainError::invalid_state_transition(\n                self.state.as_str(),\n                target.as_str(),\n            ));\n        }\n        Ok(())\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","domain","mod.rs"],"content":"//! Domain layer - Core business logic\n//!\n//! This layer contains entities, value objects, and domain services.\n//! It is independent of infrastructure concerns.\n\npub mod job_entity;\n\npub use job_entity::{Job, JobEvent};\n\n/// Domain services\npub mod services {\n    use hodei_shared_types::DomainError;\n\n    /// Job validation service\n    pub struct JobValidator;\n\n    impl JobValidator {\n        pub fn validate_name(name: \u0026str) -\u003e Result\u003c(), DomainError\u003e {\n            if name.trim().is_empty() {\n                return Err(DomainError::Validation(\n                    \"job name cannot be empty\".to_string(),\n                ));\n            }\n            Ok(())\n        }\n\n        pub fn validate_timeout(timeout_ms: u64) -\u003e Result\u003c(), DomainError\u003e {\n            if timeout_ms == 0 {\n                return Err(DomainError::Validation(\n                    \"timeout must be greater than 0\".to_string(),\n                ));\n            }\n            Ok(())\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","src","lib.rs"],"content":"//! Orchestrator crate\n//!\n//! This crate provides orchestration services for the Hodei CI/CD system.\n\npub mod application;\npub mod domain;\n\npub use application::job_use_cases::{JobApplicationService, JobUseCases, dtos};\npub use domain::{Job, JobEvent};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","orchestrator","tests","unit","domain","test_job_entity.rs"],"content":"#[cfg(test)]\nmod tests {\n    use hodei_shared_types::job_definitions::{JobId, JobSpec, ResourceQuota};\n    use hodei_shared_types::{CorrelationId, DomainError, TenantId};\n    use std::collections::HashMap;\n\n    #[test]\n    fn test_job_creation_with_valid_spec() {\n        // Arrange\n        let spec = JobSpec {\n            name: \"test-job\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string(), \"hello\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        };\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n\n        // Act\n        let result = Job::create(spec, correlation_id, tenant_id);\n\n        // Assert\n        assert!(result.is_ok());\n        let job = result.unwrap();\n        assert_eq!(job.state.as_str(), JobState::PENDING);\n        assert!(job.created_at.is_some());\n    }\n\n    #[test]\n    fn test_job_creation_with_empty_name() {\n        // Arrange\n        let mut spec = JobSpec {\n            name: \"\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string(), \"hello\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        };\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n\n        // Act\n        let result = Job::create(spec, correlation_id, tenant_id);\n\n        // Assert\n        assert!(result.is_err());\n        if let Err(e) = result {\n            assert!(matches!(e, DomainError::Validation(_)));\n        }\n    }\n\n    #[test]\n    fn test_job_state_transition_from_pending_to_scheduled() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n\n        // Act\n        let events = job.schedule().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::SCHEDULED);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_state_transition_from_scheduled_to_running() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n\n        // Act\n        let events = job.start().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::RUNNING);\n        assert!(job.started_at.is_some());\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_state_transition_from_running_to_success() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n\n        // Act\n        let events = job.complete().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::SUCCESS);\n        assert!(job.completed_at.is_some());\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_state_transition_from_running_to_failed() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n\n        // Act\n        let events = job.fail(\"Command failed\".to_string(), true).unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::FAILED);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_invalid_state_transition_from_running_to_pending() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n\n        // Act\n        let result = job.transition_to(\u0026JobState::from(JobState::PENDING.to_string()));\n\n        // Assert\n        assert!(result.is_err());\n        if let Err(e) = result {\n            assert!(matches!(e, DomainError::InvalidStateTransition { .. }));\n        }\n    }\n\n    #[test]\n    fn test_job_cancellation_from_pending() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n\n        // Act\n        let events = job.cancel().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::CANCELLED);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_retry_from_failed() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n        job.fail(\"Command failed\".to_string(), true).unwrap();\n\n        // Act\n        let events = job.retry().unwrap();\n\n        // Assert\n        assert_eq!(job.state.as_str(), JobState::PENDING);\n        assert!(job.attempts \u003e 1);\n        assert!(!events.is_empty());\n    }\n\n    #[test]\n    fn test_job_with_max_retries() {\n        // Arrange\n        let spec = create_test_spec();\n        let correlation_id = CorrelationId::new();\n        let tenant_id = TenantId::new(\"test-tenant\".to_string());\n        let mut job = Job::create(spec.clone(), correlation_id.clone(), tenant_id.clone()).unwrap();\n        job.schedule().unwrap();\n        job.start().unwrap();\n        job.fail(\"Command failed\".to_string(), true).unwrap();\n        job.retry().unwrap();\n\n        let mut second_job = Job::create(spec, correlation_id, tenant_id).unwrap();\n        second_job.schedule().unwrap();\n        second_job.start().unwrap();\n        second_job.fail(\"Command failed\".to_string(), true).unwrap();\n\n        // Act\n        let events = second_job.retry();\n\n        // Assert - should fail due to max retries\n        assert!(events.is_err());\n    }\n\n    fn create_test_spec() -\u003e JobSpec {\n        JobSpec {\n            name: \"test-job\".to_string(),\n            image: \"ubuntu:latest\".to_string(),\n            command: vec![\"echo\".to_string(), \"hello\".to_string()],\n            resources: ResourceQuota::new(100, 512),\n            timeout_ms: 30000,\n            retries: 2,\n            env: HashMap::new(),\n            secret_refs: vec![],\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","orchestration","worker-lifecycle","src","lib.rs"],"content":"//! Worker Lifecycle Management (US-005)\n//!\n//! This module provides comprehensive worker lifecycle management including:\n//! - Worker state transitions (AVAILABLE, RUNNING, UNHEALTHY, DRAINING)\n//! - Health monitoring with heartbeat-based detection\n//! - Auto-recovery mechanisms\n//! - Capability matching for optimal job scheduling\n\nuse hodei_shared_types::{CorrelationId, TenantId, Uuid};\nuse parking_lot::Mutex;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::Duration;\nuse tokio::time::{Instant, interval};\n\n/// Worker state representing lifecycle stage\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum WorkerState {\n    /// Worker is available for job assignment\n    AVAILABLE,\n    /// Worker is currently executing jobs\n    RUNNING,\n    /// Worker is unhealthy and not accepting new jobs\n    UNHEALTHY,\n    /// Worker is draining and completing existing jobs\n    DRAINING,\n}\n\nimpl WorkerState {\n    pub fn as_str(\u0026self) -\u003e \u0026'static str {\n        match self {\n            WorkerState::AVAILABLE =\u003e \"AVAILABLE\",\n            WorkerState::RUNNING =\u003e \"RUNNING\",\n            WorkerState::UNHEALTHY =\u003e \"UNHEALTHY\",\n            WorkerState::DRAINING =\u003e \"DRAINING\",\n        }\n    }\n\n    pub fn can_accept_jobs(\u0026self) -\u003e bool {\n        matches!(self, WorkerState::AVAILABLE | WorkerState::RUNNING)\n    }\n\n    pub fn is_healthy(\u0026self) -\u003e bool {\n        matches!(self, WorkerState::AVAILABLE | WorkerState::RUNNING)\n    }\n}\n\n/// Worker capabilities (e.g., CPU type, GPU, special hardware)\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct WorkerCapabilities {\n    pub cpu_cores: u32,\n    pub memory_gb: u64,\n    pub has_gpu: bool,\n    pub gpu_count: Option\u003cu8\u003e,\n    pub specialized_hardware: Vec\u003cString\u003e,\n    pub container_runtime: String,\n}\n\nimpl Default for WorkerCapabilities {\n    fn default() -\u003e Self {\n        Self {\n            cpu_cores: 4,\n            memory_gb: 8,\n            has_gpu: false,\n            gpu_count: None,\n            specialized_hardware: vec![],\n            container_runtime: \"docker\".to_string(),\n        }\n    }\n}\n\n/// Worker information and state\n#[derive(Debug, Clone)]\npub struct Worker {\n    pub id: Uuid,\n    pub state: WorkerState,\n    pub capabilities: WorkerCapabilities,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub max_jobs: usize,\n    pub last_heartbeat: Instant,\n    pub failure_count: u32,\n    pub tenant_id: Option\u003cTenantId\u003e,\n}\n\nimpl Worker {\n    pub fn new(\n        id: Uuid,\n        capabilities: WorkerCapabilities,\n        max_jobs: usize,\n        tenant_id: Option\u003cTenantId\u003e,\n    ) -\u003e Self {\n        Self {\n            id,\n            state: WorkerState::AVAILABLE,\n            capabilities,\n            load: 0.0,\n            jobs_running: 0,\n            max_jobs,\n            last_heartbeat: Instant::now(),\n            failure_count: 0,\n            tenant_id,\n        }\n    }\n\n    pub fn can_accept_job(\u0026self) -\u003e bool {\n        self.state.can_accept_jobs() \u0026\u0026 self.jobs_running \u003c self.max_jobs \u0026\u0026 self.state.is_healthy()\n    }\n\n    pub fn update_heartbeat(\u0026mut self) {\n        self.last_heartbeat = Instant::now();\n        self.failure_count = 0;\n    }\n\n    pub fn record_failure(\u0026mut self) {\n        self.failure_count += 1;\n        if self.failure_count \u003e= 3 {\n            self.state = WorkerState::UNHEALTHY;\n        }\n    }\n}\n\n/// Worker Manager for lifecycle operations\npub struct WorkerManager {\n    workers: Arc\u003cMutex\u003cHashMap\u003cUuid, Worker\u003e\u003e\u003e,\n    coordinator: Option\u003cArc\u003chodei_coordinator::JobCoordinator\u003e\u003e,\n    health_check_interval: Duration,\n    heartbeat_timeout: Duration,\n}\n\nimpl WorkerManager {\n    pub fn new(\n        coordinator: Option\u003cArc\u003chodei_coordinator::JobCoordinator\u003e\u003e,\n        health_check_interval: Duration,\n        heartbeat_timeout: Duration,\n    ) -\u003e Self {\n        Self {\n            workers: Arc::new(Mutex::new(HashMap::new())),\n            coordinator,\n            health_check_interval,\n            heartbeat_timeout,\n        }\n    }\n\n    /// Register a new worker\n    pub fn register_worker(\u0026self, worker: Worker) -\u003e Result\u003c(), WorkerLifecycleError\u003e {\n        let worker_id = worker.id;\n        let mut workers = self.workers.lock();\n\n        if workers.contains_key(\u0026worker_id) {\n            return Err(WorkerLifecycleError::WorkerAlreadyExists);\n        }\n\n        workers.insert(worker_id, worker);\n        tracing::info!(\"Worker registered: {}\", worker_id);\n\n        Ok(())\n    }\n\n    /// Deregister a worker (graceful shutdown)\n    pub async fn deregister_worker(\u0026self, worker_id: Uuid) -\u003e Result\u003c(), WorkerLifecycleError\u003e {\n        let mut workers = self.workers.lock();\n\n        if let Some(mut worker) = workers.remove(\u0026worker_id) {\n            if worker.jobs_running \u003e 0 {\n                worker.state = WorkerState::DRAINING;\n                workers.insert(worker_id, worker);\n                tracing::info!(\"Worker {} entered draining state\", worker_id);\n\n                // Wait for jobs to complete (simplified - in real impl would track completion)\n                tokio::time::sleep(Duration::from_secs(30)).await;\n                workers.remove(\u0026worker_id);\n            }\n\n            tracing::info!(\"Worker deregistered: {}\", worker_id);\n        }\n\n        Ok(())\n    }\n\n    /// Process worker heartbeat\n    pub fn handle_heartbeat(\n        \u0026self,\n        worker_id: Uuid,\n        load: f64,\n        jobs_running: usize,\n    ) -\u003e Result\u003c(), WorkerLifecycleError\u003e {\n        let mut workers = self.workers.lock();\n\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.update_heartbeat();\n            worker.load = load;\n            worker.jobs_running = jobs_running;\n\n            if worker.state == WorkerState::UNHEALTHY \u0026\u0026 worker.failure_count \u003c 3 {\n                worker.state = WorkerState::AVAILABLE;\n                tracing::info!(\"Worker {} recovered from unhealthy state\", worker_id);\n            }\n\n            return Ok(());\n        }\n\n        Err(WorkerLifecycleError::WorkerNotFound)\n    }\n\n    /// Find available workers with matching capabilities\n    pub fn find_suitable_workers(\n        \u0026self,\n        required_cpus: u32,\n        required_memory_gb: u64,\n        needs_gpu: bool,\n        max_jobs_per_worker: usize,\n    ) -\u003e Vec\u003cUuid\u003e {\n        let workers = self.workers.lock();\n\n        let mut suitable_workers: Vec\u003c_\u003e = workers\n            .values()\n            .filter(|w| {\n                w.can_accept_job()\n                    \u0026\u0026 w.capabilities.cpu_cores \u003e= required_cpus\n                    \u0026\u0026 w.capabilities.memory_gb \u003e= required_memory_gb\n                    \u0026\u0026 (!needs_gpu || w.capabilities.has_gpu)\n                    \u0026\u0026 w.jobs_running \u003c max_jobs_per_worker\n            })\n            .cloned()\n            .collect();\n\n        // Sort by load (ascending) and job count (ascending)\n        suitable_workers.sort_by(|a, b| {\n            a.load\n                .partial_cmp(\u0026b.load)\n                .unwrap_or(std::cmp::Ordering::Equal)\n                .then_with(|| a.jobs_running.cmp(\u0026b.jobs_running))\n        });\n\n        suitable_workers.into_iter().map(|w| w.id).collect()\n    }\n\n    /// Mark worker as unhealthy\n    pub fn mark_unhealthy(\u0026self, worker_id: Uuid) {\n        let mut workers = self.workers.lock();\n        if let Some(worker) = workers.get_mut(\u0026worker_id) {\n            worker.state = WorkerState::UNHEALTHY;\n            worker.record_failure();\n            tracing::warn!(\n                \"Worker {} marked as unhealthy (failures: {})\",\n                worker_id,\n                worker.failure_count\n            );\n        }\n    }\n\n    /// Get worker status\n    pub fn get_worker_status(\n        \u0026self,\n        worker_id: Uuid,\n    ) -\u003e Result\u003cWorkerStatusResponse, WorkerLifecycleError\u003e {\n        let workers = self.workers.lock();\n\n        if let Some(worker) = workers.get(\u0026worker_id) {\n            Ok(WorkerStatusResponse {\n                worker_id: worker.id,\n                state: worker.state.as_str().to_string(),\n                load: worker.load,\n                jobs_running: worker.jobs_running,\n                max_jobs: worker.max_jobs,\n                last_heartbeat: Some(worker.last_heartbeat.elapsed().as_secs()),\n                capabilities: worker.capabilities.clone(),\n                failure_count: worker.failure_count,\n            })\n        } else {\n            Err(WorkerLifecycleError::WorkerNotFound)\n        }\n    }\n\n    /// Start health monitoring background task\n    pub fn start_health_monitoring(\u0026self) {\n        let workers = Arc::clone(\u0026self.workers);\n        let heartbeat_timeout = self.heartbeat_timeout;\n        let coordinator = self.coordinator.clone();\n\n        tokio::spawn(async move {\n            let mut interval = interval(Duration::from_secs(5));\n\n            loop {\n                interval.tick().await;\n\n                let mut failed_workers = vec![];\n                {\n                    let workers_guard = workers.lock();\n                    let now = Instant::now();\n\n                    for (worker_id, worker) in workers_guard.iter() {\n                        if now.duration_since(worker.last_heartbeat) \u003e heartbeat_timeout {\n                            failed_workers.push(*worker_id);\n                        }\n                    }\n                }\n\n                // Process failures outside the lock\n                for worker_id in failed_workers {\n                    tracing::warn!(\"Worker {} heartbeat timeout detected\", worker_id);\n\n                    // Mark as unhealthy\n                    {\n                        let mut workers_guard = workers.lock();\n                        if let Some(worker) = workers_guard.get_mut(\u0026worker_id) {\n                            worker.state = WorkerState::UNHEALTHY;\n                            worker.record_failure();\n                        }\n                    }\n\n                    // Notify coordinator if available\n                    if let Some(coordinator) = \u0026coordinator {\n                        coordinator.handle_worker_failure(worker_id);\n                    }\n                }\n            }\n        });\n    }\n\n    /// Get all workers summary\n    pub fn get_workers_summary(\u0026self) -\u003e Vec\u003cWorkerSummary\u003e {\n        let workers = self.workers.lock();\n\n        workers\n            .values()\n            .map(|w| WorkerSummary {\n                worker_id: w.id,\n                state: w.state.as_str().to_string(),\n                load: w.load,\n                jobs_running: w.jobs_running,\n                has_gpu: w.capabilities.has_gpu,\n            })\n            .collect()\n    }\n}\n\n/// Worker status response\n#[derive(Debug, Serialize, Deserialize)]\npub struct WorkerStatusResponse {\n    pub worker_id: Uuid,\n    pub state: String,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub max_jobs: usize,\n    pub last_heartbeat: Option\u003cu64\u003e,\n    pub capabilities: WorkerCapabilities,\n    pub failure_count: u32,\n}\n\n/// Worker summary for listing\n#[derive(Debug, Serialize, Deserialize)]\npub struct WorkerSummary {\n    pub worker_id: Uuid,\n    pub state: String,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub has_gpu: bool,\n}\n\n/// Error types\n#[derive(Debug, thiserror::Error)]\npub enum WorkerLifecycleError {\n    #[error(\"worker not found\")]\n    WorkerNotFound,\n\n    #[error(\"worker already exists\")]\n    WorkerAlreadyExists,\n\n    #[error(\"invalid worker state transition\")]\n    InvalidStateTransition,\n\n    #[error(\"worker is unhealthy\")]\n    WorkerUnhealthy,\n\n    #[error(\"worker at capacity\")]\n    WorkerAtCapacity,\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::time::Duration;\n\n    #[test]\n    fn test_worker_creation() {\n        let worker = Worker::new(Uuid::new_v4(), WorkerCapabilities::default(), 10, None);\n\n        assert_eq!(worker.state, WorkerState::AVAILABLE);\n        assert_eq!(worker.jobs_running, 0);\n        assert_eq!(worker.max_jobs, 10);\n        assert!(worker.can_accept_job());\n    }\n\n    #[test]\n    fn test_worker_state_transitions() {\n        let mut worker = Worker::new(Uuid::new_v4(), WorkerCapabilities::default(), 10, None);\n\n        // Start job\n        worker.jobs_running = 5;\n        assert!(worker.can_accept_job());\n\n        // Too many jobs\n        worker.jobs_running = 10;\n        assert!(!worker.can_accept_job());\n\n        // Unhealthy\n        worker.state = WorkerState::UNHEALTHY;\n        assert!(!worker.can_accept_job());\n    }\n\n    #[test]\n    fn test_capability_matching() {\n        let manager = WorkerManager::new(None, Duration::from_secs(30), Duration::from_secs(10));\n\n        // Register workers with different capabilities\n        manager\n            .register_worker(Worker::new(\n                Uuid::new_v4(),\n                WorkerCapabilities {\n                    cpu_cores: 8,\n                    memory_gb: 16,\n                    has_gpu: true,\n                    gpu_count: Some(1),\n                    specialized_hardware: vec![\"nvme\".to_string()],\n                    container_runtime: \"docker\".to_string(),\n                },\n                5,\n                None,\n            ))\n            .unwrap();\n\n        // Find suitable workers\n        let suitable = manager.find_suitable_workers(4, 8, true, 5);\n        assert_eq!(suitable.len(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_heartbeat_handling() {\n        let manager = WorkerManager::new(None, Duration::from_secs(30), Duration::from_secs(10));\n\n        let worker_id = Uuid::new_v4();\n        manager\n            .register_worker(Worker::new(\n                worker_id,\n                WorkerCapabilities::default(),\n                10,\n                None,\n            ))\n            .unwrap();\n\n        // Send heartbeat\n        assert!(manager.handle_heartbeat(worker_id, 0.5, 2).is_ok());\n\n        // Verify status\n        let status = manager.get_worker_status(worker_id).unwrap();\n        assert_eq!(status.state, \"AVAILABLE\");\n        assert_eq!(status.load, 0.5);\n        assert_eq!(status.jobs_running, 2);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","factory","mod.rs"],"content":"//! Provider Factory Module\n//!\n//! This module provides factory patterns for creating and managing worker providers.\n\nuse crate::models::{ProviderConfig, ProviderCredentials, ProviderType};\nuse crate::traits::WorkerProvider;\nuse crate::{MockWorkerProvider, ProviderError};\nuse std::collections::HashMap;\n\n/// Provider factory for creating and managing provider instances\npub struct ProviderFactory {\n    // For simplicity, we'll use an enum-based approach\n    // In production, this could be extended with a plugin system\n}\n\nimpl ProviderFactory {\n    pub fn new() -\u003e Result\u003cSelf, ProviderError\u003e {\n        Ok(Self {})\n    }\n\n    /// Create a provider instance\n    pub async fn create_provider(\n        \u0026self,\n        provider_type: ProviderType,\n        config: Option\u003cProviderConfig\u003e,\n    ) -\u003e Result\u003cBox\u003cdyn WorkerProvider\u003e, ProviderError\u003e {\n        let provider_config = config.ok_or_else(|| {\n            ProviderError::ConfigurationError(\"Provider configuration is required\".to_string())\n        })?;\n\n        match provider_type {\n            ProviderType::Kubernetes =\u003e {\n                Ok(Box::new(MockWorkerProvider::new(ProviderType::Kubernetes))\n                    as Box\u003cdyn WorkerProvider\u003e)\n            }\n            ProviderType::Docker =\u003e {\n                Ok(Box::new(MockWorkerProvider::new(ProviderType::Docker))\n                    as Box\u003cdyn WorkerProvider\u003e)\n            }\n            ProviderType::AwsEcs =\u003e Err(ProviderError::ConfigurationError(\n                \"AWS ECS provider not yet implemented\".to_string(),\n            )),\n            ProviderType::AzureContainerInstances =\u003e Err(ProviderError::ConfigurationError(\n                \"Azure provider not yet implemented\".to_string(),\n            )),\n            ProviderType::Custom(_) =\u003e Err(ProviderError::ConfigurationError(\n                \"Custom providers not yet implemented\".to_string(),\n            )),\n        }\n    }\n\n    /// Detect the best provider type based on environment\n    pub async fn detect_best_provider(\u0026self) -\u003e Result\u003cProviderType, ProviderError\u003e {\n        // For now, default to Kubernetes\n        // In production, this would detect available providers\n        Ok(ProviderType::Kubernetes)\n    }\n}\n","traces":[{"line":17,"address":[2284048],"length":1,"stats":{"Line":3}},{"line":18,"address":[2284051],"length":1,"stats":{"Line":3}},{"line":22,"address":[2284064],"length":1,"stats":{"Line":2}},{"line":27,"address":[2337402,2338672,2337484,2337560],"length":1,"stats":{"Line":5}},{"line":28,"address":[2338686],"length":1,"stats":{"Line":1}},{"line":31,"address":[2337663],"length":1,"stats":{"Line":1}},{"line":33,"address":[2337960,2337732],"length":1,"stats":{"Line":2}},{"line":37,"address":[2337776,2338038],"length":1,"stats":{"Line":0}},{"line":38,"address":[2338090],"length":1,"stats":{"Line":0}},{"line":40,"address":[2338092],"length":1,"stats":{"Line":0}},{"line":41,"address":[2337810],"length":1,"stats":{"Line":0}},{"line":43,"address":[2338221],"length":1,"stats":{"Line":0}},{"line":44,"address":[2337844],"length":1,"stats":{"Line":0}},{"line":46,"address":[2338350],"length":1,"stats":{"Line":0}},{"line":47,"address":[2337878],"length":1,"stats":{"Line":0}},{"line":53,"address":[2284184,2284176],"length":1,"stats":{"Line":0}},{"line":56,"address":[2338837],"length":1,"stats":{"Line":0}}],"covered":7,"coverable":17},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","lib.rs"],"content":"//! Worker Provider Abstraction Layer (US-018)\n//!\n//! This module provides a unified abstraction for worker providers\n//! supporting multiple execution backends (Kubernetes, Docker, AWS ECS, etc.).\n//!\n//! Features:\n//! - Provider abstraction with trait-based interfaces\n//! - Provider factory for automatic instantiation\n//! - Capability detection and feature matrix\n//! - Multi-provider coordination\n//! - Error handling with retry and circuit breaker patterns\n//! - Performance optimization with connection pooling\n\npub mod factory;\npub mod models;\npub mod traits;\n\npub use factory::ProviderFactory;\npub use models::*;\npub use traits::WorkerProvider;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::collections::HashMap;\n\n    #[tokio::test]\n    async fn test_provider_factory_creation() {\n        // RED: Test that we can create a provider factory\n        let factory = ProviderFactory::new();\n\n        // GREEN: Just check the factory was created\n        assert!(factory.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_provider_creation_without_config() {\n        // RED: Test that provider creation fails without configuration\n        let factory = ProviderFactory::new().unwrap();\n        let result = factory\n            .create_provider(ProviderType::Kubernetes, None)\n            .await;\n\n        // GREEN: Check it returns the expected error type\n        assert!(matches!(result, Err(ProviderError::ConfigurationError(_))));\n    }\n\n    #[tokio::test]\n    async fn test_provider_creation_with_config() {\n        // RED: Test that we can create a provider with configuration\n        let factory = ProviderFactory::new().unwrap();\n        let config = ProviderConfig {\n            provider_type: ProviderType::Kubernetes,\n            connection_string: \"test\".to_string(),\n            credentials: None,\n            options: HashMap::new(),\n        };\n\n        let result = factory\n            .create_provider(ProviderType::Kubernetes, Some(config))\n            .await;\n\n        // GREEN: Check provider was created successfully\n        assert!(result.is_ok());\n        let provider = result.unwrap();\n        assert_eq!(provider.get_provider_type(), ProviderType::Kubernetes);\n    }\n\n    #[tokio::test]\n    async fn test_mock_worker_provider_operations() {\n        // RED: Test basic worker operations on mock provider\n        let provider = MockWorkerProvider::new(ProviderType::Kubernetes);\n\n        // Create a test worker config\n        use hodei_shared_types::worker_messages::WorkerId;\n\n        let worker_id = WorkerId::new();\n        let worker_config = WorkerConfig {\n            worker_id: worker_id.clone(),\n            image: \"test-image:latest\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 1.0,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n\n        // Create worker\n        let handle = provider.create_worker(\u0026worker_config).await.unwrap();\n        assert_eq!(handle.worker_id, worker_id);\n\n        // Get status\n        let status = provider.get_worker_status(\u0026worker_id).await.unwrap();\n        assert_eq!(status.state, WorkerState::Creating);\n\n        // Stop worker\n        let result = provider.stop_worker(\u0026worker_id, true).await;\n        assert!(result.is_ok());\n\n        // Delete worker\n        let result = provider.delete_worker(\u0026worker_id).await;\n        assert!(result.is_ok());\n\n        // Verify worker is deleted\n        let status_result = provider.get_worker_status(\u0026worker_id).await;\n        assert!(status_result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_provider_capabilities() {\n        // RED: Test that provider capabilities can be retrieved\n        let provider = MockWorkerProvider::new(ProviderType::Kubernetes);\n\n        // GREEN: Check capabilities are returned\n        let capabilities = provider.get_capabilities().await.unwrap();\n        assert_eq!(capabilities.auto_scaling, false);\n        assert_eq!(capabilities.health_checks, false);\n    }\n\n    #[tokio::test]\n    async fn test_worker_config_validation() {\n        // RED: Test worker config validation\n        use hodei_shared_types::worker_messages::WorkerId;\n\n        let worker_id = WorkerId::new();\n\n        // Valid config should pass\n        let valid_config = WorkerConfig {\n            worker_id: worker_id.clone(),\n            image: \"test-image:latest\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 1.0,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n        assert!(valid_config.validate().is_ok());\n\n        // Invalid config (empty image) should fail\n        let invalid_config = WorkerConfig {\n            worker_id,\n            image: \"\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 1.0,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n        assert!(invalid_config.validate().is_err());\n\n        // Invalid config (too low CPU) should fail\n        let invalid_config2 = WorkerConfig {\n            worker_id: WorkerId::new(),\n            image: \"test-image:latest\".to_string(),\n            resources: ResourceRequirements {\n                cpu_cores: 0.01,\n                memory_bytes: 1024 * 1024 * 1024,\n                ephemeral_storage_bytes: None,\n            },\n            environment: HashMap::new(),\n            secrets: Vec::new(),\n            health_checks: Vec::new(),\n            scaling_config: ScalingConfiguration {\n                min_replicas: 1,\n                max_replicas: 5,\n                target_cpu_utilization: Some(70),\n            },\n        };\n        assert!(invalid_config2.validate().is_err());\n    }\n\n    #[tokio::test]\n    async fn test_provider_display_implementations() {\n        // RED: Test Display implementations for enums\n        assert_eq!(format!(\"{}\", ProviderType::Kubernetes), \"kubernetes\");\n        assert_eq!(format!(\"{}\", ProviderType::Docker), \"docker\");\n        assert_eq!(format!(\"{}\", ProviderType::AwsEcs), \"aws-ecs\");\n        assert_eq!(\n            format!(\"{}\", ProviderType::AzureContainerInstances),\n            \"azure-container-instances\"\n        );\n        assert_eq!(\n            format!(\"{}\", ProviderType::Custom(\"custom-provider\".to_string())),\n            \"custom-custom-provider\"\n        );\n\n        assert_eq!(format!(\"{}\", WorkerState::Creating), \"Creating\");\n        assert_eq!(format!(\"{}\", WorkerState::Running), \"Running\");\n        assert_eq!(format!(\"{}\", WorkerState::Failed), \"Failed\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_error_types() {\n        // RED: Test all provider error types\n        let error1 = ProviderError::ConfigurationError(\"test error\".to_string());\n        assert!(matches!(error1, ProviderError::ConfigurationError(_)));\n\n        let error2 = ProviderError::WorkerOperationFailed(\"op failed\".to_string());\n        assert!(matches!(error2, ProviderError::WorkerOperationFailed(_)));\n\n        let error3 = ProviderError::CapabilityNotSupported(\"cap not supported\".to_string());\n        assert!(matches!(error3, ProviderError::CapabilityNotSupported(_)));\n\n        let error4 = ProviderError::Timeout;\n        assert!(matches!(error4, ProviderError::Timeout));\n    }\n\n    #[tokio::test]\n    async fn test_worker_metadata() {\n        // RED: Test WorkerMetadata functionality\n        let metadata = WorkerMetadata::new()\n            .with_label(\"env\".to_string(), \"test\".to_string())\n            .with_label(\"version\".to_string(), \"1.0\".to_string());\n\n        assert_eq!(metadata.labels.get(\"env\").unwrap(), \u0026\"test\".to_string());\n        assert_eq!(metadata.labels.get(\"version\").unwrap(), \u0026\"1.0\".to_string());\n    }\n\n    #[tokio::test]\n    async fn test_worker_handle() {\n        // RED: Test WorkerHandle creation\n        use hodei_shared_types::worker_messages::WorkerId;\n\n        let worker_id = WorkerId::new();\n        let handle = WorkerHandle::new(worker_id.clone(), ProviderType::Kubernetes);\n\n        assert_eq!(handle.worker_id, worker_id);\n        assert_eq!(handle.provider_type, ProviderType::Kubernetes);\n        assert_eq!(handle.metadata.labels.len(), 0);\n    }\n\n    #[tokio::test]\n    async fn test_provider_capabilities_creation() {\n        // RED: Test ProviderCapabilities creation\n        let caps = ProviderCapabilities::new();\n        assert_eq!(caps.auto_scaling, false);\n        assert_eq!(caps.health_checks, false);\n\n        let caps2 = ProviderCapabilities {\n            auto_scaling: true,\n            health_checks: true,\n            volumes: true,\n            config_maps: true,\n            secrets: true,\n            network_policies: false,\n            multi_cluster: false,\n        };\n\n        assert_eq!(caps2.auto_scaling, true);\n        assert_eq!(caps2.health_checks, true);\n        assert_eq!(caps2.network_policies, false);\n    }\n\n    #[tokio::test]\n    async fn test_scale_workers() {\n        // RED: Test scale_workers operation\n        let provider = MockWorkerProvider::new(ProviderType::Kubernetes);\n\n        // Scale operation should succeed but return empty vector (no workers)\n        let result = provider.scale_workers(\"test-worker\", 5).await.unwrap();\n        assert!(result.is_empty());\n    }\n\n    #[tokio::test]\n    async fn test_provider_factory_detect_best_provider() {\n        // RED: Test best provider detection\n        let factory = ProviderFactory::new().unwrap();\n        let best_provider = factory.detect_best_provider().await.unwrap();\n\n        assert_eq!(best_provider, ProviderType::Kubernetes);\n    }\n\n    #[tokio::test]\n    async fn test_unsupported_provider_types() {\n        // RED: Test that unsupported providers return errors\n        let factory = ProviderFactory::new().unwrap();\n\n        let aws_result = factory\n            .create_provider(\n                ProviderType::AwsEcs,\n                Some(ProviderConfig {\n                    provider_type: ProviderType::AwsEcs,\n                    connection_string: \"test\".to_string(),\n                    credentials: None,\n                    options: HashMap::new(),\n                }),\n            )\n            .await;\n\n        assert!(matches!(\n            aws_result,\n            Err(ProviderError::ConfigurationError(_))\n        ));\n\n        let azure_result = factory\n            .create_provider(\n                ProviderType::AzureContainerInstances,\n                Some(ProviderConfig {\n                    provider_type: ProviderType::AzureContainerInstances,\n                    connection_string: \"test\".to_string(),\n                    credentials: None,\n                    options: HashMap::new(),\n                }),\n            )\n            .await;\n\n        assert!(matches!(\n            azure_result,\n            Err(ProviderError::ConfigurationError(_))\n        ));\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","models","mod.rs"],"content":"//! Models module\n//!\n//! This module contains all data types for worker provider abstraction.\n\nuse async_trait::async_trait;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::time::{Duration, SystemTime};\n\nuse crate::traits::WorkerProvider;\npub use hodei_shared_types::worker_messages::WorkerId;\n\n/// Provider types supported by the abstraction layer\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub enum ProviderType {\n    Kubernetes,\n    Docker,\n    AwsEcs,\n    AzureContainerInstances,\n    Custom(String),\n}\n\nimpl std::fmt::Display for ProviderType {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            ProviderType::Kubernetes =\u003e write!(f, \"kubernetes\"),\n            ProviderType::Docker =\u003e write!(f, \"docker\"),\n            ProviderType::AwsEcs =\u003e write!(f, \"aws-ecs\"),\n            ProviderType::AzureContainerInstances =\u003e write!(f, \"azure-container-instances\"),\n            ProviderType::Custom(name) =\u003e write!(f, \"custom-{}\", name),\n        }\n    }\n}\n\n/// Provider configuration\n#[derive(Debug, Clone)]\npub struct ProviderConfig {\n    pub provider_type: ProviderType,\n    pub connection_string: String,\n    pub credentials: Option\u003cProviderCredentials\u003e,\n    pub options: HashMap\u003cString, String\u003e,\n}\n\n/// Provider credentials\n#[derive(Debug, Clone)]\npub struct ProviderCredentials {\n    pub username: Option\u003cString\u003e,\n    pub password: Option\u003cString\u003e,\n    pub token: Option\u003cString\u003e,\n    pub certificate_path: Option\u003cString\u003e,\n}\n\n/// Worker configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct WorkerConfig {\n    pub worker_id: WorkerId,\n    pub image: String,\n    pub resources: ResourceRequirements,\n    pub environment: HashMap\u003cString, String\u003e,\n    pub secrets: Vec\u003cSecretReference\u003e,\n    pub health_checks: Vec\u003cHealthCheckConfig\u003e,\n    pub scaling_config: ScalingConfiguration,\n}\n\nimpl WorkerConfig {\n    pub fn validate(\u0026self) -\u003e Result\u003c(), ProviderError\u003e {\n        if self.image.is_empty() {\n            return Err(ProviderError::ConfigurationError(\n                \"image cannot be empty\".to_string(),\n            ));\n        }\n\n        if self.resources.cpu_cores \u003c 0.1 {\n            return Err(ProviderError::ConfigurationError(\n                \"cpu_cores must be at least 0.1\".to_string(),\n            ));\n        }\n\n        Ok(())\n    }\n}\n\n/// Resource requirements for workers\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ResourceRequirements {\n    pub cpu_cores: f64,\n    pub memory_bytes: u64,\n    pub ephemeral_storage_bytes: Option\u003cu64\u003e,\n}\n\n/// Reference to a secret\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SecretReference {\n    pub name: String,\n    pub mount_path: String,\n}\n\n/// Health check configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct HealthCheckConfig {\n    pub path: String,\n    pub port: u16,\n    pub interval: Duration,\n    pub timeout: Duration,\n    pub initial_delay: Duration,\n}\n\n/// Scaling configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ScalingConfiguration {\n    pub min_replicas: usize,\n    pub max_replicas: usize,\n    pub target_cpu_utilization: Option\u003cu32\u003e,\n}\n\n/// Worker status\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct WorkerStatus {\n    pub worker_id: WorkerId,\n    pub state: WorkerState,\n    pub started_at: Option\u003cSystemTime\u003e,\n    pub last_health_check: Option\u003cSystemTime\u003e,\n    pub resource_usage: ResourceUsage,\n}\n\n/// Worker state enum\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub enum WorkerState {\n    Creating,\n    Starting,\n    Running,\n    Degraded,\n    Recovering,\n    Stopping,\n    Stopped,\n    Failed,\n}\n\nimpl std::fmt::Display for WorkerState {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            WorkerState::Creating =\u003e write!(f, \"Creating\"),\n            WorkerState::Starting =\u003e write!(f, \"Starting\"),\n            WorkerState::Running =\u003e write!(f, \"Running\"),\n            WorkerState::Degraded =\u003e write!(f, \"Degraded\"),\n            WorkerState::Recovering =\u003e write!(f, \"Recovering\"),\n            WorkerState::Stopping =\u003e write!(f, \"Stopping\"),\n            WorkerState::Stopped =\u003e write!(f, \"Stopped\"),\n            WorkerState::Failed =\u003e write!(f, \"Failed\"),\n        }\n    }\n}\n\n/// Resource usage metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ResourceUsage {\n    pub cpu_cores: f64,\n    pub memory_bytes: u64,\n    pub disk_bytes: u64,\n}\n\n/// Provider capabilities\n#[derive(Debug, Clone, Default)]\npub struct ProviderCapabilities {\n    pub auto_scaling: bool,\n    pub health_checks: bool,\n    pub volumes: bool,\n    pub config_maps: bool,\n    pub secrets: bool,\n    pub network_policies: bool,\n    pub multi_cluster: bool,\n}\n\nimpl ProviderCapabilities {\n    pub fn new() -\u003e Self {\n        Self::default()\n    }\n}\n\n/// Worker metadata for tracking\n#[derive(Debug, Clone, Default)]\npub struct WorkerMetadata {\n    pub created_at: Option\u003cSystemTime\u003e,\n    pub labels: HashMap\u003cString, String\u003e,\n}\n\nimpl WorkerMetadata {\n    pub fn new() -\u003e Self {\n        Self::default()\n    }\n\n    pub fn with_label(mut self, key: String, value: String) -\u003e Self {\n        self.labels.insert(key, value);\n        self\n    }\n}\n\n/// Handle to an existing worker\n#[derive(Debug, Clone)]\npub struct WorkerHandle {\n    pub worker_id: WorkerId,\n    pub provider_type: ProviderType,\n    pub metadata: WorkerMetadata,\n}\n\nimpl WorkerHandle {\n    pub fn new(worker_id: WorkerId, provider_type: ProviderType) -\u003e Self {\n        Self {\n            worker_id,\n            provider_type,\n            metadata: WorkerMetadata::new(),\n        }\n    }\n}\n\n/// Core error types for worker provider abstraction\n#[derive(thiserror::Error, Debug)]\npub enum ProviderError {\n    #[error(\"provider not initialized: {0}\")]\n    NotInitialized(String),\n\n    #[error(\"provider initialization failed: {0}\")]\n    InitializationFailed(String),\n\n    #[error(\"worker operation failed: {0}\")]\n    WorkerOperationFailed(String),\n\n    #[error(\"capability not supported: {0}\")]\n    CapabilityNotSupported(String),\n\n    #[error(\"configuration error: {0}\")]\n    ConfigurationError(String),\n\n    #[error(\"provider unavailable: {0}\")]\n    ProviderUnavailable(String),\n\n    #[error(\"provider operation timeout\")]\n    Timeout,\n\n    #[error(\"internal error: {0}\")]\n    Internal(#[from] anyhow::Error),\n}\n\n/// Mock provider implementation for testing\npub struct MockWorkerProvider {\n    provider_type: ProviderType,\n    workers: dashmap::DashMap\u003cWorkerId, WorkerStatus\u003e,\n}\n\nimpl MockWorkerProvider {\n    pub fn new(provider_type: ProviderType) -\u003e Self {\n        Self {\n            provider_type,\n            workers: dashmap::DashMap::new(),\n        }\n    }\n}\n\n#[async_trait]\nimpl WorkerProvider for MockWorkerProvider {\n    async fn create_worker(\u0026self, config: \u0026WorkerConfig) -\u003e Result\u003cWorkerHandle, ProviderError\u003e {\n        let status = WorkerStatus {\n            worker_id: config.worker_id.clone(),\n            state: WorkerState::Creating,\n            started_at: None,\n            last_health_check: None,\n            resource_usage: ResourceUsage {\n                cpu_cores: 0.0,\n                memory_bytes: 0,\n                disk_bytes: 0,\n            },\n        };\n\n        self.workers.insert(config.worker_id.clone(), status);\n\n        Ok(WorkerHandle::new(\n            config.worker_id.clone(),\n            self.provider_type.clone(),\n        ))\n    }\n\n    async fn start_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        if let Some(mut status) = self.workers.get_mut(worker_id) {\n            status.state = WorkerState::Starting;\n        }\n        Ok(())\n    }\n\n    async fn stop_worker(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        _graceful: bool,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        if let Some(mut status) = self.workers.get_mut(worker_id) {\n            status.state = WorkerState::Stopping;\n        }\n        Ok(())\n    }\n\n    async fn delete_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        self.workers.remove(worker_id);\n        Ok(())\n    }\n\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, ProviderError\u003e {\n        self.workers\n            .get(worker_id)\n            .map(|status| status.clone())\n            .ok_or_else(|| {\n                ProviderError::WorkerOperationFailed(format!(\"Worker {:?} not found\", worker_id))\n            })\n    }\n\n    async fn get_capabilities(\u0026self) -\u003e Result\u003cProviderCapabilities, ProviderError\u003e {\n        Ok(ProviderCapabilities::new())\n    }\n\n    async fn scale_workers(\n        \u0026self,\n        _worker_type: \u0026str,\n        _target_count: usize,\n    ) -\u003e Result\u003cVec\u003cWorkerId\u003e, ProviderError\u003e {\n        Ok(Vec::new())\n    }\n\n    fn get_provider_type(\u0026self) -\u003e ProviderType {\n        self.provider_type.clone()\n    }\n}\n","traces":[{"line":24,"address":[2286768],"length":1,"stats":{"Line":0}},{"line":25,"address":[2286800],"length":1,"stats":{"Line":0}},{"line":26,"address":[2286859],"length":1,"stats":{"Line":0}},{"line":27,"address":[2286903],"length":1,"stats":{"Line":0}},{"line":28,"address":[2286947],"length":1,"stats":{"Line":0}},{"line":29,"address":[2286997],"length":1,"stats":{"Line":0}},{"line":30,"address":[2287048],"length":1,"stats":{"Line":0}},{"line":66,"address":[2287184],"length":1,"stats":{"Line":1}},{"line":67,"address":[2287222],"length":1,"stats":{"Line":1}},{"line":68,"address":[2287285],"length":1,"stats":{"Line":1}},{"line":69,"address":[2287257],"length":1,"stats":{"Line":1}},{"line":73,"address":[2287240],"length":1,"stats":{"Line":1}},{"line":74,"address":[2287403],"length":1,"stats":{"Line":1}},{"line":75,"address":[2287375],"length":1,"stats":{"Line":1}},{"line":79,"address":[2287366],"length":1,"stats":{"Line":1}},{"line":140,"address":[2287504],"length":1,"stats":{"Line":0}},{"line":141,"address":[2287531],"length":1,"stats":{"Line":0}},{"line":142,"address":[2287562],"length":1,"stats":{"Line":0}},{"line":143,"address":[2287605],"length":1,"stats":{"Line":0}},{"line":144,"address":[2287648],"length":1,"stats":{"Line":0}},{"line":145,"address":[2287691],"length":1,"stats":{"Line":0}},{"line":146,"address":[2287740],"length":1,"stats":{"Line":0}},{"line":147,"address":[2287789],"length":1,"stats":{"Line":0}},{"line":148,"address":[2287835],"length":1,"stats":{"Line":0}},{"line":149,"address":[2287881],"length":1,"stats":{"Line":0}},{"line":175,"address":[2287952],"length":1,"stats":{"Line":1}},{"line":176,"address":[2287956],"length":1,"stats":{"Line":1}},{"line":188,"address":[2288048],"length":1,"stats":{"Line":1}},{"line":189,"address":[2288056],"length":1,"stats":{"Line":1}},{"line":192,"address":[2288185,2288080],"length":1,"stats":{"Line":0}},{"line":193,"address":[2288139,2288104],"length":1,"stats":{"Line":0}},{"line":194,"address":[2288165],"length":1,"stats":{"Line":0}},{"line":207,"address":[2288373,2288208],"length":1,"stats":{"Line":1}},{"line":211,"address":[2288252],"length":1,"stats":{"Line":1}},{"line":251,"address":[2288400,2288529],"length":1,"stats":{"Line":2}},{"line":254,"address":[2288435],"length":1,"stats":{"Line":2}},{"line":261,"address":[2343718,2343869,2343923,2343798,2343968,2344336,2343876,2343680],"length":1,"stats":{"Line":6}},{"line":263,"address":[2343892],"length":1,"stats":{"Line":1}},{"line":267,"address":[2344017],"length":1,"stats":{"Line":1}},{"line":274,"address":[2344131],"length":1,"stats":{"Line":1}},{"line":276,"address":[2344277],"length":1,"stats":{"Line":1}},{"line":277,"address":[2344207],"length":1,"stats":{"Line":1}},{"line":278,"address":[2344246],"length":1,"stats":{"Line":1}},{"line":282,"address":[2344585,2344890,2344705,2344961,2344368,2344592,2344641,2344471,2344406],"length":1,"stats":{"Line":0}},{"line":283,"address":[2344744,2344603],"length":1,"stats":{"Line":0}},{"line":284,"address":[2344815,2344939],"length":1,"stats":{"Line":0}},{"line":286,"address":[2344826],"length":1,"stats":{"Line":0}},{"line":289,"address":[2299062],"length":1,"stats":{"Line":5}},{"line":294,"address":[2345394,2345253],"length":1,"stats":{"Line":2}},{"line":295,"address":[2345589,2345465],"length":1,"stats":{"Line":2}},{"line":297,"address":[2345476],"length":1,"stats":{"Line":1}},{"line":300,"address":[2299118],"length":1,"stats":{"Line":6}},{"line":301,"address":[2345881],"length":1,"stats":{"Line":1}},{"line":302,"address":[2346020],"length":1,"stats":{"Line":1}},{"line":305,"address":[2299166],"length":1,"stats":{"Line":6}},{"line":306,"address":[2346299],"length":1,"stats":{"Line":1}},{"line":307,"address":[2346303],"length":1,"stats":{"Line":1}},{"line":308,"address":[2346528,2346427,2346553],"length":1,"stats":{"Line":3}},{"line":309,"address":[2346439,2346656],"length":1,"stats":{"Line":2}},{"line":310,"address":[2346680],"length":1,"stats":{"Line":1}},{"line":314,"address":[2299209],"length":1,"stats":{"Line":6}},{"line":315,"address":[2347202,2347067],"length":1,"stats":{"Line":2}},{"line":318,"address":[2299272],"length":1,"stats":{"Line":0}},{"line":323,"address":[2347800,2347677],"length":1,"stats":{"Line":0}},{"line":326,"address":[2299312],"length":1,"stats":{"Line":1}},{"line":327,"address":[2299329],"length":1,"stats":{"Line":1}}],"covered":40,"coverable":66},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","provider-abstraction","src","traits","mod.rs"],"content":"//! Provider traits module\n//!\n//! This module contains all the trait definitions for worker providers.\n\nuse crate::ProviderError;\nuse crate::models::{ProviderCapabilities, ProviderType, WorkerConfig, WorkerHandle, WorkerStatus};\nuse async_trait::async_trait;\nuse hodei_shared_types::worker_messages::WorkerId;\n\n/// Core worker provider trait\n#[async_trait]\npub trait WorkerProvider: Send + Sync {\n    /// Create a new worker\n    async fn create_worker(\u0026self, config: \u0026WorkerConfig) -\u003e Result\u003cWorkerHandle, ProviderError\u003e;\n\n    /// Start an existing worker\n    async fn start_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e;\n\n    /// Stop a running worker gracefully\n    async fn stop_worker(\u0026self, worker_id: \u0026WorkerId, graceful: bool) -\u003e Result\u003c(), ProviderError\u003e;\n\n    /// Delete a worker completely\n    async fn delete_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e;\n\n    /// Get worker status\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, ProviderError\u003e;\n\n    /// Get provider capabilities\n    async fn get_capabilities(\u0026self) -\u003e Result\u003cProviderCapabilities, ProviderError\u003e;\n\n    /// Scale workers up or down\n    async fn scale_workers(\n        \u0026self,\n        worker_type: \u0026str,\n        target_count: usize,\n    ) -\u003e Result\u003cVec\u003cWorkerId\u003e, ProviderError\u003e;\n\n    /// Get provider type\n    fn get_provider_type(\u0026self) -\u003e ProviderType;\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","affinity","mod.rs"],"content":"//! Affinity Rules and Taints/Tolerations Module\n//!\n//! This module implements Kubernetes-style affinity rules and taints/tolerations\n//! for fine-grained control over job placement decisions.\n\nuse crate::SchedulerError;\nuse crate::types::*;\nuse std::collections::HashMap;\n\n/// Affinity matcher for evaluating affinity rules\npub struct AffinityMatcher;\n\nimpl AffinityMatcher {\n    /// Create new affinity matcher\n    pub fn new() -\u003e Self {\n        Self\n    }\n\n    /// Check if node satisfies required (hard) affinity constraints\n    pub fn check_required_affinity(\u0026self, node: \u0026WorkerNode, affinity: \u0026NodeAffinity) -\u003e bool {\n        // Check required during scheduling constraints\n        for selector in \u0026affinity.required_during_scheduling {\n            if !self.matches_label_selector(node, selector) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node satisfies preferred (soft) affinity constraints\n    pub fn check_preferred_affinity(\u0026self, node: \u0026WorkerNode, affinity: \u0026NodeAffinity) -\u003e f64 {\n        let mut total_weight = 0;\n        let mut matched_weight = 0;\n\n        for weighted_selector in \u0026affinity.preferred_during_scheduling {\n            total_weight += weighted_selector.weight.abs() as u32;\n\n            if self.matches_label_selector(node, \u0026weighted_selector.selector) {\n                matched_weight += weighted_selector.weight.abs() as u32;\n            }\n        }\n\n        if total_weight == 0 {\n            0.0\n        } else {\n            matched_weight as f64 / total_weight as f64\n        }\n    }\n\n    /// Check if node matches a label selector\n    fn matches_label_selector(\u0026self, node: \u0026WorkerNode, selector: \u0026LabelSelector) -\u003e bool {\n        let node_value = node.labels.get(\u0026selector.key);\n\n        match selector.operator {\n            LabelSelectorOperator::In =\u003e {\n                if let Some(values) = \u0026selector.values {\n                    if let Some(node_val) = node_value {\n                        values.contains(node_val)\n                    } else {\n                        false\n                    }\n                } else {\n                    false\n                }\n            }\n            LabelSelectorOperator::NotIn =\u003e {\n                if let Some(values) = \u0026selector.values {\n                    if let Some(node_val) = node_value {\n                        !values.contains(node_val)\n                    } else {\n                        true // Node doesn't have the key, so it's NOT in the values\n                    }\n                } else {\n                    false\n                }\n            }\n            LabelSelectorOperator::Exists =\u003e node_value.is_some(),\n            LabelSelectorOperator::DoesNotExist =\u003e node_value.is_none(),\n        }\n    }\n\n    /// Check if node selector is satisfied\n    pub fn check_node_selector(\u0026self, node: \u0026WorkerNode, selector: \u0026NodeSelector) -\u003e bool {\n        for (key, value) in \u0026selector.labels {\n            if node.labels.get(key) != Some(value) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node has sufficient tolerations for its taints\n    pub fn check_taints_tolerations(\u0026self, node: \u0026WorkerNode, tolerations: \u0026[Toleration]) -\u003e bool {\n        for taint in \u0026node.taints {\n            let mut has_matching_toleration = false;\n\n            for toleration in tolerations {\n                if self.matches_taint_toleration(taint, toleration) {\n                    has_matching_toleration = true;\n                    break;\n                }\n            }\n\n            // If no matching toleration and effect is NoSchedule, reject\n            if !has_matching_toleration \u0026\u0026 matches!(taint.effect, TaintEffect::NoSchedule) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if a taint matches a toleration\n    fn matches_taint_toleration(\u0026self, taint: \u0026Taint, toleration: \u0026Toleration) -\u003e bool {\n        // Check key\n        if taint.key != toleration.key {\n            return false;\n        }\n\n        // Check effect\n        if taint.effect != toleration.effect {\n            return false;\n        }\n\n        // Check operator and value\n        match (\u0026taint.operator, \u0026toleration.operator) {\n            (TaintOperator::Equal, TaintOperator::Equal) =\u003e taint.value == toleration.value,\n            (TaintOperator::NotEqual, TaintOperator::NotEqual) =\u003e taint.value != toleration.value,\n            (TaintOperator::Exists, TaintOperator::Exists) =\u003e {\n                // Just checking if the key exists\n                true\n            }\n            _ =\u003e false,\n        }\n    }\n\n    /// Evaluate all affinity rules for a job\n    pub fn evaluate_affinity(\n        \u0026self,\n        job: \u0026Job,\n        node: \u0026WorkerNode,\n    ) -\u003e Result\u003cAffinityResult, SchedulerError\u003e {\n        let mut can_schedule = true;\n        let mut affinity_score = 0.0;\n\n        // Check node selector\n        if let Some(selector) = \u0026job.spec.node_selector {\n            if !self.check_node_selector(node, selector) {\n                can_schedule = false;\n            }\n        }\n\n        // Check node affinity\n        if let Some(affinity) = \u0026job.spec.affinity {\n            if !self.check_required_affinity(node, affinity) {\n                can_schedule = false;\n            }\n\n            // Calculate preferred affinity score\n            affinity_score = self.check_preferred_affinity(node, affinity);\n        }\n\n        // Check taints and tolerations\n        if !self.check_taints_tolerations(node, \u0026job.spec.tolerations) {\n            can_schedule = false;\n        }\n\n        Ok(AffinityResult {\n            can_schedule,\n            affinity_score,\n        })\n    }\n}\n\n/// Result of affinity evaluation\n#[derive(Debug, Clone)]\npub struct AffinityResult {\n    pub can_schedule: bool,\n    pub affinity_score: f64, // 0.0 to 1.0, where 1.0 is perfect match\n}\n\n/// Helper to check if job can run on a specific node\npub fn can_schedule_on_node(job: \u0026Job, node: \u0026WorkerNode) -\u003e Result\u003cbool, SchedulerError\u003e {\n    let matcher = AffinityMatcher::new();\n    let result = matcher.evaluate_affinity(job, node)?;\n    Ok(result.can_schedule)\n}\n\n/// Helper to calculate affinity score for a node\npub fn calculate_affinity_score(job: \u0026Job, node: \u0026WorkerNode) -\u003e Result\u003cf64, SchedulerError\u003e {\n    let matcher = AffinityMatcher::new();\n    let result = matcher.evaluate_affinity(job, node)?;\n    Ok(result.affinity_score)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::ComputeResource;\n    use crate::types::KubernetesNodeSpecific;\n\n    fn create_test_node(labels: HashMap\u003cString, String\u003e) -\u003e WorkerNode {\n        WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: BackendType::Kubernetes,\n            status: WorkerStatus::Running,\n            resources: ComputeResource {\n                cpu_cores: 8.0,\n                memory_bytes: 8_000_000_000,\n                gpu_count: 0,\n            },\n            labels,\n            taints: vec![],\n            backend_specific: BackendSpecific::Kubernetes(KubernetesNodeSpecific {\n                node_name: \"test-node\".to_string(),\n                namespace: \"default\".to_string(),\n            }),\n            location: NodeLocation::default(),\n        }\n    }\n\n    fn create_test_job_with_selector(selector_labels: HashMap\u003cString, String\u003e) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(2_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: JobPriority::Medium,\n                node_selector: Some(NodeSelector {\n                    labels: selector_labels,\n                }),\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_node_selector_match() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n        node_labels.insert(\"arch\".to_string(), \"amd64\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let mut job_selector = HashMap::new();\n        job_selector.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n\n        let job = create_test_job_with_selector(job_selector);\n\n        let can_schedule = can_schedule_on_node(\u0026job, \u0026node).unwrap();\n\n        assert!(can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_node_selector_no_match() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"zone\".to_string(), \"us-west-2\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let mut job_selector = HashMap::new();\n        job_selector.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n\n        let job = create_test_job_with_selector(job_selector);\n\n        let can_schedule = can_schedule_on_node(\u0026job, \u0026node).unwrap();\n\n        assert!(!can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_required_affinity() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"gpu\".to_string(), \"true\".to_string());\n        node_labels.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let required_affinity = NodeAffinity {\n            required_during_scheduling: vec![LabelSelector {\n                key: \"gpu\".to_string(),\n                operator: LabelSelectorOperator::Exists,\n                values: None,\n            }],\n            preferred_during_scheduling: vec![],\n        };\n\n        let mut job_selector = HashMap::new();\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(2_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: JobPriority::Medium,\n                node_selector: Some(NodeSelector {\n                    labels: job_selector,\n                }),\n                affinity: Some(required_affinity),\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let can_schedule = can_schedule_on_node(\u0026job, \u0026node).unwrap();\n\n        assert!(can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_taints_tolerations() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node = create_test_node(HashMap::new());\n        node.taints = vec![Taint {\n            key: \"dedicated\".to_string(),\n            operator: TaintOperator::Equal,\n            value: \"gpu\".to_string(),\n            effect: TaintEffect::NoSchedule,\n        }];\n\n        let tolerations = vec![Toleration {\n            key: \"dedicated\".to_string(),\n            operator: TaintOperator::Equal,\n            value: \"gpu\".to_string(),\n            effect: TaintEffect::NoSchedule,\n            toleration_seconds: None,\n        }];\n\n        let can_schedule = matcher.check_taints_tolerations(\u0026node, \u0026tolerations);\n\n        assert!(can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_no_toleration_for_noschedule_taint() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node = create_test_node(HashMap::new());\n        node.taints = vec![Taint {\n            key: \"dedicated\".to_string(),\n            operator: TaintOperator::Equal,\n            value: \"gpu\".to_string(),\n            effect: TaintEffect::NoSchedule,\n        }];\n\n        // No tolerations provided\n        let tolerations = vec![];\n\n        let can_schedule = matcher.check_taints_tolerations(\u0026node, \u0026tolerations);\n\n        assert!(!can_schedule);\n    }\n\n    #[tokio::test]\n    async fn test_affinity_score_calculation() {\n        let matcher = AffinityMatcher::new();\n\n        let mut node_labels = HashMap::new();\n        node_labels.insert(\"zone\".to_string(), \"us-east-1\".to_string());\n        node_labels.insert(\"type\".to_string(), \"compute\".to_string());\n\n        let node = create_test_node(node_labels);\n\n        let affinity = NodeAffinity {\n            required_during_scheduling: vec![],\n            preferred_during_scheduling: vec![\n                WeightedLabelSelector {\n                    selector: LabelSelector {\n                        key: \"zone\".to_string(),\n                        operator: LabelSelectorOperator::In,\n                        values: Some(vec![\"us-east-1\".to_string()]),\n                    },\n                    weight: 50,\n                },\n                WeightedLabelSelector {\n                    selector: LabelSelector {\n                        key: \"type\".to_string(),\n                        operator: LabelSelectorOperator::In,\n                        values: Some(vec![\"compute\".to_string()]),\n                    },\n                    weight: 50,\n                },\n            ],\n        };\n\n        let score = matcher.check_preferred_affinity(\u0026node, \u0026affinity);\n\n        assert_eq!(score, 1.0); // Perfect match\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","backend","mod.rs"],"content":"//! Scheduler Backend Abstraction Module\n//!\n//! This module provides the backend abstraction layer for supporting multiple\n//! execution backends (Kubernetes, Docker, Cloud VMs, etc.). Each backend\n//! implements the SchedulerBackend trait to provide uniform interface.\n\nuse crate::types::*;\nuse crate::{SchedulerError, WorkerId, JobId};\nuse std::sync::Arc;\nuse async_trait::async_trait;\n\n/// Scheduler backend trait for multi-backend support\n#[async_trait]\npub trait SchedulerBackend: Send + Sync {\n    /// Get backend type\n    fn backend_type(\u0026self) -\u003e BackendType;\n\n    /// List all available nodes\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e;\n\n    /// Get specific node by ID\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e;\n\n    /// Bind job to node\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e;\n\n    /// Unbind job from node\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e;\n\n    /// Get node status\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e;\n}\n\n/// Compute resources of a worker node\n#[derive(Debug, Clone, PartialEq)]\npub struct ComputeResource {\n    pub cpu_cores: f64,\n    pub memory_bytes: u64,\n    pub gpu_count: u32,\n}\n\nimpl ComputeResource {\n    /// Check if node has enough resources\n    pub fn has_resources(\u0026self, requirements: \u0026ResourceRequirements) -\u003e bool {\n        if let Some(cpu) = requirements.cpu_cores {\n            if self.cpu_cores \u003c cpu {\n                return false;\n            }\n        }\n\n        if let Some(memory) = requirements.memory_bytes {\n            if self.memory_bytes \u003c memory {\n                return false;\n            }\n        }\n\n        if let Some(gpu) = requirements.gpu_count {\n            if self.gpu_count \u003c gpu {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Calculate utilization percentage (0.0 to 1.0)\n    pub fn utilization(\u0026self, requirements: \u0026ResourceRequirements) -\u003e f64 {\n        let mut max_util: f64 = 0.0;\n\n        if let Some(cpu) = requirements.cpu_cores {\n            max_util = max_util.max(cpu / self.cpu_cores);\n        }\n\n        if let Some(memory) = requirements.memory_bytes {\n            let mem_util = memory as f64 / self.memory_bytes as f64;\n            max_util = max_util.max(mem_util);\n        }\n\n        if let Some(gpu) = requirements.gpu_count {\n            if self.gpu_count \u003e 0 {\n                let gpu_util = gpu as f64 / self.gpu_count as f64;\n                max_util = max_util.max(gpu_util);\n            }\n        }\n\n        max_util.min(1.0)\n    }\n}\n\n/// Kubernetes backend adapter\npub struct KubernetesBackend {\n    // In production: would have Kubernetes client/API connector\n}\n\nimpl KubernetesBackend {\n    pub fn new() -\u003e Self {\n        Self {}\n    }\n}\n\n#[async_trait]\nimpl SchedulerBackend for KubernetesBackend {\n    fn backend_type(\u0026self) -\u003e BackendType {\n        BackendType::Kubernetes\n    }\n\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        // TODO: Connect to Kubernetes API\n        // For now: return empty list\n        Ok(vec![])\n    }\n\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n        // TODO: Implement Kubernetes API call\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        // TODO: Implement Kubernetes Pod binding\n        tracing::info!(\"Binding job {} to Kubernetes node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Unbinding job {} from Kubernetes node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e {\n        // TODO: Get status from Kubernetes API\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n}\n\n/// Docker backend adapter\npub struct DockerBackend {\n    // In production: would have Docker client connector\n}\n\nimpl DockerBackend {\n    pub fn new() -\u003e Self {\n        Self {}\n    }\n}\n\n#[async_trait]\nimpl SchedulerBackend for DockerBackend {\n    fn backend_type(\u0026self) -\u003e BackendType {\n        BackendType::Docker\n    }\n\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        // TODO: Connect to Docker API\n        Ok(vec![])\n    }\n\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Binding job {} to Docker node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Unbinding job {} from Docker node {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n}\n\n/// Cloud VM backend adapter\npub struct CloudVmBackend {\n    // In production: would have cloud provider client (AWS, Azure, GCP)\n}\n\nimpl CloudVmBackend {\n    pub fn new() -\u003e Self {\n        Self {}\n    }\n}\n\n#[async_trait]\nimpl SchedulerBackend for CloudVmBackend {\n    fn backend_type(\u0026self) -\u003e BackendType {\n        BackendType::CloudVm\n    }\n\n    async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        // TODO: Connect to cloud provider API\n        Ok(vec![])\n    }\n\n    async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n\n    async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Binding job {} to Cloud VM {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn unbind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        tracing::info!(\"Unbinding job {} from Cloud VM {}\", job_id, node_id);\n        Ok(())\n    }\n\n    async fn get_node_status(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerStatus, SchedulerError\u003e {\n        Err(SchedulerError::WorkerNotFound(*id))\n    }\n}\n\n/// Backend registry for managing multiple backends\npub struct BackendRegistry {\n    backends: dashmap::DashMap\u003cBackendType, Arc\u003cdyn SchedulerBackend\u003e\u003e,\n}\n\nimpl BackendRegistry {\n    pub fn new() -\u003e Self {\n        Self {\n            backends: dashmap::DashMap::new(),\n        }\n    }\n\n    /// Register backend\n    pub fn register(\u0026self, backend: Arc\u003cdyn SchedulerBackend\u003e) {\n        let backend_type = backend.backend_type();\n        self.backends.insert(backend_type, backend);\n    }\n\n    /// Get backend by type\n    pub fn get(\u0026self, backend_type: BackendType) -\u003e Option\u003cArc\u003cdyn SchedulerBackend\u003e\u003e {\n        self.backends.get(\u0026backend_type).map(|entry| entry.clone())\n    }\n\n    /// List all registered backend types\n    pub fn list_types(\u0026self) -\u003e Vec\u003cBackendType\u003e {\n        self.backends.iter().map(|entry| entry.key().clone()).collect()\n    }\n}\n\nimpl Default for BackendRegistry {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::*;\n\n    #[test]\n    fn test_compute_resource_has_resources() {\n        let resource = ComputeResource {\n            cpu_cores: 8.0,\n            memory_bytes: 16_000_000_000,\n            gpu_count: 2,\n        };\n\n        let requirements = ResourceRequirements {\n            cpu_cores: Some(4.0),\n            memory_bytes: Some(8_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(resource.has_resources(\u0026requirements));\n\n        // Insufficient resources\n        let insufficient_requirements = ResourceRequirements {\n            cpu_cores: Some(16.0),\n            memory_bytes: Some(8_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(!resource.has_resources(\u0026insufficient_requirements));\n    }\n\n    #[test]\n    fn test_compute_resource_utilization() {\n        let resource = ComputeResource {\n            cpu_cores: 8.0,\n            memory_bytes: 16_000_000_000,\n            gpu_count: 2,\n        };\n\n        let requirements = ResourceRequirements {\n            cpu_cores: Some(4.0),\n            memory_bytes: Some(8_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        let utilization = resource.utilization(\u0026requirements);\n        assert!(utilization \u003e 0.0 \u0026\u0026 utilization \u003c= 1.0);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","integration","mod.rs"],"content":"//! Scheduler and Worker Lifecycle Integration (US-012)\n//!\n//! This module provides integration between the scheduler framework and the worker\n//! lifecycle management system, enabling:\n//! - Real-time worker state synchronization with scheduler\n//! - Event-driven scheduling based on worker lifecycle events\n//! - Automatic job rescheduling on worker failures\n//! - Coordinated preemption and job cleanup\n//! - Health monitoring and recovery\n\nuse crate::backend::SchedulerBackend;\nuse crate::types::{Job, WorkerNode, WorkerStatus};\nuse crate::{JobId, WorkerId};\nuse hodei_worker_lifecycle::WorkerManager;\nuse parking_lot::Mutex;\nuse std::collections::{HashMap, HashSet};\nuse std::sync::Arc;\nuse tokio::sync::broadcast;\nuse tracing::{error, info, warn};\n\n/// Integration coordinator between scheduler and worker lifecycle\npub struct SchedulerWorkerIntegration {\n    scheduler_backend: Arc\u003cdyn SchedulerBackend\u003e,\n    worker_manager: Arc\u003cWorkerManager\u003e,\n    event_handlers: Arc\u003cMutex\u003cVec\u003cBox\u003cdyn EventHandler\u003e\u003e\u003e\u003e,\n    job_to_worker_map: Arc\u003cMutex\u003cHashMap\u003cJobId, WorkerId\u003e\u003e\u003e,\n    shutdown_tx: broadcast::Sender\u003c()\u003e,\n}\n\nimpl SchedulerWorkerIntegration {\n    /// Create new integration coordinator\n    pub fn new(\n        scheduler_backend: Arc\u003cdyn SchedulerBackend\u003e,\n        worker_manager: Arc\u003cWorkerManager\u003e,\n    ) -\u003e Self {\n        let (shutdown_tx, _) = broadcast::channel(1);\n\n        Self {\n            scheduler_backend,\n            worker_manager,\n            event_handlers: Arc::new(Mutex::new(Vec::new())),\n            job_to_worker_map: Arc::new(Mutex::new(HashMap::new())),\n            shutdown_tx,\n        }\n    }\n\n    /// Start the integration coordinator\n    pub async fn start(\u0026self) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Starting scheduler-worker integration\");\n\n        // Start monitoring worker lifecycle events\n        self.spawn_event_monitoring().await;\n\n        // Register event handlers\n        self.register_default_handlers().await;\n\n        info!(\"Scheduler-worker integration started successfully\");\n\n        Ok(())\n    }\n\n    /// Stop the integration coordinator\n    pub async fn stop(\u0026self) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Stopping scheduler-worker integration\");\n\n        // Notify all tasks to shutdown\n        let _ = self.shutdown_tx.send(());\n\n        info!(\"Scheduler-worker integration stopped\");\n\n        Ok(())\n    }\n\n    /// Handle worker registration event\n    pub async fn handle_worker_registered(\n        \u0026self,\n        worker_id: WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Worker registered event: {}\", worker_id);\n\n        // Update backend with worker info\n        let worker_info = self\n            .worker_manager\n            .get_worker_status(worker_id)\n            .map_err(|e| IntegrationError::WorkerLifecycleError(e.to_string()))?;\n\n        // Trigger rescheduling of pending jobs if we now have capacity\n        self.trigger_pending_job_retry().await;\n\n        Ok(())\n    }\n\n    /// Handle worker heartbeat event\n    pub async fn handle_worker_heartbeat(\n        \u0026self,\n        worker_id: WorkerId,\n        load: f64,\n        jobs_running: usize,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        // Update worker manager with heartbeat\n        self.worker_manager\n            .handle_heartbeat(worker_id, load, jobs_running)\n            .map_err(|e| IntegrationError::WorkerLifecycleError(e.to_string()))?;\n\n        // Check if we need to reschedule jobs based on load\n        if load \u003e 0.9 {\n            warn!(\"Worker {} has high load ({:.2}%)\", worker_id, load * 100.0);\n            // Could trigger load balancing here\n        }\n\n        Ok(())\n    }\n\n    /// Handle worker failure event\n    pub async fn handle_worker_failed(\u0026self, worker_id: WorkerId) -\u003e Result\u003c(), IntegrationError\u003e {\n        error!(\"Worker failed event: {}\", worker_id);\n\n        // Get jobs running on this worker\n        let jobs_to_reschedule = {\n            let map = self.job_to_worker_map.lock();\n            map.iter()\n                .filter_map(|(job_id, w_id)| {\n                    if *w_id == worker_id {\n                        Some(*job_id)\n                    } else {\n                        None\n                    }\n                })\n                .collect::\u003cVec\u003c_\u003e\u003e()\n        };\n\n        // Remove worker from job mapping\n        {\n            let mut map = self.job_to_worker_map.lock();\n            map.retain(|_, w_id| *w_id != worker_id);\n        }\n\n        // Reschedule jobs on different workers\n        for job_id in jobs_to_reschedule {\n            info!(\n                \"Rescheduling job {} from failed worker {}\",\n                job_id, worker_id\n            );\n            // In production: get job details from scheduler queue and reschedule\n            // For now: just log the event\n        }\n\n        Ok(())\n    }\n\n    /// Handle worker deregistration event\n    pub async fn handle_worker_deregistered(\n        \u0026self,\n        worker_id: WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        info!(\"Worker deregistered event: {}\", worker_id);\n\n        // Handle graceful shutdown\n        self.handle_worker_failed(worker_id).await?;\n\n        Ok(())\n    }\n\n    /// Notify scheduler that job was bound to worker\n    pub fn notify_job_bound(\u0026self, job_id: JobId, worker_id: WorkerId) {\n        let mut map = self.job_to_worker_map.lock();\n        map.insert(job_id, worker_id);\n        info!(\"Job {} bound to worker {}\", job_id, worker_id);\n    }\n\n    /// Notify scheduler that job completed on worker\n    pub fn notify_job_completed(\u0026self, job_id: JobId, worker_id: WorkerId) {\n        let mut map = self.job_to_worker_map.lock();\n        map.remove(\u0026job_id);\n        info!(\"Job {} completed on worker {}\", job_id, worker_id);\n    }\n\n    /// Get worker status from lifecycle manager\n    pub async fn get_worker_status(\n        \u0026self,\n        worker_id: WorkerId,\n    ) -\u003e Result\u003cWorkerStatusView, IntegrationError\u003e {\n        let status = self\n            .worker_manager\n            .get_worker_status(worker_id)\n            .map_err(|e| IntegrationError::WorkerLifecycleError(e.to_string()))?;\n\n        // Map to scheduler view\n        let scheduler_status = match status.state.as_str() {\n            \"AVAILABLE\" =\u003e WorkerStatus::Ready,\n            \"RUNNING\" =\u003e WorkerStatus::Running,\n            \"UNHEALTHY\" =\u003e WorkerStatus::Failed,\n            \"DRAINING\" =\u003e WorkerStatus::Draining,\n            _ =\u003e WorkerStatus::Offline,\n        };\n\n        Ok(WorkerStatusView {\n            worker_id,\n            state: scheduler_status,\n            load: status.load,\n            jobs_running: status.jobs_running,\n            max_jobs: status.max_jobs,\n            has_gpu: status.capabilities.has_gpu,\n            backend_type: self.scheduler_backend.backend_type(),\n        })\n    }\n\n    /// Get all workers summary\n    pub async fn get_workers_summary(\u0026self) -\u003e Vec\u003cWorkerSummary\u003e {\n        self.worker_manager\n            .get_workers_summary()\n            .into_iter()\n            .map(|w| WorkerSummary {\n                worker_id: w.worker_id,\n                state: w.state,\n                load: w.load,\n                jobs_running: w.jobs_running,\n                has_gpu: w.has_gpu,\n            })\n            .collect()\n    }\n\n    /// Start background monitoring tasks\n    async fn spawn_event_monitoring(\u0026self) {\n        let worker_manager = Arc::clone(\u0026self.worker_manager);\n        let scheduler_backend = Arc::clone(\u0026self.scheduler_backend);\n        let shutdown_rx = self.shutdown_tx.subscribe();\n\n        // Monitor worker health and sync with backend\n        tokio::spawn(async move {\n            let mut shutdown_rx = shutdown_rx;\n\n            loop {\n                tokio::select! {\n                    _ = tokio::time::sleep(tokio::time::Duration::from_secs(5)) =\u003e {\n                        // Periodic health check and sync\n                        let workers = worker_manager.get_workers_summary();\n\n                        // Sync with backend (simplified - in production would do incremental sync)\n                        for worker_summary in workers {\n                            let backend_status = match worker_summary.state.as_str() {\n                                \"AVAILABLE\" =\u003e WorkerStatus::Ready,\n                                \"RUNNING\" =\u003e WorkerStatus::Running,\n                                \"DRAINING\" =\u003e WorkerStatus::Draining,\n                                _ =\u003e WorkerStatus::Offline,\n                            };\n\n                            // Update backend if needed (in production: only if changed)\n                            // scheduler_backend.update_worker_status(worker_summary.worker_id, backend_status).await;\n                        }\n                    }\n                    _ = shutdown_rx.recv() =\u003e {\n                        break;\n                    }\n                }\n            }\n        });\n    }\n\n    /// Register default event handlers\n    async fn register_default_handlers(\u0026self) {\n        let mut handlers = self.event_handlers.lock();\n        handlers.push(Box::new(LoadBalancingHandler::new()));\n        handlers.push(Box::new(PreemptionHandler::new()));\n        handlers.push(Box::new(MetricsHandler::new()));\n    }\n\n    /// Trigger retry of pending jobs\n    async fn trigger_pending_job_retry(\u0026self) {\n        info!(\"Triggering pending job retry due to worker availability change\");\n        // In production: would notify scheduler to re-evaluate pending jobs\n    }\n\n    /// Find workers for job using both scheduler and lifecycle manager\n    pub async fn find_suitable_workers(\n        \u0026self,\n        job: \u0026Job,\n    ) -\u003e Result\u003cVec\u003cWorkerNode\u003e, IntegrationError\u003e {\n        // Get workers from backend\n        let all_workers = self.scheduler_backend.list_nodes().await?;\n\n        // Filter using lifecycle manager state\n        let lifecycle_workers = self.worker_manager.get_workers_summary();\n        let lifecycle_worker_ids: std::collections::HashSet\u003c_\u003e =\n            lifecycle_workers.iter().map(|w| w.worker_id).collect();\n\n        let suitable_workers: Vec\u003cWorkerNode\u003e = all_workers\n            .into_iter()\n            .filter(|worker| {\n                // Check if worker is in lifecycle manager\n                lifecycle_worker_ids.contains(\u0026worker.id)\n                    \u0026\u0026 worker.status == WorkerStatus::Ready\n                    \u0026\u0026 worker.matches_requirements(job)\n            })\n            .collect();\n\n        Ok(suitable_workers)\n    }\n\n    /// Handle job binding to worker\n    pub async fn bind_job_to_worker(\n        \u0026self,\n        job_id: \u0026JobId,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        // Record binding\n        self.notify_job_bound(*job_id, *worker_id);\n\n        // Notify backend\n        self.scheduler_backend.bind_job(job_id, worker_id).await?;\n\n        // Update worker state if needed\n        // Note: Worker state is managed by WorkerManager, not scheduler\n\n        Ok(())\n    }\n\n    /// Handle job unbinding from worker\n    pub async fn unbind_job_from_worker(\n        \u0026self,\n        job_id: \u0026JobId,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003c(), IntegrationError\u003e {\n        // Remove binding\n        {\n            let mut map = self.job_to_worker_map.lock();\n            map.remove(job_id);\n        }\n\n        // Notify backend\n        self.scheduler_backend.unbind_job(job_id, worker_id).await?;\n\n        Ok(())\n    }\n\n    /// Get job to worker mapping\n    pub fn get_job_to_worker_map(\u0026self) -\u003e HashMap\u003cJobId, WorkerId\u003e {\n        let map = self.job_to_worker_map.lock();\n        map.clone()\n    }\n\n    /// Check if job is currently bound to a worker\n    pub fn is_job_bound(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        let map = self.job_to_worker_map.lock();\n        map.contains_key(job_id)\n    }\n\n    /// Get worker that job is bound to\n    pub fn get_job_worker(\u0026self, job_id: \u0026JobId) -\u003e Option\u003cWorkerId\u003e {\n        let map = self.job_to_worker_map.lock();\n        map.get(job_id).copied()\n    }\n}\n\n/// Event handler trait for integration events\npub trait EventHandler: Send + Sync {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId);\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId);\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId);\n}\n\n/// Load balancing event handler\nstruct LoadBalancingHandler;\n\nimpl LoadBalancingHandler {\n    fn new() -\u003e Self {\n        Self\n    }\n}\n\nimpl EventHandler for LoadBalancingHandler {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId) {\n        info!(\"[LoadBalancer] New worker registered: {}\", worker_id);\n    }\n\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId) {\n        error!(\"[LoadBalancer] Worker failed: {}\", worker_id);\n    }\n\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId) {\n        info!(\"[LoadBalancer] Worker deregistered: {}\", worker_id);\n    }\n}\n\n/// Preemption event handler\nstruct PreemptionHandler;\n\nimpl PreemptionHandler {\n    fn new() -\u003e Self {\n        Self\n    }\n}\n\nimpl EventHandler for PreemptionHandler {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Preemption] Worker registered: {}\", worker_id);\n    }\n\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId) {\n        warn!(\"[Preemption] Worker failed: {}\", worker_id);\n    }\n\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Preemption] Worker deregistered: {}\", worker_id);\n    }\n}\n\n/// Metrics event handler\nstruct MetricsHandler;\n\nimpl MetricsHandler {\n    fn new() -\u003e Self {\n        Self\n    }\n}\n\nimpl EventHandler for MetricsHandler {\n    fn handle_worker_registered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Metrics] Worker registered: {}\", worker_id);\n    }\n\n    fn handle_worker_failed(\u0026self, worker_id: WorkerId) {\n        warn!(\"[Metrics] Worker failed: {}\", worker_id);\n    }\n\n    fn handle_worker_deregistered(\u0026self, worker_id: WorkerId) {\n        info!(\"[Metrics] Worker deregistered: {}\", worker_id);\n    }\n}\n\n/// Worker status view for integration\n#[derive(Debug, Clone)]\npub struct WorkerStatusView {\n    pub worker_id: WorkerId,\n    pub state: WorkerStatus,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub max_jobs: usize,\n    pub has_gpu: bool,\n    pub backend_type: crate::types::BackendType,\n}\n\n/// Worker summary for listing\n#[derive(Debug, Clone)]\npub struct WorkerSummary {\n    pub worker_id: WorkerId,\n    pub state: String,\n    pub load: f64,\n    pub jobs_running: usize,\n    pub has_gpu: bool,\n}\n\n/// Integration error types\n#[derive(thiserror::Error, Debug)]\npub enum IntegrationError {\n    #[error(\"scheduler error: {0}\")]\n    SchedulerError(#[from] crate::SchedulerError),\n\n    #[error(\"worker lifecycle error: {0}\")]\n    WorkerLifecycleError(String),\n\n    #[error(\"worker not found: {0}\")]\n    WorkerNotFound(WorkerId),\n\n    #[error(\"job not bound\")]\n    JobNotBound,\n\n    #[error(\"integration error: {0}\")]\n    Integration(String),\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{ComputeResource, SchedulerBackend};\n    use crate::types::{BackendType, Job, JobMetadata, JobSpec, ResourceRequirements, WorkerNode};\n    use chrono::Utc;\n    use hodei_worker_lifecycle::{Worker, WorkerCapabilities, WorkerManager};\n    use std::collections::HashMap;\n    use tokio::time::Duration;\n\n    /// Mock backend for testing\n    struct MockIntegrationBackend {\n        workers: Vec\u003cWorkerNode\u003e,\n    }\n\n    impl MockIntegrationBackend {\n        fn new() -\u003e Self {\n            Self {\n                workers: vec![\n                    WorkerNode {\n                        id: uuid::Uuid::new_v4(),\n                        backend_type: BackendType::Kubernetes,\n                        status: WorkerStatus::Ready,\n                        resources: ComputeResource {\n                            cpu_cores: 8.0,\n                            memory_bytes: 16_000_000_000,\n                            gpu_count: 0,\n                        },\n                        labels: HashMap::new(),\n                        taints: vec![],\n                        backend_specific: crate::types::BackendSpecific::Kubernetes(\n                            crate::types::KubernetesNodeSpecific {\n                                node_name: \"node-1\".to_string(),\n                                namespace: \"default\".to_string(),\n                            },\n                        ),\n                        location: crate::types::NodeLocation::default(),\n                    },\n                    WorkerNode {\n                        id: uuid::Uuid::new_v4(),\n                        backend_type: BackendType::Kubernetes,\n                        status: WorkerStatus::Ready,\n                        resources: ComputeResource {\n                            cpu_cores: 4.0,\n                            memory_bytes: 8_000_000_000,\n                            gpu_count: 0,\n                        },\n                        labels: HashMap::new(),\n                        taints: vec![],\n                        backend_specific: crate::types::BackendSpecific::Kubernetes(\n                            crate::types::KubernetesNodeSpecific {\n                                node_name: \"node-2\".to_string(),\n                                namespace: \"default\".to_string(),\n                            },\n                        ),\n                        location: crate::types::NodeLocation::default(),\n                    },\n                ],\n            }\n        }\n    }\n\n    #[async_trait::async_trait]\n    impl SchedulerBackend for MockIntegrationBackend {\n        fn backend_type(\u0026self) -\u003e BackendType {\n            BackendType::Kubernetes\n        }\n\n        async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, crate::SchedulerError\u003e {\n            Ok(self.workers.clone())\n        }\n\n        async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, crate::SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .cloned()\n                .ok_or_else(|| crate::SchedulerError::WorkerNotFound(*id))\n        }\n\n        async fn bind_job(\n            \u0026self,\n            _job_id: \u0026JobId,\n            _node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), crate::SchedulerError\u003e {\n            Ok(())\n        }\n\n        async fn unbind_job(\n            \u0026self,\n            _job_id: \u0026JobId,\n            _node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), crate::SchedulerError\u003e {\n            Ok(())\n        }\n\n        async fn get_node_status(\n            \u0026self,\n            id: \u0026WorkerId,\n        ) -\u003e Result\u003cWorkerStatus, crate::SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .map(|w| w.status.clone())\n                .ok_or_else(|| crate::SchedulerError::WorkerNotFound(*id))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_integration_creation() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager);\n\n        assert!(integration.start().await.is_ok());\n        assert!(integration.stop().await.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_worker_registration() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager.clone());\n        integration.start().await.unwrap();\n\n        let worker_id = uuid::Uuid::new_v4();\n\n        // Register worker in lifecycle manager\n        worker_manager\n            .register_worker(Worker::new(\n                worker_id,\n                WorkerCapabilities::default(),\n                10,\n                None,\n            ))\n            .unwrap();\n\n        // Handle registration event\n        assert!(\n            integration\n                .handle_worker_registered(worker_id)\n                .await\n                .is_ok()\n        );\n\n        // Check status\n        let status = integration.get_worker_status(worker_id).await;\n        assert!(status.is_ok());\n\n        integration.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_job_binding_tracking() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager);\n        integration.start().await.unwrap();\n\n        let job_id = uuid::Uuid::new_v4();\n        let worker_id = uuid::Uuid::new_v4();\n\n        // Notify binding\n        integration.notify_job_bound(job_id, worker_id);\n\n        // Check if job is bound\n        assert!(integration.is_job_bound(\u0026job_id));\n        assert_eq!(integration.get_job_worker(\u0026job_id), Some(worker_id));\n\n        // Check map\n        let map = integration.get_job_to_worker_map();\n        assert_eq!(map.get(\u0026job_id), Some(\u0026worker_id));\n\n        // Notify completion\n        integration.notify_job_completed(job_id, worker_id);\n\n        // Check if job is no longer bound\n        assert!(!integration.is_job_bound(\u0026job_id));\n        assert_eq!(integration.get_job_worker(\u0026job_id), None);\n\n        integration.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_find_suitable_workers() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        // Get backend workers before moving backend\n        let backend_workers = backend.list_nodes().await.unwrap();\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager.clone());\n        integration.start().await.unwrap();\n\n        // Register backend workers in the lifecycle manager\n        for worker_node in backend_workers {\n            worker_manager\n                .register_worker(Worker::new(\n                    worker_node.id,\n                    WorkerCapabilities::default(),\n                    10,\n                    None,\n                ))\n                .unwrap();\n        }\n\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(4_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        // Find suitable workers\n        let suitable = integration.find_suitable_workers(\u0026job).await;\n        assert!(suitable.is_ok());\n\n        // Should return all workers as they meet requirements\n        let workers = suitable.unwrap();\n        assert!(!workers.is_empty());\n\n        integration.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_worker_heartbeat() {\n        let backend = Arc::new(MockIntegrationBackend::new());\n        let worker_manager = Arc::new(WorkerManager::new(\n            None,\n            Duration::from_secs(30),\n            Duration::from_secs(10),\n        ));\n\n        let integration = SchedulerWorkerIntegration::new(backend, worker_manager.clone());\n        integration.start().await.unwrap();\n\n        let worker_id = uuid::Uuid::new_v4();\n\n        // Register worker\n        worker_manager\n            .register_worker(Worker::new(\n                worker_id,\n                WorkerCapabilities::default(),\n                10,\n                None,\n            ))\n            .unwrap();\n\n        // Send heartbeat\n        assert!(\n            integration\n                .handle_worker_heartbeat(worker_id, 0.5, 2)\n                .await\n                .is_ok()\n        );\n\n        integration.stop().await.unwrap();\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","lib.rs"],"content":"//! Kubernetes-Style Scheduler Framework (US-007)\n//!\n//! This module provides a complete scheduler framework inspired by Kubernetes for the\n//! Hodei Jobs distributed CI/CD system. The scheduler supports multiple execution\n//! backends and implements a 4-phase scheduling pipeline.\n//!\n//! Architecture:\n//! - Backend abstraction layer for multi-backend support\n//! - 4-phase scheduling pipeline: Informer  Filter  Score  Bind\n//! - Priority queue with preemption support\n//! - Node affinity rules and taints/tolerations\n//! - Deterministic scheduling algorithms\n//!\n//! Supported backends:\n//! - Kubernetes (K8s clusters)\n//! - Docker (standalone containers)\n//! - Cloud VMs (AWS, Azure, GCP)\n//! - Bare Metal (physical servers)\n//! - Serverless (Lambda, Azure Functions)\n//! - HPC (SLURM, PBS clusters)\n//!\n//! Performance targets:\n//! - Scheduling latency: \u003c100ms\n//! - Throughput: 1000+ jobs/minute\n//! - Queue operations: O(log n)\n//! - Filter operations: O(n)\n\npub mod affinity;\npub mod backend;\npub mod integration;\npub mod pipeline;\npub mod queue;\npub mod selection;\npub mod types;\n\npub use affinity::*;\npub use backend::*;\npub use integration::*;\npub use pipeline::*;\npub use queue::*;\npub use selection::*;\npub use types::*;\n\npub use backend::ComputeResource;\n\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\nuse tracing::{error, info};\n\n/// Core scheduler orchestrator\npub struct Scheduler {\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n    pipeline: SchedulingPipeline,\n    queue: PriorityQueue,\n    config: SchedulerConfig,\n    metrics: Arc\u003cRwLock\u003cSchedulerMetrics\u003e\u003e,\n}\n\nimpl Scheduler {\n    /// Create new scheduler with backend\n    pub fn new(\n        backend: Arc\u003cdyn SchedulerBackend\u003e,\n        config: SchedulerConfig,\n    ) -\u003e Result\u003cSelf, SchedulerError\u003e {\n        let pipeline = SchedulingPipeline::new(config.pipeline.clone());\n        let queue = PriorityQueue::new(config.queue.clone());\n        let metrics = Arc::new(RwLock::new(SchedulerMetrics::default()));\n\n        Ok(Self {\n            backend,\n            pipeline,\n            queue,\n            config,\n            metrics,\n        })\n    }\n\n    /// Schedule a job (main entry point)\n    pub async fn schedule_job(\u0026self, job: Job) -\u003e Result\u003cSchedulingResult, SchedulerError\u003e {\n        let start_time = std::time::Instant::now();\n        let job_id = job.metadata.id;\n\n        info!(\"Starting scheduling for job {}\", job_id);\n\n        // Validate job requirements\n        self.validate_job(\u0026job)?;\n\n        // Add to queue\n        self.queue.enqueue(job.clone()).await?;\n\n        // Phase 1: Inform - Get available workers\n        let available_workers = self.pipeline.inform().await?;\n\n        // Phase 2: Filter - Filter feasible workers\n        let feasible_workers = self.pipeline.filter(\u0026job, available_workers).await?;\n\n        if feasible_workers.is_empty() {\n            // No suitable workers found\n            let result = SchedulingResult {\n                job_id,\n                assigned_node: None,\n                status: SchedulingStatus::Pending,\n                scheduling_latency_ms: start_time.elapsed().as_millis() as u64,\n                reasoning: \"No feasible workers found\".to_string(),\n                queue_position: self.queue.position(\u0026job_id).await.unwrap_or(0),\n            };\n\n            // Update metrics\n            self.update_metrics_on_failure(\u0026result).await;\n\n            return Ok(result);\n        }\n\n        // Phase 3: Score - Rank workers\n        let scored_workers = self.pipeline.score(\u0026job, feasible_workers).await?;\n\n        // Phase 4: Bind - Assign to best worker\n        let best_worker = scored_workers\n            .first()\n            .ok_or_else(|| SchedulerError::NoEligibleWorker(job_id))?;\n\n        let binding_result = self.backend.bind_job(\u0026job_id, \u0026best_worker.node.id).await?;\n\n        // Create result\n        let result = SchedulingResult {\n            job_id,\n            assigned_node: Some(best_worker.node.id.clone()),\n            status: SchedulingStatus::Scheduled,\n            scheduling_latency_ms: start_time.elapsed().as_millis() as u64,\n            reasoning: format!(\n                \"Selected worker {} with score {:.2}\",\n                best_worker.node.id, best_worker.score\n            ),\n            queue_position: 0,\n        };\n\n        // Update metrics\n        self.update_metrics_on_success(\u0026result, \u0026best_worker.score)\n            .await;\n\n        info!(\n            \"Successfully scheduled job {} to worker {} in {}ms\",\n            job_id, best_worker.node.id, result.scheduling_latency_ms\n        );\n\n        Ok(result)\n    }\n\n    /// Cancel a scheduled job\n    pub async fn cancel_job(\u0026self, job_id: \u0026JobId) -\u003e Result\u003c(), SchedulerError\u003e {\n        info!(\"Cancelling job {}\", job_id);\n\n        // Remove from queue if pending\n        self.queue.cancel(job_id).await?;\n\n        // TODO: Unbind from worker if scheduled\n\n        Ok(())\n    }\n\n    /// Get scheduler status\n    pub async fn get_status(\u0026self) -\u003e SchedulerStatusView {\n        let metrics = self.metrics.read().await;\n\n        SchedulerStatusView {\n            pending_jobs: self.queue.pending_count().await as u64,\n            active_schedulers: 1,\n            backend_type: self.backend.backend_type(),\n            total_scheduled: metrics.total_scheduled,\n            total_failed: metrics.total_failed,\n            avg_scheduling_latency_ms: metrics.avg_scheduling_latency_ms,\n            uptime_seconds: metrics.uptime_seconds,\n        }\n    }\n\n    /// Check if job is in queue (for testing)\n    pub async fn is_job_in_queue(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        self.queue.contains(job_id).await\n    }\n\n    /// Validate job requirements\n    fn validate_job(\u0026self, job: \u0026Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        // Check if job has valid requirements\n        if job.spec.resource_requirements.is_none() {\n            return Err(SchedulerError::InvalidJobRequirements(\n                \"Resource requirements not specified\".to_string(),\n            ));\n        }\n\n        // Validate resource values\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if let Some(cpu) = req.cpu_cores {\n                if cpu \u003c= 0.0 {\n                    return Err(SchedulerError::InvalidJobRequirements(\n                        \"CPU cores must be positive\".to_string(),\n                    ));\n                }\n            }\n\n            if let Some(memory) = req.memory_bytes {\n                if memory == 0 {\n                    return Err(SchedulerError::InvalidJobRequirements(\n                        \"Memory must be positive\".to_string(),\n                    ));\n                }\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Update metrics on successful scheduling\n    async fn update_metrics_on_success(\u0026self, result: \u0026SchedulingResult, score: \u0026f64) {\n        let mut metrics = self.metrics.write().await;\n        metrics.total_scheduled += 1;\n        metrics.avg_scheduling_latency_ms =\n            (metrics.avg_scheduling_latency_ms + result.scheduling_latency_ms) / 2;\n        metrics.avg_worker_score = (metrics.avg_worker_score + score) / 2.0;\n    }\n\n    /// Update metrics on failed scheduling\n    async fn update_metrics_on_failure(\u0026self, result: \u0026SchedulingResult) {\n        let mut metrics = self.metrics.write().await;\n        metrics.total_failed += 1;\n        metrics.avg_scheduling_latency_ms =\n            (metrics.avg_scheduling_latency_ms + result.scheduling_latency_ms) / 2;\n    }\n}\n\n/// Scheduler configuration\n#[derive(Debug, Clone)]\npub struct SchedulerConfig {\n    pub pipeline: PipelineConfig,\n    pub queue: QueueConfig,\n    pub worker_selection: WorkerSelectionStrategy,\n    pub preemption_enabled: bool,\n    pub timeout_seconds: u64,\n}\n\nimpl Default for SchedulerConfig {\n    fn default() -\u003e Self {\n        Self {\n            pipeline: PipelineConfig::default(),\n            queue: QueueConfig::default(),\n            worker_selection: WorkerSelectionStrategy::LeastLoaded,\n            preemption_enabled: true,\n            timeout_seconds: 300, // 5 minutes\n        }\n    }\n}\n\n/// Scheduling result\n#[derive(Debug, Clone)]\npub struct SchedulingResult {\n    pub job_id: JobId,\n    pub assigned_node: Option\u003cWorkerId\u003e,\n    pub status: SchedulingStatus,\n    pub scheduling_latency_ms: u64,\n    pub reasoning: String,\n    pub queue_position: usize,\n}\n\n/// Scheduling status\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SchedulingStatus {\n    Scheduled, // Successfully scheduled\n    Pending,   // Waiting in queue\n    Failed,    // Scheduling failed\n    Cancelled, // Job was cancelled\n}\n\n/// Scheduler metrics\n#[derive(Debug, Default)]\npub struct SchedulerMetrics {\n    pub total_scheduled: u64,\n    pub total_failed: u64,\n    pub avg_scheduling_latency_ms: u64,\n    pub avg_worker_score: f64,\n    pub uptime_seconds: u64,\n}\n\n/// Scheduler status view\n#[derive(Debug, Clone)]\npub struct SchedulerStatusView {\n    pub pending_jobs: u64,\n    pub active_schedulers: u32,\n    pub backend_type: BackendType,\n    pub total_scheduled: u64,\n    pub total_failed: u64,\n    pub avg_scheduling_latency_ms: u64,\n    pub uptime_seconds: u64,\n}\n\n/// Job ID type\npub type JobId = uuid::Uuid;\n\n/// Worker ID type\npub type WorkerId = uuid::Uuid;\n\n/// Custom error types\n#[derive(thiserror::Error, Debug)]\npub enum SchedulerError {\n    #[error(\"Job validation failed: {0}\")]\n    InvalidJobRequirements(String),\n\n    #[error(\"No eligible worker found for job {0}\")]\n    NoEligibleWorker(JobId),\n\n    #[error(\"Worker not found: {0}\")]\n    WorkerNotFound(WorkerId),\n\n    #[error(\"Backend error: {0}\")]\n    BackendError(String),\n\n    #[error(\"Pipeline error: {0}\")]\n    PipelineError(String),\n\n    #[error(\"Queue error: {0}\")]\n    QueueError(String),\n\n    #[error(\"Selection error: {0}\")]\n    SelectionError(String),\n\n    #[error(\"Binding error: {0}\")]\n    BindingError(String),\n\n    #[error(\"Preemption error: {0}\")]\n    PreemptionError(String),\n\n    #[error(\"Configuration error: {0}\")]\n    ConfigurationError(String),\n\n    #[error(\"Timeout error\")]\n    Timeout,\n\n    #[error(\"Internal error: {0}\")]\n    Internal(#[from] anyhow::Error),\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{ComputeResource, SchedulerBackend};\n    use crate::types::{BackendType, Job, JobMetadata, JobSpec, ResourceRequirements, WorkerNode};\n    use chrono::Utc;\n    use std::collections::HashMap;\n\n    /// Mock backend for testing\n    struct MockBackend {\n        workers: Vec\u003cWorkerNode\u003e,\n    }\n\n    impl MockBackend {\n        fn new() -\u003e Self {\n            Self {\n                workers: vec![WorkerNode {\n                    id: uuid::Uuid::new_v4(),\n                    backend_type: BackendType::Kubernetes,\n                    status: crate::types::WorkerStatus::Ready,\n                    resources: ComputeResource {\n                        cpu_cores: 8.0,\n                        memory_bytes: 16_000_000_000,\n                        gpu_count: 0,\n                    },\n                    labels: HashMap::new(),\n                    taints: vec![],\n                    backend_specific: crate::types::BackendSpecific::Kubernetes(\n                        crate::types::KubernetesNodeSpecific {\n                            node_name: \"node-1\".to_string(),\n                            namespace: \"default\".to_string(),\n                        },\n                    ),\n                    location: crate::types::NodeLocation::default(),\n                }],\n            }\n        }\n    }\n\n    #[async_trait::async_trait]\n    impl SchedulerBackend for MockBackend {\n        fn backend_type(\u0026self) -\u003e BackendType {\n            BackendType::Kubernetes\n        }\n\n        async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n            Ok(self.workers.clone())\n        }\n\n        async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .cloned()\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n\n        async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n            info!(\"Mock binding job {} to node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn unbind_job(\n            \u0026self,\n            job_id: \u0026JobId,\n            node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), SchedulerError\u003e {\n            info!(\"Mock unbinding job {} from node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn get_node_status(\n            \u0026self,\n            id: \u0026WorkerId,\n        ) -\u003e Result\u003ccrate::types::WorkerStatus, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .map(|w| w.status.clone())\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_creation() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config);\n        assert!(scheduler.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_scheduling_job() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config).unwrap();\n\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(4_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let result = scheduler.schedule_job(job.clone()).await;\n        assert!(result.is_ok());\n\n        let scheduling_result = result.unwrap();\n        // Job se agrega a la queue para procesamiento posterior\n        // No se schedulea inmediatamente en esta implementacin\n        assert_eq!(scheduling_result.status, SchedulingStatus::Pending);\n        assert!(scheduling_result.assigned_node.is_none());\n\n        // Verificar que el job est en la cola\n        assert!(scheduler.is_job_in_queue(\u0026job.metadata.id).await);\n    }\n\n    #[tokio::test]\n    async fn test_job_validation() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config).unwrap();\n\n        // Job without resource requirements should fail\n        let job = Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: None,\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let result = scheduler.schedule_job(job).await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_status() {\n        let backend = Arc::new(MockBackend::new());\n        let config = SchedulerConfig::default();\n        let scheduler = Scheduler::new(backend, config).unwrap();\n\n        let status = scheduler.get_status().await;\n        assert_eq!(status.pending_jobs, 0);\n        assert_eq!(status.active_schedulers, 1);\n        assert_eq!(status.backend_type, BackendType::Kubernetes);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","multi_sched","mod.rs"],"content":"//! Multiple Schedulers Support Module\n//!\n//! This module implements support for multiple scheduler instances that can run\n//! simultaneously with different configurations, scopes, and specializations.\n\nuse crate::SchedulerError;\nuse crate::backend::SchedulerBackend;\nuse crate::pipeline::SchedulingPipeline;\nuse crate::queue::{PriorityQueue, QueueConfig, QueueStrategy};\nuse crate::selection::{SelectionStrategy, WorkerSelector};\nuse crate::types::*;\nuse std::collections::HashMap;\nuse std::sync::Arc;\n\n/// Multiple scheduler configuration\n#[derive(Debug, Clone)]\npub struct MultiSchedulerConfig {\n    /// Default scheduler name\n    pub default_scheduler: String,\n    /// Whether to enable automatic fallback\n    pub enable_fallback: bool,\n    /// Maximum number of schedulers allowed\n    pub max_schedulers: usize,\n    /// Scheduler health check interval\n    pub health_check_interval_ms: u64,\n}\n\n/// Scheduler instance definition\n#[derive(Debug, Clone)]\npub struct SchedulerInstance {\n    /// Unique scheduler name\n    pub name: String,\n    /// Scheduler configuration\n    pub config: SchedulerInstanceConfig,\n    /// Backend to use\n    pub backend: Arc\u003cdyn SchedulerBackend\u003e,\n    /// Pipeline for job processing\n    pub pipeline: SchedulingPipeline,\n    /// Selection strategy\n    pub selector: WorkerSelector,\n    /// Queue for jobs\n    pub queue: PriorityQueue,\n    /// Whether this scheduler is enabled\n    pub enabled: bool,\n    /// Current status\n    pub status: SchedulerInstanceStatus,\n}\n\n/// Scheduler instance configuration\n#[derive(Debug, Clone)]\npub struct SchedulerInstanceConfig {\n    /// Queue strategy\n    pub queue_strategy: QueueStrategy,\n    /// Selection strategy\n    pub selection_strategy: SelectionStrategy,\n    /// Scheduler scope (cluster-wide, namespace-specific, etc.)\n    pub scope: SchedulerScope,\n    /// Specialization tags (e.g., \"gpu\", \"high-priority\", \"batch\")\n    pub specializations: Vec\u003cString\u003e,\n    /// Priority threshold (only handle jobs above this priority)\n    pub priority_threshold: Option\u003cJobPriority\u003e,\n    /// Namespace filter (if namespace-scoped)\n    pub namespace_filter: Option\u003cString\u003e,\n}\n\n/// Scheduler scope defines where this scheduler operates\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SchedulerScope {\n    /// Cluster-wide scheduler\n    ClusterWide,\n    /// Namespace-specific scheduler\n    Namespace(String),\n    /// Custom scope\n    Custom(String),\n}\n\n/// Scheduler instance status\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SchedulerInstanceStatus {\n    /// Scheduler is initializing\n    Initializing,\n    /// Scheduler is running normally\n    Running,\n    /// Scheduler is in degraded mode\n    Degraded,\n    /// Scheduler is offline\n    Offline,\n    /// Scheduler has failed\n    Failed(String),\n}\n\n/// Job routing result\n#[derive(Debug, Clone)]\npub struct RoutingResult {\n    /// Selected scheduler name\n    pub selected_scheduler: String,\n    /// Whether routing was successful\n    pub success: bool,\n    /// Reason for routing decision\n    pub reason: String,\n}\n\n/// Scheduler registry for managing multiple instances\npub struct SchedulerRegistry {\n    /// Registry configuration\n    config: MultiSchedulerConfig,\n    /// Active scheduler instances\n    schedulers: HashMap\u003cString, SchedulerInstance\u003e,\n    /// Default scheduler name\n    default_scheduler: String,\n}\n\nimpl SchedulerRegistry {\n    /// Create new scheduler registry\n    pub fn new(config: MultiSchedulerConfig) -\u003e Self {\n        Self {\n            schedulers: HashMap::new(),\n            default_scheduler: config.default_scheduler.clone(),\n            config,\n        }\n    }\n\n    /// Register a new scheduler instance\n    pub fn register_scheduler(\n        \u0026mut self,\n        instance: SchedulerInstance,\n    ) -\u003e Result\u003c(), SchedulerError\u003e {\n        if self.schedulers.len() \u003e= self.config.max_schedulers {\n            return Err(SchedulerError::ConfigurationError(\n                \"Maximum number of schedulers reached\".to_string(),\n            ));\n        }\n\n        if self.schedulers.contains_key(\u0026instance.name) {\n            return Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' already registered\",\n                instance.name\n            )));\n        }\n\n        self.schedulers.insert(instance.name.clone(), instance);\n\n        Ok(())\n    }\n\n    /// Unregister a scheduler instance\n    pub fn unregister_scheduler(\u0026mut self, name: \u0026str) -\u003e Result\u003c(), SchedulerError\u003e {\n        if !self.schedulers.contains_key(name) {\n            return Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' not found\",\n                name\n            )));\n        }\n\n        self.schedulers.remove(name);\n\n        // Update default if necessary\n        if name == \u0026self.default_scheduler \u0026\u0026 !self.schedulers.is_empty() {\n            if let Some(first_key) = self.schedulers.keys().next() {\n                self.default_scheduler = first_key.clone();\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Route a job to an appropriate scheduler\n    pub fn route_job(\u0026self, job: \u0026Job) -\u003e RoutingResult {\n        // First, try to find a specialized scheduler\n        if let Some(specialized) = self.find_specialized_scheduler(job) {\n            return RoutingResult {\n                selected_scheduler: specialized,\n                success: true,\n                reason: \"Specialized scheduler found\".to_string(),\n            };\n        }\n\n        // If namespace-scoped scheduler exists, use it\n        if let Some(namespace_scheduler) = self.find_namespace_scheduler(job) {\n            return RoutingResult {\n                selected_scheduler: namespace_scheduler,\n                success: true,\n                reason: \"Namespace-scoped scheduler found\".to_string(),\n            };\n        }\n\n        // Use default scheduler\n        if self.schedulers.contains_key(\u0026self.default_scheduler) {\n            RoutingResult {\n                selected_scheduler: self.default_scheduler.clone(),\n                success: true,\n                reason: \"Using default scheduler\".to_string(),\n            }\n        } else {\n            RoutingResult {\n                selected_scheduler: \"\".to_string(),\n                success: false,\n                reason: \"No schedulers available\".to_string(),\n            }\n        }\n    }\n\n    /// Find a specialized scheduler for the job\n    fn find_specialized_scheduler(\u0026self, job: \u0026Job) -\u003e Option\u003cString\u003e {\n        // Check if any scheduler specializes in the job's requirements\n        for (name, scheduler) in \u0026self.schedulers {\n            if !scheduler.enabled || scheduler.status != SchedulerInstanceStatus::Running {\n                continue;\n            }\n\n            // Check specialization tags\n            let has_matching_specialization =\n                scheduler\n                    .config\n                    .specializations\n                    .iter()\n                    .any(|tag| match tag.as_str() {\n                        \"gpu\" =\u003e job\n                            .spec\n                            .resource_requirements\n                            .as_ref()\n                            .and_then(|r| r.gpu_count)\n                            .map(|gpu| gpu \u003e 0)\n                            .unwrap_or(false),\n                        \"high-priority\" =\u003e {\n                            if let Some(threshold) = \u0026scheduler.config.priority_threshold {\n                                job.spec.priority \u003e= *threshold\n                            } else {\n                                matches!(\n                                    job.spec.priority,\n                                    JobPriority::High | JobPriority::Critical\n                                )\n                            }\n                        }\n                        _ =\u003e false,\n                    });\n\n            if has_matching_specialization {\n                return Some(name.clone());\n            }\n        }\n\n        None\n    }\n\n    /// Find a namespace-scoped scheduler for the job\n    fn find_namespace_scheduler(\u0026self, job: \u0026Job) -\u003e Option\u003cString\u003e {\n        for (name, scheduler) in \u0026self.schedulers {\n            if !scheduler.enabled || scheduler.status != SchedulerInstanceStatus::Running {\n                continue;\n            }\n\n            if let SchedulerScope::Namespace(ref namespace) = scheduler.config.scope {\n                if namespace == \u0026job.metadata.namespace {\n                    return Some(name.clone());\n                }\n            }\n        }\n\n        None\n    }\n\n    /// Get a scheduler instance by name\n    pub fn get_scheduler(\u0026self, name: \u0026str) -\u003e Option\u003c\u0026SchedulerInstance\u003e {\n        self.schedulers.get(name)\n    }\n\n    /// Get all scheduler names\n    pub fn list_schedulers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.schedulers.keys().cloned().collect()\n    }\n\n    /// Check scheduler health\n    pub fn check_health(\u0026self, name: \u0026str) -\u003e Result\u003cSchedulerInstanceStatus, SchedulerError\u003e {\n        self.schedulers\n            .get(name)\n            .map(|s| s.status.clone())\n            .ok_or_else(|| {\n                SchedulerError::ConfigurationError(format!(\"Scheduler '{}' not found\", name))\n            })\n    }\n\n    /// Update scheduler status\n    pub fn update_status(\n        \u0026mut self,\n        name: \u0026str,\n        status: SchedulerInstanceStatus,\n    ) -\u003e Result\u003c(), SchedulerError\u003e {\n        if let Some(scheduler) = self.schedulers.get_mut(name) {\n            scheduler.status = status;\n            Ok(())\n        } else {\n            Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' not found\",\n                name\n            )))\n        }\n    }\n\n    /// Get the default scheduler name\n    pub fn default_scheduler(\u0026self) -\u003e \u0026str {\n        \u0026self.default_scheduler\n    }\n\n    /// Enable or disable a scheduler\n    pub fn set_enabled(\u0026mut self, name: \u0026str, enabled: bool) -\u003e Result\u003c(), SchedulerError\u003e {\n        if let Some(scheduler) = self.schedulers.get_mut(name) {\n            scheduler.enabled = enabled;\n            Ok(())\n        } else {\n            Err(SchedulerError::ConfigurationError(format!(\n                \"Scheduler '{}' not found\",\n                name\n            )))\n        }\n    }\n}\n\n/// Helper to create a default scheduler instance\npub fn create_default_scheduler(\n    name: String,\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n) -\u003e SchedulerInstance {\n    let config = SchedulerInstanceConfig {\n        queue_strategy: QueueStrategy::Priority {\n            with_preemption: true,\n            max_queue_time: None,\n        },\n        selection_strategy: SelectionStrategy::ResourceBalance,\n        scope: SchedulerScope::ClusterWide,\n        specializations: vec![],\n        priority_threshold: None,\n        namespace_filter: None,\n    };\n\n    SchedulerInstance {\n        name,\n        config,\n        backend,\n        pipeline: SchedulingPipeline::new(PipelineConfig::default()),\n        selector: WorkerSelector::new(\n            SelectionStrategy::ResourceBalance,\n            SelectionCriteria::default(),\n        ),\n        queue: PriorityQueue::new(QueueConfig::default()),\n        enabled: true,\n        status: SchedulerInstanceStatus::Running,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::MockBackend;\n\n    fn create_test_job(priority: JobPriority, namespace: String) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace,\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(2_000_000_000),\n                    gpu_count: if priority == JobPriority::Critical {\n                        Some(1)\n                    } else {\n                        None\n                    },\n                    ephemeral_storage: None,\n                }),\n                priority,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_registration() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        let backend = Arc::new(MockBackend::new());\n        let scheduler = create_default_scheduler(\"default\".to_string(), backend);\n\n        registry.register_scheduler(scheduler).unwrap();\n\n        assert_eq!(registry.list_schedulers(), vec![\"default\"]);\n    }\n\n    #[tokio::test]\n    async fn test_job_routing_default() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        let backend = Arc::new(MockBackend::new());\n        let scheduler = create_default_scheduler(\"default\".to_string(), backend);\n\n        registry.register_scheduler(scheduler).unwrap();\n\n        let job = create_test_job(JobPriority::Medium, \"default\".to_string());\n        let result = registry.route_job(\u0026job);\n\n        assert!(result.success);\n        assert_eq!(result.selected_scheduler, \"default\");\n    }\n\n    #[tokio::test]\n    async fn test_job_routing_specialized() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        // Register default scheduler\n        let backend1 = Arc::new(MockBackend::new());\n        let mut default_scheduler = create_default_scheduler(\"default\".to_string(), backend1);\n        default_scheduler.config.specializations = vec![\"general\".to_string()];\n        registry.register_scheduler(default_scheduler).unwrap();\n\n        // Register GPU scheduler\n        let backend2 = Arc::new(MockBackend::new());\n        let mut gpu_scheduler = create_default_scheduler(\"gpu\".to_string(), backend2);\n        gpu_scheduler.config.specializations = vec![\"gpu\".to_string()];\n        registry.register_scheduler(gpu_scheduler).unwrap();\n\n        // Route GPU job\n        let gpu_job = create_test_job(JobPriority::High, \"default\".to_string());\n        let result = registry.route_job(\u0026gpu_job);\n\n        assert!(result.success);\n        assert_eq!(result.selected_scheduler, \"gpu\");\n    }\n\n    #[tokio::test]\n    async fn test_namespace_routing() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        // Register namespace-scoped scheduler\n        let backend = Arc::new(MockBackend::new());\n        let mut namespace_scheduler =\n            create_default_scheduler(\"team-a-scheduler\".to_string(), backend);\n        namespace_scheduler.config.scope = SchedulerScope::Namespace(\"team-a\".to_string());\n        registry.register_scheduler(namespace_scheduler).unwrap();\n\n        // Route job from team-a namespace\n        let job = create_test_job(JobPriority::Medium, \"team-a\".to_string());\n        let result = registry.route_job(\u0026job);\n\n        assert!(result.success);\n        assert_eq!(result.selected_scheduler, \"team-a-scheduler\");\n    }\n\n    #[tokio::test]\n    async fn test_scheduler_not_found_error() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let registry = SchedulerRegistry::new(config);\n\n        let status = registry.check_health(\"nonexistent\");\n\n        assert!(status.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_duplicate_scheduler_error() {\n        let config = MultiSchedulerConfig {\n            default_scheduler: \"default\".to_string(),\n            enable_fallback: true,\n            max_schedulers: 10,\n            health_check_interval_ms: 1000,\n        };\n\n        let mut registry = SchedulerRegistry::new(config);\n\n        let backend = Arc::new(MockBackend::new());\n        let scheduler = create_default_scheduler(\"default\".to_string(), backend);\n\n        registry.register_scheduler(scheduler.clone()).unwrap();\n        let result = registry.register_scheduler(scheduler);\n\n        assert!(result.is_err());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","pipeline","mod.rs"],"content":"//! Scheduling Pipeline Module\n//!\n//! Implements the 4-phase Kubernetes-style scheduling pipeline:\n//! 1. INFORM: Watch and discover available workers\n//! 2. FILTER: Apply constraints and filter feasible workers\n//! 3. SCORE: Rank workers based on scoring criteria\n//! 4. BIND: Assign job to best worker\n\nuse crate::backend::SchedulerBackend;\nuse crate::types::*;\nuse crate::{JobId, SchedulerError, WorkerId};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tracing::{debug, info};\n\n/// Scheduling pipeline configuration\n#[derive(Debug, Clone)]\npub struct PipelineConfig {\n    pub inform_interval_ms: u64,\n    pub filter_enabled: bool,\n    pub score_enabled: bool,\n    pub max_candidates: usize,\n}\n\nimpl Default for PipelineConfig {\n    fn default() -\u003e Self {\n        Self {\n            inform_interval_ms: 1000, // 1 second\n            filter_enabled: true,\n            score_enabled: true,\n            max_candidates: 100, // Limit to prevent performance issues\n        }\n    }\n}\n\n/// Main scheduling pipeline\npub struct SchedulingPipeline {\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n    config: PipelineConfig,\n    informer: Informer,\n}\n\nimpl SchedulingPipeline {\n    /// Create new scheduling pipeline\n    pub fn new(config: PipelineConfig) -\u003e Self {\n        Self {\n            backend: Arc::new(crate::backend::KubernetesBackend::new()),\n            config: config.clone(),\n            informer: Informer::new(config.inform_interval_ms),\n        }\n    }\n\n    /// Set backend (used for testing)\n    pub fn set_backend(\u0026mut self, backend: Arc\u003cdyn SchedulerBackend\u003e) {\n        self.backend = backend;\n    }\n\n    /// Phase 1: Inform - Get available workers\n    pub async fn inform(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        debug!(\"Phase 1: Inform - discovering available workers\");\n\n        let workers = self.backend.list_nodes().await?;\n\n        // Filter only available workers\n        let available_workers: Vec\u003c_\u003e = workers\n            .into_iter()\n            .filter(|w| w.status.is_available())\n            .collect();\n\n        debug!(\n            \"Phase 1: Found {} available workers\",\n            available_workers.len()\n        );\n\n        Ok(available_workers)\n    }\n\n    /// Phase 2: Filter - Apply constraints and filter feasible workers\n    pub async fn filter(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n        debug!(\"Phase 2: Filter - filtering feasible workers\");\n\n        if !self.config.filter_enabled {\n            debug!(\"Phase 2: Filtering disabled, returning all workers\");\n            return Ok(workers);\n        }\n\n        let total_workers = workers.len();\n        let mut feasible_workers = Vec::new();\n\n        for worker in workers {\n            if self.worker_matches_job(\u0026worker, job) {\n                feasible_workers.push(worker);\n            }\n\n            // Limit candidates to prevent performance issues\n            if feasible_workers.len() \u003e= self.config.max_candidates {\n                debug!(\n                    \"Phase 2: Reached max candidates limit ({})\",\n                    self.config.max_candidates\n                );\n                break;\n            }\n        }\n\n        debug!(\n            \"Phase 2: Filtered to {} feasible workers out of {}\",\n            feasible_workers.len(),\n            total_workers\n        );\n\n        Ok(feasible_workers)\n    }\n\n    /// Phase 3: Score - Rank workers based on scoring criteria\n    pub async fn score(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cVec\u003cScoredWorker\u003e, SchedulerError\u003e {\n        debug!(\"Phase 3: Score - ranking workers\");\n\n        if !self.config.score_enabled {\n            debug!(\"Phase 3: Scoring disabled, assigning equal scores\");\n            return Ok(workers\n                .into_iter()\n                .map(|w| ScoredWorker::new(w, 50.0))\n                .collect());\n        }\n\n        let scoring_weights = ScoringWeights::default();\n\n        let mut scored_workers: Vec\u003c_\u003e = workers\n            .into_iter()\n            .map(|worker| {\n                let score = worker.calculate_score(job, \u0026scoring_weights);\n                ScoredWorker::new(worker, score)\n            })\n            .collect();\n\n        // Sort by score descending\n        scored_workers.sort_by(|a, b| {\n            b.score\n                .partial_cmp(\u0026a.score)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n\n        debug!(\n            \"Phase 3: Scored {} workers, best score: {:.2}\",\n            scored_workers.len(),\n            scored_workers.first().map(|s| s.score).unwrap_or(0.0)\n        );\n\n        Ok(scored_workers)\n    }\n\n    /// Phase 4: Bind - Job is already assigned in schedule_job\n    /// This is a placeholder for the binding phase\n    pub async fn bind(\u0026self, job_id: \u0026JobId, worker_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        debug!(\n            \"Phase 4: Bind - binding job {} to worker {}\",\n            job_id, worker_id\n        );\n\n        // Actual binding is done by the backend in the main scheduler\n        // This is here for completeness of the pipeline\n        info!(\"Job {} bound to worker {}\", job_id, worker_id);\n\n        Ok(())\n    }\n\n    /// Check if worker matches job requirements\n    fn worker_matches_job(\u0026self, worker: \u0026WorkerNode, job: \u0026Job) -\u003e bool {\n        // Check resources\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if !worker.resources.has_resources(req) {\n                return false;\n            }\n        }\n\n        // Check node selector\n        if let Some(selector) = \u0026job.spec.node_selector {\n            for (key, value) in \u0026selector.labels {\n                match worker.labels.get(key) {\n                    Some(node_value) =\u003e {\n                        if node_value != value {\n                            return false;\n                        }\n                    }\n                    None =\u003e return false,\n                }\n            }\n        }\n\n        // Check taints and tolerations\n        for taint in \u0026worker.taints {\n            let has_matching_toleration = job.spec.tolerations.iter().any(|tol| {\n                tol.key == taint.key \u0026\u0026 tol.value == taint.value \u0026\u0026 tol.effect == taint.effect\n            });\n\n            // If there's no matching toleration and effect is NoSchedule, reject\n            if !has_matching_toleration \u0026\u0026 taint.effect == TaintEffect::NoSchedule {\n                return false;\n            }\n        }\n\n        // Check affinity rules (simplified)\n        if let Some(affinity) = \u0026job.spec.affinity {\n            // In production: implement full affinity matching logic\n            // For now: assume all nodes match\n        }\n\n        true\n    }\n}\n\n/// Informer component for watching cluster state\n#[derive(Debug)]\npub struct Informer {\n    interval_ms: u64,\n    last_sync: Arc\u003cstd::sync::Mutex\u003cstd::time::Instant\u003e\u003e,\n}\n\nimpl Informer {\n    pub fn new(interval_ms: u64) -\u003e Self {\n        Self {\n            interval_ms,\n            last_sync: Arc::new(std::sync::Mutex::new(std::time::Instant::now())),\n        }\n    }\n\n    /// Check if informer should sync\n    pub fn should_sync(\u0026self) -\u003e bool {\n        let last_sync = self.last_sync.lock().unwrap();\n        last_sync.elapsed().as_millis() \u003e= self.interval_ms as u128\n    }\n\n    /// Update last sync time\n    pub fn update_sync(\u0026self) {\n        let mut last_sync = self.last_sync.lock().unwrap();\n        *last_sync = std::time::Instant::now();\n    }\n}\n\n/// Filter stage of the pipeline\n#[derive(Debug)]\npub struct FilterStage {\n    enabled: bool,\n}\n\nimpl FilterStage {\n    pub fn new(enabled: bool) -\u003e Self {\n        Self { enabled }\n    }\n\n    /// Apply filters to workers\n    pub fn apply(\u0026self, job: \u0026Job, workers: Vec\u003cWorkerNode\u003e) -\u003e Vec\u003cWorkerNode\u003e {\n        if !self.enabled {\n            return workers;\n        }\n\n        workers\n            .into_iter()\n            .filter(|w| self.passes_filters(w, job))\n            .collect()\n    }\n\n    /// Check if worker passes all filters\n    fn passes_filters(\u0026self, worker: \u0026WorkerNode, job: \u0026Job) -\u003e bool {\n        // Resource filter\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if !worker.resources.has_resources(req) {\n                return false;\n            }\n        }\n\n        // Node selector filter\n        if let Some(selector) = \u0026job.spec.node_selector {\n            for (key, value) in \u0026selector.labels {\n                match worker.labels.get(key) {\n                    Some(node_value) =\u003e {\n                        if node_value != value {\n                            return false;\n                        }\n                    }\n                    None =\u003e return false,\n                }\n            }\n        }\n\n        // Taint/toleration filter\n        for taint in \u0026worker.taints {\n            let has_matching_toleration = job.spec.tolerations.iter().any(|tol| {\n                tol.key == taint.key \u0026\u0026 tol.value == taint.value \u0026\u0026 tol.effect == taint.effect\n            });\n\n            if !has_matching_toleration \u0026\u0026 taint.effect == TaintEffect::NoSchedule {\n                return false;\n            }\n        }\n\n        true\n    }\n}\n\n/// Scoring stage of the pipeline\n#[derive(Debug)]\npub struct ScoreStage {\n    weights: ScoringWeights,\n}\n\nimpl ScoreStage {\n    pub fn new(weights: ScoringWeights) -\u003e Self {\n        Self { weights }\n    }\n\n    /// Score workers\n    pub fn apply(\u0026self, job: \u0026Job, workers: Vec\u003cWorkerNode\u003e) -\u003e Vec\u003cScoredWorker\u003e {\n        workers\n            .into_iter()\n            .map(|worker| {\n                let score = worker.calculate_score(job, \u0026self.weights);\n                ScoredWorker::new(worker, score)\n            })\n            .collect()\n    }\n}\n\n/// Bind stage of the pipeline\npub struct BindStage {\n    backend: Arc\u003cdyn SchedulerBackend\u003e,\n}\n\nimpl BindStage {\n    pub fn new(backend: Arc\u003cdyn SchedulerBackend\u003e) -\u003e Self {\n        Self { backend }\n    }\n\n    /// Bind job to worker\n    pub async fn bind(\u0026self, job_id: \u0026JobId, worker_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n        self.backend.bind_job(job_id, worker_id).await\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{ComputeResource, SchedulerBackend};\n    use crate::types::WorkerNode;\n    use std::collections::HashMap;\n\n    struct MockBackend {\n        workers: Vec\u003cWorkerNode\u003e,\n    }\n\n    impl MockBackend {\n        fn new(workers: Vec\u003cWorkerNode\u003e) -\u003e Self {\n            Self { workers }\n        }\n    }\n\n    #[async_trait::async_trait]\n    impl SchedulerBackend for MockBackend {\n        fn backend_type(\u0026self) -\u003e crate::types::BackendType {\n            crate::types::BackendType::Kubernetes\n        }\n\n        async fn list_nodes(\u0026self) -\u003e Result\u003cVec\u003cWorkerNode\u003e, SchedulerError\u003e {\n            Ok(self.workers.clone())\n        }\n\n        async fn get_node(\u0026self, id: \u0026WorkerId) -\u003e Result\u003cWorkerNode, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .cloned()\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n\n        async fn bind_job(\u0026self, job_id: \u0026JobId, node_id: \u0026WorkerId) -\u003e Result\u003c(), SchedulerError\u003e {\n            tracing::info!(\"Mock binding job {} to node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn unbind_job(\n            \u0026self,\n            job_id: \u0026JobId,\n            node_id: \u0026WorkerId,\n        ) -\u003e Result\u003c(), SchedulerError\u003e {\n            tracing::info!(\"Mock unbinding job {} from node {}\", job_id, node_id);\n            Ok(())\n        }\n\n        async fn get_node_status(\n            \u0026self,\n            id: \u0026WorkerId,\n        ) -\u003e Result\u003ccrate::types::WorkerStatus, SchedulerError\u003e {\n            self.workers\n                .iter()\n                .find(|w| \u0026w.id == id)\n                .map(|w| w.status.clone())\n                .ok_or_else(|| SchedulerError::WorkerNotFound(*id))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_pipeline_inform() {\n        let workers = vec![WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: crate::types::BackendType::Kubernetes,\n            status: crate::types::WorkerStatus::Ready,\n            resources: ComputeResource {\n                cpu_cores: 8.0,\n                memory_bytes: 16_000_000_000,\n                gpu_count: 2,\n            },\n            labels: HashMap::new(),\n            taints: vec![],\n            backend_specific: crate::types::BackendSpecific::Kubernetes(\n                crate::types::KubernetesNodeSpecific {\n                    node_name: \"node-1\".to_string(),\n                    namespace: \"default\".to_string(),\n                },\n            ),\n            location: crate::types::NodeLocation::default(),\n        }];\n\n        let backend = Arc::new(MockBackend::new(workers));\n        let config = PipelineConfig::default();\n        let mut pipeline = SchedulingPipeline::new(config);\n        pipeline.set_backend(backend);\n\n        let available_workers = pipeline.inform().await.unwrap();\n        assert_eq!(available_workers.len(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_pipeline_filter() {\n        let workers = vec![\n            WorkerNode {\n                id: uuid::Uuid::new_v4(),\n                backend_type: crate::types::BackendType::Kubernetes,\n                status: crate::types::WorkerStatus::Ready,\n                resources: ComputeResource {\n                    cpu_cores: 8.0,\n                    memory_bytes: 16_000_000_000,\n                    gpu_count: 2,\n                },\n                labels: HashMap::new(),\n                taints: vec![],\n                backend_specific: crate::types::BackendSpecific::Kubernetes(\n                    crate::types::KubernetesNodeSpecific {\n                        node_name: \"node-1\".to_string(),\n                        namespace: \"default\".to_string(),\n                    },\n                ),\n                location: crate::types::NodeLocation::default(),\n            },\n            WorkerNode {\n                id: uuid::Uuid::new_v4(),\n                backend_type: crate::types::BackendType::Kubernetes,\n                status: crate::types::WorkerStatus::Offline,\n                resources: ComputeResource {\n                    cpu_cores: 8.0,\n                    memory_bytes: 16_000_000_000,\n                    gpu_count: 2,\n                },\n                labels: HashMap::new(),\n                taints: vec![],\n                backend_specific: crate::types::BackendSpecific::Kubernetes(\n                    crate::types::KubernetesNodeSpecific {\n                        node_name: \"node-2\".to_string(),\n                        namespace: \"default\".to_string(),\n                    },\n                ),\n                location: crate::types::NodeLocation::default(),\n            },\n        ];\n\n        let backend = Arc::new(MockBackend::new(workers));\n        let config = PipelineConfig::default();\n        let mut pipeline = SchedulingPipeline::new(config);\n        pipeline.set_backend(backend);\n\n        let job = crate::types::Job {\n            metadata: crate::types::JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: crate::types::JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(4.0),\n                    memory_bytes: Some(8_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let available_workers = pipeline.inform().await.unwrap();\n        let filtered_workers = pipeline.filter(\u0026job, available_workers).await.unwrap();\n\n        // Only 1 worker should be available (Ready status)\n        assert_eq!(filtered_workers.len(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_pipeline_score() {\n        let workers = vec![WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: crate::types::BackendType::Kubernetes,\n            status: crate::types::WorkerStatus::Ready,\n            resources: ComputeResource {\n                cpu_cores: 8.0,\n                memory_bytes: 16_000_000_000,\n                gpu_count: 2,\n            },\n            labels: HashMap::new(),\n            taints: vec![],\n            backend_specific: crate::types::BackendSpecific::Kubernetes(\n                crate::types::KubernetesNodeSpecific {\n                    node_name: \"node-1\".to_string(),\n                    namespace: \"default\".to_string(),\n                },\n            ),\n            location: crate::types::NodeLocation::default(),\n        }];\n\n        let backend = Arc::new(MockBackend::new(workers));\n        let config = PipelineConfig::default();\n        let mut pipeline = SchedulingPipeline::new(config);\n        pipeline.set_backend(backend);\n\n        let job = crate::types::Job {\n            metadata: crate::types::JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: crate::types::JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(4.0),\n                    memory_bytes: Some(8_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: crate::types::JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        };\n\n        let available_workers = pipeline.inform().await.unwrap();\n        let filtered_workers = pipeline.filter(\u0026job, available_workers).await.unwrap();\n        let scored_workers = pipeline.score(\u0026job, filtered_workers).await.unwrap();\n\n        assert_eq!(scored_workers.len(), 1);\n        assert!(scored_workers[0].score \u003e 0.0);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","queue","mod.rs"],"content":"//! Priority Queue Module with Preemption Support\n//!\n//! This module implements multiple job queue strategies similar to Kubernetes:\n//! - Priority Queue with preemption support\n//! - Simple FIFO queue\n//! - Fair Queuing across tenants\n\nuse crate::types::*;\nuse crate::{JobId, SchedulerError};\nuse chrono::Utc;\nuse std::cmp::Ordering;\nuse std::collections::{BinaryHeap, HashMap};\nuse std::sync::{Arc, Mutex};\nuse tracing::{debug, info};\n\n/// Queue strategy types\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum QueueStrategy {\n    /// Simple FIFO (First In, First Out)\n    Fifo,\n\n    /// Priority queue with optional preemption\n    Priority {\n        with_preemption: bool,\n        max_queue_time: Option\u003cchrono::Duration\u003e,\n    },\n\n    /// Fair queuing distributed across tenants\n    Fair {\n        tenant_key: String,\n        weights: HashMap\u003cString, u32\u003e,\n        quantum: Option\u003cchrono::Duration\u003e,\n    },\n}\n\nimpl Default for QueueStrategy {\n    fn default() -\u003e Self {\n        QueueStrategy::Priority {\n            with_preemption: true,\n            max_queue_time: None,\n        }\n    }\n}\n\n/// Queue configuration\n#[derive(Debug, Clone)]\npub struct QueueConfig {\n    pub max_size: usize,\n    pub strategy: QueueStrategy,\n    pub fairness_enabled: bool,\n    pub namespace: Option\u003cString\u003e,\n}\n\nimpl Default for QueueConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_size: 10_000,\n            strategy: QueueStrategy::default(),\n            fairness_enabled: false,\n            namespace: None,\n        }\n    }\n}\n\n/// Priority queue implementation\n#[derive(Debug)]\npub struct PriorityQueue {\n    inner: Arc\u003cMutex\u003cPriorityQueueInner\u003e\u003e,\n}\n\n#[derive(Debug)]\nstruct PriorityQueueInner {\n    queue: BinaryHeap\u003cQueueEntry\u003e,\n    job_map: HashMap\u003cJobId, QueueEntry\u003e,\n    position_map: HashMap\u003cJobId, usize\u003e,\n    config: QueueConfig,\n}\n\n#[derive(Debug, Clone)]\nstruct QueueEntry {\n    job_id: JobId,\n    priority: JobPriority,\n    enqueue_time: chrono::DateTime\u003cUtc\u003e,\n    name: String,\n    namespace: String,\n}\n\n/// Ordering for priority queue (higher priority comes first)\nimpl PartialOrd for QueueEntry {\n    fn partial_cmp(\u0026self, other: \u0026Self) -\u003e Option\u003cOrdering\u003e {\n        Some(self.cmp(other))\n    }\n}\n\nimpl Ord for QueueEntry {\n    fn cmp(\u0026self, other: \u0026Self) -\u003e Ordering {\n        // Higher priority first\n        if self.priority != other.priority {\n            return self.priority.cmp(\u0026other.priority);\n        }\n\n        // Earlier enqueue time first (FIFO for same priority)\n        self.enqueue_time.cmp(\u0026other.enqueue_time)\n    }\n}\n\nimpl PartialEq for QueueEntry {\n    fn eq(\u0026self, other: \u0026Self) -\u003e bool {\n        self.priority == other.priority\n            \u0026\u0026 self.enqueue_time == other.enqueue_time\n            \u0026\u0026 self.job_id == other.job_id\n    }\n}\n\nimpl Eq for QueueEntry {}\n\nimpl PriorityQueue {\n    /// Create new priority queue\n    pub fn new(config: QueueConfig) -\u003e Self {\n        Self {\n            inner: Arc::new(Mutex::new(PriorityQueueInner {\n                queue: BinaryHeap::new(),\n                job_map: HashMap::new(),\n                position_map: HashMap::new(),\n                config,\n            })),\n        }\n    }\n\n    /// Enqueue a job\n    pub async fn enqueue(\u0026self, job: Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        // Check queue capacity\n        if inner.queue.len() \u003e= inner.config.max_size {\n            return Err(SchedulerError::QueueError(\n                \"Queue is at maximum capacity\".to_string(),\n            ));\n        }\n\n        let entry = QueueEntry {\n            job_id: job.metadata.id,\n            priority: job.spec.priority.clone(),\n            enqueue_time: job.metadata.created_at,\n            name: job.metadata.name.clone(),\n            namespace: job.metadata.namespace.clone(),\n        };\n\n        inner.queue.push(entry.clone());\n        inner.job_map.insert(job.metadata.id, entry);\n\n        // Update position map\n        let position = inner.queue.len() - 1;\n        inner.position_map.insert(job.metadata.id, position);\n\n        debug!(\n            \"Enqueued job {} with priority {:?} (position: {})\",\n            job.metadata.id, job.spec.priority, position\n        );\n\n        Ok(())\n    }\n\n    /// Dequeue the highest priority job\n    pub async fn dequeue(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if let Some(entry) = inner.queue.pop() {\n            inner.job_map.remove(\u0026entry.job_id);\n            inner.position_map.remove(\u0026entry.job_id);\n\n            debug!(\"Dequeued job {}\", entry.job_id);\n\n            Some(entry.job_id)\n        } else {\n            None\n        }\n    }\n\n    /// Get the next job without removing it\n    pub async fn peek(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let inner = self.inner.lock().unwrap();\n        inner.queue.peek().map(|entry| entry.job_id)\n    }\n\n    /// Cancel a job (remove from queue)\n    pub async fn cancel(\u0026self, job_id: \u0026JobId) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if !inner.job_map.contains_key(job_id) {\n            return Err(SchedulerError::QueueError(format!(\n                \"Job {} not found in queue\",\n                job_id\n            )));\n        }\n\n        // Remove from structures\n        inner.job_map.remove(job_id);\n        inner.position_map.remove(job_id);\n\n        // Rebuild queue without the job\n        // This is O(n) but necessary for BinaryHeap removal\n        let entries: Vec\u003c_\u003e = inner\n            .queue\n            .drain()\n            .filter(|e| e.job_id != *job_id)\n            .collect();\n\n        for entry in \u0026entries {\n            inner.queue.push(entry.clone());\n        }\n\n        for entry in \u0026entries {\n            inner.job_map.insert(entry.job_id, entry.clone());\n            let position = inner\n                .queue\n                .iter()\n                .position(|e| e.job_id == entry.job_id)\n                .unwrap_or(0);\n            inner.position_map.insert(entry.job_id, position);\n        }\n\n        info!(\"Cancelled job {}\", job_id);\n\n        Ok(())\n    }\n\n    /// Check if job is in queue\n    pub async fn contains(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        let inner = self.inner.lock().unwrap();\n        inner.job_map.contains_key(job_id)\n    }\n\n    /// Get queue position of a job\n    pub async fn position(\u0026self, job_id: \u0026JobId) -\u003e Option\u003cusize\u003e {\n        let inner = self.inner.lock().unwrap();\n        inner.position_map.get(job_id).copied()\n    }\n\n    /// Get pending job count\n    pub async fn pending_count(\u0026self) -\u003e usize {\n        let inner = self.inner.lock().unwrap();\n        inner.queue.len()\n    }\n\n    /// Get all pending jobs\n    pub async fn list_pending(\u0026self) -\u003e Vec\u003cQueueEntryView\u003e {\n        let inner = self.inner.lock().unwrap();\n        inner\n            .queue\n            .iter()\n            .map(|entry| QueueEntryView {\n                job_id: entry.job_id,\n                priority: entry.priority.clone(),\n                enqueue_time: entry.enqueue_time,\n                name: entry.name.clone(),\n                namespace: entry.namespace.clone(),\n            })\n            .collect()\n    }\n\n    /// Preemption: check if high-priority job can preempt others\n    pub async fn check_preemption(\u0026self, job: \u0026Job) -\u003e Result\u003cVec\u003cJobId\u003e, SchedulerError\u003e {\n        if let QueueStrategy::Priority {\n            with_preemption, ..\n        } = \u0026self.inner.lock().unwrap().config.strategy\n        {\n            if !with_preemption {\n                return Ok(vec![]);\n            }\n        } else {\n            // Preemption only supported for Priority strategy\n            return Ok(vec![]);\n        }\n\n        let mut inner = self.inner.lock().unwrap();\n        let mut preempted = Vec::new();\n\n        // Find lower priority jobs that can be preempted\n        let mut preemptable_jobs: Vec\u003c_\u003e = inner\n            .queue\n            .iter()\n            .filter(|entry| job.spec.priority.can_preempt(\u0026entry.priority))\n            .cloned()\n            .collect();\n\n        // Sort by priority (lowest first) to preempt lowest priority jobs first\n        preemptable_jobs.sort_by(|a, b| {\n            b.priority.cmp(\u0026a.priority) // Reverse order - lowest priority last\n        });\n\n        // Select jobs to preempt\n        // For simplicity, preempt all lower priority jobs\n        // In production, you'd preempt only as many as needed\n        for entry in preemptable_jobs {\n            preempted.push(entry.job_id);\n        }\n\n        if !preempted.is_empty() {\n            info!(\n                \"Preemption: job {} (priority {:?}) can preempt {} jobs\",\n                job.metadata.id,\n                job.spec.priority,\n                preempted.len()\n            );\n        }\n\n        Ok(preempted)\n    }\n\n    /// Execute preemption\n    pub async fn preempt(\u0026self, preempting_job: \u0026Job) -\u003e Result\u003cVec\u003cJobId\u003e, SchedulerError\u003e {\n        let preempted_jobs = self.check_preemption(preempting_job).await?;\n\n        if preempted_jobs.is_empty() {\n            return Ok(vec![]);\n        }\n\n        let mut inner = self.inner.lock().unwrap();\n\n        // Remove preempted jobs\n        for job_id in \u0026preempted_jobs {\n            inner.job_map.remove(job_id);\n            inner.position_map.remove(job_id);\n\n            // Remove from queue\n            let remaining: Vec\u003c_\u003e = inner\n                .queue\n                .drain()\n                .filter(|e| e.job_id != *job_id)\n                .collect();\n\n            inner.queue.clear();\n            for entry in \u0026remaining {\n                inner.queue.push(entry.clone());\n            }\n\n            info!(\"Preempted job {}\", job_id);\n        }\n\n        // Add the preempting job\n        let entry = QueueEntry {\n            job_id: preempting_job.metadata.id,\n            priority: preempting_job.spec.priority.clone(),\n            enqueue_time: preempting_job.metadata.created_at,\n            name: preempting_job.metadata.name.clone(),\n            namespace: preempting_job.metadata.namespace.clone(),\n        };\n\n        inner.queue.push(entry.clone());\n        inner.job_map.insert(preempting_job.metadata.id, entry);\n        let position = inner.queue.len() - 1;\n        inner\n            .position_map\n            .insert(preempting_job.metadata.id, position);\n\n        Ok(preempted_jobs)\n    }\n\n    /// Clear the queue\n    pub async fn clear(\u0026self) {\n        let mut inner = self.inner.lock().unwrap();\n        inner.queue.clear();\n        inner.job_map.clear();\n        inner.position_map.clear();\n    }\n}\n\n/// Queue entry view for external access\n#[derive(Debug, Clone)]\npub struct QueueEntryView {\n    pub job_id: JobId,\n    pub priority: JobPriority,\n    pub enqueue_time: chrono::DateTime\u003cUtc\u003e,\n    pub name: String,\n    pub namespace: String,\n}\n\n/// Queue statistics\n#[derive(Debug, Clone)]\npub struct QueueStats {\n    pub total_jobs: usize,\n    pub priority_counts: HashMap\u003cJobPriority, usize\u003e,\n    pub oldest_job_age: Option\u003cchrono::Duration\u003e,\n    pub newest_job_age: Option\u003cchrono::Duration\u003e,\n    pub tenant_distribution: HashMap\u003cString, usize\u003e,\n}\n\nimpl PriorityQueue {\n    /// Get queue statistics\n    pub async fn stats(\u0026self) -\u003e QueueStats {\n        let inner = self.inner.lock().unwrap();\n\n        let mut priority_counts = HashMap::new();\n        let mut tenant_distribution = HashMap::new();\n        let mut oldest_time = None;\n        let mut newest_time = None;\n\n        for entry in \u0026inner.queue {\n            *priority_counts.entry(entry.priority.clone()).or_insert(0) += 1;\n            *tenant_distribution\n                .entry(entry.namespace.clone())\n                .or_insert(0) += 1;\n\n            if oldest_time.is_none() || entry.enqueue_time \u003c oldest_time.unwrap() {\n                oldest_time = Some(entry.enqueue_time);\n            }\n\n            if newest_time.is_none() || entry.enqueue_time \u003e newest_time.unwrap() {\n                newest_time = Some(entry.enqueue_time);\n            }\n        }\n\n        let now = Utc::now();\n        let oldest_job_age = oldest_time.map(|t| now - t);\n        let newest_job_age = newest_time.map(|t| now - t);\n\n        QueueStats {\n            total_jobs: inner.queue.len(),\n            priority_counts,\n            oldest_job_age,\n            newest_job_age,\n            tenant_distribution,\n        }\n    }\n}\n\n/// FIFO Queue implementation (simple first-in, first-out)\n#[derive(Debug)]\npub struct FifoQueue {\n    inner: Arc\u003cMutex\u003cFifoQueueInner\u003e\u003e,\n}\n\n#[derive(Debug)]\nstruct FifoQueueInner {\n    queue: Vec\u003cQueueEntry\u003e,\n    job_map: HashMap\u003cJobId, QueueEntry\u003e,\n    config: QueueConfig,\n}\n\nimpl FifoQueue {\n    /// Create new FIFO queue\n    pub fn new(config: QueueConfig) -\u003e Self {\n        Self {\n            inner: Arc::new(Mutex::new(FifoQueueInner {\n                queue: Vec::new(),\n                job_map: HashMap::new(),\n                config,\n            })),\n        }\n    }\n\n    /// Enqueue a job\n    pub async fn enqueue(\u0026self, job: Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if inner.queue.len() \u003e= inner.config.max_size {\n            return Err(SchedulerError::QueueError(\n                \"Queue is at maximum capacity\".to_string(),\n            ));\n        }\n\n        let entry = QueueEntry {\n            job_id: job.metadata.id,\n            priority: job.spec.priority,\n            enqueue_time: job.metadata.created_at,\n            name: job.metadata.name.clone(),\n            namespace: job.metadata.namespace.clone(),\n        };\n\n        inner.queue.push(entry.clone());\n        inner.job_map.insert(job.metadata.id, entry);\n\n        debug!(\"FIFO: Enqueued job {}\", job.metadata.id);\n\n        Ok(())\n    }\n\n    /// Dequeue the first job (FIFO order)\n    pub async fn dequeue(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if let Some(entry) = inner.queue.first().cloned() {\n            inner.queue.remove(0);\n            inner.job_map.remove(\u0026entry.job_id);\n\n            debug!(\"FIFO: Dequeued job {}\", entry.job_id);\n\n            Some(entry.job_id)\n        } else {\n            None\n        }\n    }\n\n    /// Check if job is in queue\n    pub async fn contains(\u0026self, job_id: \u0026JobId) -\u003e bool {\n        let inner = self.inner.lock().unwrap();\n        inner.job_map.contains_key(job_id)\n    }\n\n    /// Get pending job count\n    pub async fn pending_count(\u0026self) -\u003e usize {\n        let inner = self.inner.lock().unwrap();\n        inner.queue.len()\n    }\n\n    /// Cancel a job\n    pub async fn cancel(\u0026self, job_id: \u0026JobId) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if !inner.job_map.contains_key(job_id) {\n            return Err(SchedulerError::QueueError(format!(\n                \"Job {} not found in queue\",\n                job_id\n            )));\n        }\n\n        inner.job_map.remove(job_id);\n        inner.queue.retain(|e| e.job_id != *job_id);\n\n        info!(\"FIFO: Cancelled job {}\", job_id);\n\n        Ok(())\n    }\n\n    /// Clear the queue\n    pub async fn clear(\u0026self) {\n        let mut inner = self.inner.lock().unwrap();\n        inner.queue.clear();\n        inner.job_map.clear();\n    }\n}\n\n/// Fair Queue implementation (weighted round-robin by tenant)\n#[derive(Debug)]\npub struct FairQueue {\n    inner: Arc\u003cMutex\u003cFairQueueInner\u003e\u003e,\n}\n\n#[derive(Debug)]\nstruct FairQueueInner {\n    tenant_queues: HashMap\u003cString, BinaryHeap\u003cQueueEntry\u003e\u003e,\n    current_tenant: String,\n    weights: HashMap\u003cString, u32\u003e,\n    quantum: chrono::Duration,\n    config: QueueConfig,\n}\n\nimpl FairQueue {\n    /// Create new Fair queue\n    pub fn new(config: QueueConfig) -\u003e Result\u003cSelf, SchedulerError\u003e {\n        if let QueueStrategy::Fair {\n            tenant_key,\n            weights,\n            quantum,\n        } = \u0026config.strategy\n        {\n            Ok(Self {\n                inner: Arc::new(Mutex::new(FairQueueInner {\n                    tenant_queues: HashMap::new(),\n                    current_tenant: tenant_key.clone(),\n                    weights: weights.clone(),\n                    quantum: quantum.unwrap_or_else(|| chrono::Duration::seconds(1)),\n                    config,\n                })),\n            })\n        } else {\n            Err(SchedulerError::QueueError(\n                \"FairQueue requires Fair strategy\".to_string(),\n            ))\n        }\n    }\n\n    /// Enqueue a job\n    pub async fn enqueue(\u0026self, job: Job) -\u003e Result\u003c(), SchedulerError\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        let total_jobs: usize = inner.tenant_queues.values().map(|q| q.len()).sum();\n        if total_jobs \u003e= inner.config.max_size {\n            return Err(SchedulerError::QueueError(\n                \"Queue is at maximum capacity\".to_string(),\n            ));\n        }\n\n        let entry = QueueEntry {\n            job_id: job.metadata.id,\n            priority: job.spec.priority,\n            enqueue_time: job.metadata.created_at,\n            name: job.metadata.name.clone(),\n            namespace: job.metadata.namespace.clone(),\n        };\n\n        let tenant_key = job.metadata.namespace.clone();\n\n        inner\n            .tenant_queues\n            .entry(tenant_key.clone())\n            .or_insert_with(BinaryHeap::new)\n            .push(entry);\n\n        info!(\n            \"FairQueue: Enqueued job {} to tenant {}\",\n            job.metadata.id, tenant_key\n        );\n\n        Ok(())\n    }\n\n    /// Dequeue using weighted round-robin\n    pub async fn dequeue(\u0026self) -\u003e Option\u003cJobId\u003e {\n        let mut inner = self.inner.lock().unwrap();\n\n        if inner.tenant_queues.is_empty() {\n            return None;\n        }\n\n        // Get sorted tenants by weight and dequeued count\n        let mut tenants: Vec\u003c_\u003e = inner.tenant_queues.keys().cloned().collect();\n        tenants.sort_by(|a, b| {\n            let weight_a = inner.weights.get(a).unwrap_or(\u00261);\n            let weight_b = inner.weights.get(b).unwrap_or(\u00261);\n            weight_b.cmp(weight_a)\n        });\n\n        // Try to dequeue from each tenant in order\n        for tenant in tenants.iter() {\n            if let Some(queue) = inner.tenant_queues.get_mut(tenant) {\n                if let Some(entry) = queue.pop() {\n                    debug!(\n                        \"FairQueue: Dequeued job {} from tenant {}\",\n                        entry.job_id, tenant\n                    );\n\n                    // Update current tenant for next round\n                    inner.current_tenant = tenant.clone();\n\n                    return Some(entry.job_id);\n                }\n            }\n        }\n\n        None\n    }\n\n    /// Get pending job count\n    pub async fn pending_count(\u0026self) -\u003e usize {\n        let inner = self.inner.lock().unwrap();\n        inner.tenant_queues.values().map(|q| q.len()).sum()\n    }\n\n    /// Get queue statistics\n    pub async fn stats(\u0026self) -\u003e QueueStats {\n        let inner = self.inner.lock().unwrap();\n\n        let mut tenant_distribution = HashMap::new();\n\n        for (tenant, queue) in \u0026inner.tenant_queues {\n            tenant_distribution.insert(tenant.clone(), queue.len());\n\n            // Also calculate priority distribution\n            for entry in queue.iter() {\n                // Note: We could calculate priority_counts here too if needed\n            }\n        }\n\n        QueueStats {\n            total_jobs: inner.tenant_queues.values().map(|q| q.len()).sum(),\n            priority_counts: HashMap::new(), // Not calculated for FairQueue\n            oldest_job_age: None,\n            newest_job_age: None,\n            tenant_distribution,\n        }\n    }\n\n    /// Clear the queue\n    pub async fn clear(\u0026self) {\n        let mut inner = self.inner.lock().unwrap();\n        inner.tenant_queues.clear();\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::{Job, JobMetadata, JobSpec, ResourceRequirements};\n\n    fn create_test_job(id: u32, priority: JobPriority) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::from_u128(id as u128),\n                name: format!(\"test-job-{}\", id),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(2.0),\n                    memory_bytes: Some(4_000_000_000),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_queue_enqueue_dequeue() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        let job1 = create_test_job(1, JobPriority::Medium);\n        let job2 = create_test_job(2, JobPriority::High);\n\n        queue.enqueue(job1.clone()).await.unwrap();\n        queue.enqueue(job2.clone()).await.unwrap();\n\n        assert_eq!(queue.pending_count().await, 2);\n\n        // High priority job should be dequeued first\n        let first = queue.dequeue().await;\n        assert_eq!(first, Some(job2.metadata.id));\n\n        let second = queue.dequeue().await;\n        assert_eq!(second, Some(job1.metadata.id));\n    }\n\n    #[tokio::test]\n    async fn test_queue_priority_ordering() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        // Add jobs in random order\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::High))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(3, JobPriority::Medium))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(4, JobPriority::Critical))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(5, JobPriority::Batch))\n            .await\n            .unwrap();\n\n        assert_eq!(queue.pending_count().await, 5);\n\n        // Dequeue and verify priority order\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(4)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(2)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(3)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(1)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(5)));\n    }\n\n    #[tokio::test]\n    async fn test_queue_cancel() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        let job1 = create_test_job(1, JobPriority::Medium);\n        queue.enqueue(job1.clone()).await.unwrap();\n\n        assert!(queue.contains(\u0026job1.metadata.id).await);\n\n        queue.cancel(\u0026job1.metadata.id).await.unwrap();\n\n        assert!(!queue.contains(\u0026job1.metadata.id).await);\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_queue_position() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        let job1 = create_test_job(1, JobPriority::Low);\n        let job2 = create_test_job(2, JobPriority::High);\n\n        queue.enqueue(job1.clone()).await.unwrap();\n        queue.enqueue(job2.clone()).await.unwrap();\n\n        // Job 2 (High) should be before job1 (Low) in priority queue\n        // High priority jobs come first regardless of insertion order\n        assert!(queue.position(\u0026job2.metadata.id).await.is_some());\n        assert!(queue.position(\u0026job1.metadata.id).await.is_some());\n\n        // Both jobs should be in the queue\n        assert!(queue.contains(\u0026job1.metadata.id).await);\n        assert!(queue.contains(\u0026job2.metadata.id).await);\n    }\n\n    #[tokio::test]\n    async fn test_preemption() {\n        let config = QueueConfig {\n            strategy: QueueStrategy::Priority {\n                with_preemption: true,\n                max_queue_time: None,\n            },\n            ..Default::default()\n        };\n        let queue = PriorityQueue::new(config);\n\n        // Add low priority jobs\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::Medium))\n            .await\n            .unwrap();\n\n        // Add high priority job that should trigger preemption\n        let high_job = create_test_job(3, JobPriority::High);\n        let preempted = queue.preempt(\u0026high_job).await.unwrap();\n\n        // High priority job should preempt lower priority jobs\n        assert!(!preempted.is_empty());\n\n        // Check that high priority job is now in queue\n        assert!(queue.contains(\u0026high_job.metadata.id).await);\n    }\n\n    #[tokio::test]\n    async fn test_queue_stats() {\n        let queue = PriorityQueue::new(QueueConfig::default());\n\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::High))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(3, JobPriority::Medium))\n            .await\n            .unwrap();\n\n        let stats = queue.stats().await;\n\n        assert_eq!(stats.total_jobs, 3);\n        assert_eq!(stats.priority_counts[\u0026JobPriority::Low], 1);\n        assert_eq!(stats.priority_counts[\u0026JobPriority::Medium], 1);\n        assert_eq!(stats.priority_counts[\u0026JobPriority::High], 1);\n    }\n\n    #[tokio::test]\n    async fn test_queue_capacity() {\n        let config = QueueConfig {\n            max_size: 2,\n            strategy: QueueStrategy::default(),\n            ..Default::default()\n        };\n        let queue = PriorityQueue::new(config);\n\n        queue\n            .enqueue(create_test_job(1, JobPriority::Low))\n            .await\n            .unwrap();\n        queue\n            .enqueue(create_test_job(2, JobPriority::Medium))\n            .await\n            .unwrap();\n\n        // Third enqueue should fail\n        let result = queue.enqueue(create_test_job(3, JobPriority::High)).await;\n\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_fifo_queue_basic() {\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fifo,\n            ..Default::default()\n        };\n        let queue = FifoQueue::new(config);\n\n        // Enqueue jobs in specific order\n        let job1 = create_test_job(1, JobPriority::Medium);\n        let job2 = create_test_job(2, JobPriority::High);\n        let job3 = create_test_job(3, JobPriority::Low);\n\n        queue.enqueue(job1).await.unwrap();\n        queue.enqueue(job2).await.unwrap();\n        queue.enqueue(job3).await.unwrap();\n\n        // FIFO should return jobs in insertion order\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(1)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(2)));\n        assert_eq!(queue.dequeue().await, Some(uuid::Uuid::from_u128(3)));\n\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_fifo_queue_cancel() {\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fifo,\n            ..Default::default()\n        };\n        let queue = FifoQueue::new(config);\n\n        let job1 = create_test_job(1, JobPriority::Medium);\n        queue.enqueue(job1.clone()).await.unwrap();\n\n        assert!(queue.contains(\u0026job1.metadata.id).await);\n\n        queue.cancel(\u0026job1.metadata.id).await.unwrap();\n\n        assert!(!queue.contains(\u0026job1.metadata.id).await);\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_fair_queue_basic() {\n        let mut weights = HashMap::new();\n        weights.insert(\"tenant-a\".to_string(), 2);\n        weights.insert(\"tenant-b\".to_string(), 1);\n\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fair {\n                tenant_key: \"namespace\".to_string(),\n                weights: weights.clone(),\n                quantum: None,\n            },\n            ..Default::default()\n        };\n\n        let queue = FairQueue::new(config).unwrap();\n\n        // Create jobs with different tenants\n        let mut job1 = create_test_job(1, JobPriority::Medium);\n        job1.metadata.namespace = \"tenant-a\".to_string();\n        queue.enqueue(job1).await.unwrap();\n\n        let mut job2 = create_test_job(2, JobPriority::High);\n        job2.metadata.namespace = \"tenant-b\".to_string();\n        queue.enqueue(job2).await.unwrap();\n\n        let mut job3 = create_test_job(3, JobPriority::Low);\n        job3.metadata.namespace = \"tenant-a\".to_string();\n        queue.enqueue(job3).await.unwrap();\n\n        // Fair queue should alternate between tenants\n        // tenant-a has weight 2, so should get more jobs\n        let first = queue.dequeue().await;\n        assert!(first.is_some());\n\n        let second = queue.dequeue().await;\n        assert!(second.is_some());\n\n        let third = queue.dequeue().await;\n        assert!(third.is_some());\n\n        // All jobs should be dequeued\n        assert_eq!(queue.pending_count().await, 0);\n    }\n\n    #[tokio::test]\n    async fn test_fair_queue_stats() {\n        let mut weights = HashMap::new();\n        weights.insert(\"tenant-a\".to_string(), 2);\n        weights.insert(\"tenant-b\".to_string(), 1);\n\n        let config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fair {\n                tenant_key: \"namespace\".to_string(),\n                weights: weights.clone(),\n                quantum: None,\n            },\n            ..Default::default()\n        };\n\n        let queue = FairQueue::new(config).unwrap();\n\n        // Create jobs with different tenants\n        for i in 1..=3 {\n            let mut job = create_test_job(i, JobPriority::Medium);\n            job.metadata.namespace = \"tenant-a\".to_string();\n            queue.enqueue(job).await.unwrap();\n        }\n\n        for i in 4..=5 {\n            let mut job = create_test_job(i, JobPriority::Medium);\n            job.metadata.namespace = \"tenant-b\".to_string();\n            queue.enqueue(job).await.unwrap();\n        }\n\n        let stats = queue.stats().await;\n\n        assert_eq!(stats.total_jobs, 5);\n        assert_eq!(stats.tenant_distribution[\"tenant-a\"], 3);\n        assert_eq!(stats.tenant_distribution[\"tenant-b\"], 2);\n    }\n\n    #[tokio::test]\n    async fn test_queue_strategy_different_types() {\n        // Test Priority Queue\n        let priority_config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Priority {\n                with_preemption: true,\n                max_queue_time: Some(chrono::Duration::minutes(5)),\n            },\n            ..Default::default()\n        };\n        let priority_queue = PriorityQueue::new(priority_config);\n        assert!(priority_queue.pending_count().await == 0);\n\n        // Test FIFO Queue\n        let fifo_config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fifo,\n            ..Default::default()\n        };\n        let fifo_queue = FifoQueue::new(fifo_config);\n        assert!(fifo_queue.pending_count().await == 0);\n\n        // Test Fair Queue\n        let mut weights = HashMap::new();\n        weights.insert(\"tenant-1\".to_string(), 1);\n\n        let fair_config = QueueConfig {\n            max_size: 100,\n            strategy: QueueStrategy::Fair {\n                tenant_key: \"namespace\".to_string(),\n                weights: weights.clone(),\n                quantum: None,\n            },\n            ..Default::default()\n        };\n        let fair_queue = FairQueue::new(fair_config).unwrap();\n        assert!(fair_queue.pending_count().await == 0);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","selection","mod.rs"],"content":"//! Worker Selection Algorithms Module\n//!\n//! This module implements various algorithms for selecting the best worker\n//! for a job based on different criteria and optimization goals.\n\nuse crate::SchedulerError;\nuse crate::backend::ComputeResource;\nuse crate::types::*;\nuse std::collections::HashMap;\n\n/// Worker selection strategy\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum SelectionStrategy {\n    /// Select worker with lowest current load\n    LeastLoaded,\n\n    /// Select worker that balances resources across cluster\n    ResourceBalance,\n\n    /// Pack jobs efficiently to minimize fragmentation\n    BinPacking,\n\n    /// Distribute jobs evenly across workers (Round Robin)\n    RoundRobin,\n\n    /// Custom strategy with plugin support\n    Custom(String),\n}\n\n/// Selection criteria configuration\n#[derive(Debug, Clone)]\npub struct SelectionCriteria {\n    pub resource_weights: ResourceWeights,\n    pub balance_threshold: f64, // How balanced resources should be\n    pub locality_aware: bool,   // Prefer local workers\n}\n\n/// Resource weights for scoring\n#[derive(Debug, Clone)]\npub struct ResourceWeights {\n    pub cpu_weight: f64,\n    pub memory_weight: f64,\n    pub gpu_weight: f64,\n    pub network_weight: f64,\n}\n\nimpl Default for SelectionCriteria {\n    fn default() -\u003e Self {\n        Self {\n            resource_weights: ResourceWeights {\n                cpu_weight: 1.0,\n                memory_weight: 1.0,\n                gpu_weight: 2.0, // GPUs are more critical\n                network_weight: 0.5,\n            },\n            balance_threshold: 0.8, // 80% balance threshold\n            locality_aware: false,\n        }\n    }\n}\n\n/// Selection result\n#[derive(Debug, Clone)]\npub struct SelectionResult {\n    pub selected_worker: WorkerNode,\n    pub score: f64,\n    pub reason: String,\n}\n\n/// Worker Selection Engine\npub struct WorkerSelector {\n    strategy: SelectionStrategy,\n    criteria: SelectionCriteria,\n    round_robin_index: usize,\n}\n\nimpl WorkerSelector {\n    /// Create new worker selector\n    pub fn new(mut strategy: SelectionStrategy, criteria: SelectionCriteria) -\u003e Self {\n        // For Round Robin, we don't need mutable state in the selector\n        // The index can be managed externally if needed\n        let round_robin_index = 0;\n\n        Self {\n            strategy,\n            criteria,\n            round_robin_index,\n        }\n    }\n\n    /// Select best worker from available workers\n    pub fn select_worker(\n        \u0026self,\n        job: \u0026Job,\n        mut workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        if workers.is_empty() {\n            return Err(SchedulerError::SelectionError(\n                \"No workers available for selection\".to_string(),\n            ));\n        }\n\n        match self.strategy {\n            SelectionStrategy::LeastLoaded =\u003e self.select_least_loaded(job, workers),\n            SelectionStrategy::ResourceBalance =\u003e self.select_resource_balance(job, workers),\n            SelectionStrategy::BinPacking =\u003e self.select_bin_packing(job, workers),\n            SelectionStrategy::RoundRobin =\u003e {\n                // For round robin, shuffle workers deterministically\n                workers.sort_by(|a, b| a.labels.get(\"name\").cmp(\u0026b.labels.get(\"name\")));\n                self.select_round_robin(workers)\n            }\n            SelectionStrategy::Custom(_) =\u003e {\n                // For now, fallback to Least Loaded\n                // TODO: Implement plugin-based custom strategies\n                self.select_least_loaded(job, workers)\n            }\n        }\n    }\n\n    /// Least Loaded: Select worker with lowest current load\n    fn select_least_loaded(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        let mut best_worker = None;\n        let mut best_load = f64::MAX;\n\n        for worker in workers.iter() {\n            let load = self.calculate_load(\u0026worker);\n\n            // Check if worker can fit the job\n            if self.can_fit_job(job, \u0026worker) \u0026\u0026 load \u003c best_load {\n                best_load = load;\n                best_worker = Some(worker);\n            }\n        }\n\n        if let Some(worker) = best_worker {\n            Ok(SelectionResult {\n                selected_worker: worker.clone(),\n                score: 1.0 / (1.0 + best_load),\n                reason: format!(\"Lowest load: {:.2}\", best_load),\n            })\n        } else {\n            Err(SchedulerError::SelectionError(\n                \"No worker can accommodate the job\".to_string(),\n            ))\n        }\n    }\n\n    /// Resource Balance: Select worker that best balances cluster resources\n    fn select_resource_balance(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        let mut best_worker = None;\n        let mut best_score = f64::MIN;\n\n        for worker in workers.iter() {\n            // Check if worker can fit the job\n            if !self.can_fit_job(job, \u0026worker) {\n                continue;\n            }\n\n            let balance_score = self.calculate_balance_score(\u0026workers, \u0026worker);\n\n            if balance_score \u003e best_score {\n                best_score = balance_score;\n                best_worker = Some(worker);\n            }\n        }\n\n        if let Some(worker) = best_worker {\n            Ok(SelectionResult {\n                selected_worker: worker.clone(),\n                score: best_score,\n                reason: format!(\"Best balance score: {:.2}\", best_score),\n            })\n        } else {\n            Err(SchedulerError::SelectionError(\n                \"No worker can accommodate the job\".to_string(),\n            ))\n        }\n    }\n\n    /// Bin Packing: Efficiently pack jobs to minimize fragmentation\n    fn select_bin_packing(\n        \u0026self,\n        job: \u0026Job,\n        workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        let mut best_worker = None;\n        let mut best_fit_score = f64::MAX;\n\n        for worker in workers.iter() {\n            // Check if worker can fit the job\n            if !self.can_fit_job(job, \u0026worker) {\n                continue;\n            }\n\n            // Calculate fit score: how well the job fits\n            let fit_score = self.calculate_fit_score(job, \u0026worker);\n\n            // Lower is better (First Fit Decreasing)\n            if fit_score \u003c best_fit_score {\n                best_fit_score = fit_score;\n                best_worker = Some(worker);\n            }\n        }\n\n        if let Some(worker) = best_worker {\n            Ok(SelectionResult {\n                selected_worker: worker.clone(),\n                score: 1.0 / (1.0 + best_fit_score),\n                reason: format!(\"Best fit score: {:.2}\", best_fit_score),\n            })\n        } else {\n            Err(SchedulerError::SelectionError(\n                \"No worker can accommodate the job\".to_string(),\n            ))\n        }\n    }\n\n    /// Round Robin: Distribute jobs evenly across workers\n    fn select_round_robin(\n        \u0026self,\n        mut workers: Vec\u003cWorkerNode\u003e,\n    ) -\u003e Result\u003cSelectionResult, SchedulerError\u003e {\n        // Sort workers by name for deterministic round-robin\n        workers.sort_by(|a, b| a.labels.get(\"name\").cmp(\u0026b.labels.get(\"name\")));\n\n        // Select first worker (deterministic)\n        let worker = workers.first().cloned().unwrap();\n\n        Ok(SelectionResult {\n            selected_worker: worker,\n            score: 1.0,\n            reason: \"Round Robin selection\".to_string(),\n        })\n    }\n\n    /// Calculate current load of a worker\n    fn calculate_load(\u0026self, worker: \u0026WorkerNode) -\u003e f64 {\n        let resource = \u0026worker.resources;\n        // Calculate load as a simple utilization metric\n        // Since we don't have used/available breakdown, assume some baseline utilization\n        // For this example, we'll use a heuristic based on cpu and memory\n        let cpu_utilization = if resource.cpu_cores \u003e 0.0 {\n            (resource.cpu_cores / 100.0).min(1.0) // Normalize\n        } else {\n            0.0\n        };\n\n        let memory_utilization = if resource.memory_bytes \u003e 0 {\n            ((resource.memory_bytes as f64 / 1_000_000_000.0) / 100.0).min(1.0) // Normalize GB to 0-1\n        } else {\n            0.0\n        };\n\n        // Weighted average\n        (cpu_utilization * self.criteria.resource_weights.cpu_weight\n            + memory_utilization * self.criteria.resource_weights.memory_weight)\n            / (self.criteria.resource_weights.cpu_weight\n                + self.criteria.resource_weights.memory_weight)\n    }\n\n    /// Check if worker can fit the job's resource requirements\n    fn can_fit_job(\u0026self, job: \u0026Job, worker: \u0026WorkerNode) -\u003e bool {\n        if let Some(requirements) = \u0026job.spec.resource_requirements {\n            let resource = \u0026worker.resources;\n            // Check CPU\n            if let Some(cpu_required) = requirements.cpu_cores {\n                if resource.cpu_cores \u003c cpu_required {\n                    return false;\n                }\n            }\n\n            // Check Memory\n            if let Some(memory_required) = requirements.memory_bytes {\n                if resource.memory_bytes \u003c memory_required {\n                    return false;\n                }\n            }\n\n            // Check GPU\n            if let Some(gpu_required) = requirements.gpu_count {\n                if resource.gpu_count \u003c gpu_required {\n                    return false;\n                }\n            }\n\n            true\n        } else {\n            true // No resource requirements\n        }\n    }\n\n    /// Calculate balance score for resource balance strategy\n    fn calculate_balance_score(\u0026self, workers: \u0026Vec\u003cWorkerNode\u003e, candidate: \u0026WorkerNode) -\u003e f64 {\n        let resource = \u0026candidate.resources;\n\n        // Simple balance: check how well the candidate's resources compare to cluster average\n        let avg_cpu: f64 =\n            workers.iter().map(|w| w.resources.cpu_cores).sum::\u003cf64\u003e() / workers.len() as f64;\n        let avg_memory: f64 = workers\n            .iter()\n            .map(|w| w.resources.memory_bytes as f64)\n            .sum::\u003cf64\u003e()\n            / workers.len() as f64;\n\n        let cpu_deviation = (resource.cpu_cores - avg_cpu).abs() / avg_cpu;\n        let memory_deviation = ((resource.memory_bytes as f64 - avg_memory) / avg_memory).abs();\n\n        // Lower deviation is better (negative because we want to maximize)\n        -(cpu_deviation + memory_deviation) / 2.0\n    }\n\n    /// Calculate fit score for bin packing strategy\n    fn calculate_fit_score(\u0026self, job: \u0026Job, worker: \u0026WorkerNode) -\u003e f64 {\n        if let Some(requirements) = \u0026job.spec.resource_requirements {\n            let resource = \u0026worker.resources;\n            let mut fit_score = 0.0;\n\n            // CPU fit: how well the job fits in available CPU\n            if let Some(cpu_required) = requirements.cpu_cores {\n                fit_score += (resource.cpu_cores - cpu_required) / resource.cpu_cores;\n            }\n\n            // Memory fit\n            if let Some(memory_required) = requirements.memory_bytes {\n                let memory_ratio = memory_required as f64 / resource.memory_bytes as f64;\n                fit_score += 1.0 - memory_ratio; // Lower ratio is better (more space left)\n            }\n\n            fit_score\n        } else {\n            0.0\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::types::{BackendSpecific, JobMetadata, NodeLocation};\n\n    fn create_test_worker(\n        id: \u0026str,\n        cpu_cores: f64,\n        memory_bytes: u64,\n        _used_cpu: f64,\n        _used_memory: u64,\n    ) -\u003e WorkerNode {\n        WorkerNode {\n            id: uuid::Uuid::new_v4(),\n            backend_type: BackendType::Kubernetes,\n            status: WorkerStatus::Running,\n            resources: ComputeResource {\n                cpu_cores,\n                memory_bytes,\n                gpu_count: 0,\n            },\n            labels: {\n                let mut labels = HashMap::new();\n                labels.insert(\"name\".to_string(), format!(\"worker-{}\", id));\n                labels\n            },\n            taints: vec![],\n            backend_specific: BackendSpecific::Kubernetes(KubernetesNodeSpecific {\n                node_name: format!(\"worker-{}\", id),\n                namespace: \"default\".to_string(),\n            }),\n            location: NodeLocation::default(),\n        }\n    }\n\n    fn create_test_job(cpu_cores: f64, memory_bytes: u64) -\u003e Job {\n        Job {\n            metadata: JobMetadata {\n                id: uuid::Uuid::new_v4(),\n                name: \"test-job\".to_string(),\n                namespace: \"default\".to_string(),\n                labels: HashMap::new(),\n                created_at: chrono::Utc::now(),\n            },\n            spec: JobSpec {\n                resource_requirements: Some(ResourceRequirements {\n                    cpu_cores: Some(cpu_cores),\n                    memory_bytes: Some(memory_bytes),\n                    gpu_count: None,\n                    ephemeral_storage: None,\n                }),\n                priority: JobPriority::Medium,\n                node_selector: None,\n                affinity: None,\n                tolerations: vec![],\n                max_retries: 3,\n            },\n        }\n    }\n\n    #[tokio::test]\n    async fn test_least_loaded_strategy() {\n        let mut selector =\n            WorkerSelector::new(SelectionStrategy::LeastLoaded, SelectionCriteria::default());\n\n        let workers = vec![\n            create_test_worker(\"1\", 8.0, 8_000_000_000, 6.0, 6_000_000_000), // 75% load\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 2.0, 2_000_000_000), // 25% load (best)\n            create_test_worker(\"3\", 8.0, 8_000_000_000, 4.0, 4_000_000_000), // 50% load\n        ];\n\n        let job = create_test_job(1.0, 1_000_000_000);\n\n        let result = selector.select_worker(\u0026job, workers).unwrap();\n\n        // Should select a valid worker\n        assert!(result.selected_worker.labels.get(\"name\").is_some());\n        assert!(result.score \u003e 0.0);\n    }\n\n    #[tokio::test]\n    async fn test_resource_balance_strategy() {\n        let selector = WorkerSelector::new(\n            SelectionStrategy::ResourceBalance,\n            SelectionCriteria::default(),\n        );\n\n        let workers = vec![\n            create_test_worker(\"1\", 8.0, 8_000_000_000, 2.0, 6_000_000_000), // CPU: 25%, Mem: 75% (imbalanced)\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 4.0, 4_000_000_000), // CPU: 50%, Mem: 50% (balanced)\n            create_test_worker(\"3\", 8.0, 8_000_000_000, 6.0, 2_000_000_000), // CPU: 75%, Mem: 25% (imbalanced)\n        ];\n\n        let job = create_test_job(1.0, 1_000_000_000);\n\n        let result = selector.select_worker(\u0026job, workers).unwrap();\n\n        // Should select a valid worker\n        assert!(result.selected_worker.labels.get(\"name\").is_some());\n    }\n\n    #[tokio::test]\n    async fn test_bin_packing_strategy() {\n        let selector =\n            WorkerSelector::new(SelectionStrategy::BinPacking, SelectionCriteria::default());\n\n        let workers = vec![\n            create_test_worker(\"1\", 16.0, 16_000_000_000, 8.0, 8_000_000_000), // Half full\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 2.0, 2_000_000_000), // Quarter full (best fit)\n            create_test_worker(\"3\", 4.0, 4_000_000_000, 1.0, 1_000_000_000), // Quarter full\n        ];\n\n        let job = create_test_job(2.0, 2_000_000_000);\n\n        let result = selector.select_worker(\u0026job, workers).unwrap();\n\n        // Should select a well-fitting worker\n        assert!(result.score \u003e 0.0);\n    }\n\n    #[tokio::test]\n    async fn test_round_robin_strategy() {\n        let mut selector =\n            WorkerSelector::new(SelectionStrategy::RoundRobin, SelectionCriteria::default());\n\n        let workers = vec![\n            create_test_worker(\"1\", 8.0, 8_000_000_000, 0.0, 0),\n            create_test_worker(\"2\", 8.0, 8_000_000_000, 0.0, 0),\n            create_test_worker(\"3\", 8.0, 8_000_000_000, 0.0, 0),\n        ];\n\n        // All selections should return the same worker (deterministic)\n        let result1 = selector\n            .select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers.clone())\n            .unwrap();\n        assert_eq!(\n            result1.selected_worker.labels.get(\"name\"),\n            Some(\u0026\"worker-1\".to_string())\n        );\n\n        let result2 = selector\n            .select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers.clone())\n            .unwrap();\n        assert_eq!(\n            result2.selected_worker.labels.get(\"name\"),\n            Some(\u0026\"worker-1\".to_string())\n        );\n\n        let result3 = selector\n            .select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers.clone())\n            .unwrap();\n        assert_eq!(\n            result3.selected_worker.labels.get(\"name\"),\n            Some(\u0026\"worker-1\".to_string())\n        );\n    }\n\n    #[tokio::test]\n    async fn test_selection_with_insufficient_resources() {\n        let selector =\n            WorkerSelector::new(SelectionStrategy::LeastLoaded, SelectionCriteria::default());\n\n        // Workers with insufficient resources\n        let workers = vec![\n            create_test_worker(\"1\", 2.0, 2_000_000_000, 1.5, 1_500_000_000), // Can't fit 4 CPU job\n        ];\n\n        let job = create_test_job(4.0, 4_000_000_000); // Requires more than available\n\n        let result = selector.select_worker(\u0026job, workers);\n\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_no_workers_available() {\n        let selector =\n            WorkerSelector::new(SelectionStrategy::LeastLoaded, SelectionCriteria::default());\n\n        let workers = vec![]; // No workers\n\n        let result = selector.select_worker(\u0026create_test_job(1.0, 1_000_000_000), workers);\n\n        assert!(result.is_err());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","scheduler","src","types","mod.rs"],"content":"//! Core Types and Data Structures for Scheduler\n//!\n//! This module defines all the core data types used by the scheduler framework,\n//! including jobs, workers, priorities, affinity rules, taints, etc.\n\nuse chrono::{DateTime, Utc};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n/// Job priority levels (higher value = higher priority)\n#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize)]\npub enum JobPriority {\n    Batch = 1,    // Low-priority batch processing\n    Low = 2,      // Background jobs\n    Medium = 3,   // Regular CI/CD jobs\n    High = 4,     // Production deployments\n    Critical = 5, // System critical jobs\n}\n\nimpl JobPriority {\n    /// Check if this priority can preempt another\n    pub fn can_preempt(\u0026self, other: \u0026JobPriority) -\u003e bool {\n        self \u003e other\n    }\n\n    /// Get numeric value\n    pub fn value(\u0026self) -\u003e u32 {\n        match self {\n            JobPriority::Critical =\u003e 5,\n            JobPriority::High =\u003e 4,\n            JobPriority::Medium =\u003e 3,\n            JobPriority::Low =\u003e 2,\n            JobPriority::Batch =\u003e 1,\n        }\n    }\n}\n\nimpl std::fmt::Display for JobPriority {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            JobPriority::Critical =\u003e write!(f, \"Critical\"),\n            JobPriority::High =\u003e write!(f, \"High\"),\n            JobPriority::Medium =\u003e write!(f, \"Medium\"),\n            JobPriority::Low =\u003e write!(f, \"Low\"),\n            JobPriority::Batch =\u003e write!(f, \"Batch\"),\n        }\n    }\n}\n\n/// Backend types supported by the scheduler\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub enum BackendType {\n    Kubernetes,\n    Docker,\n    CloudVm,\n    BareMetal,\n    Serverless,\n    Hpc,\n}\n\nimpl std::fmt::Display for BackendType {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            BackendType::Kubernetes =\u003e write!(f, \"Kubernetes\"),\n            BackendType::Docker =\u003e write!(f, \"Docker\"),\n            BackendType::CloudVm =\u003e write!(f, \"Cloud VM\"),\n            BackendType::BareMetal =\u003e write!(f, \"Bare Metal\"),\n            BackendType::Serverless =\u003e write!(f, \"Serverless\"),\n            BackendType::Hpc =\u003e write!(f, \"HPC\"),\n        }\n    }\n}\n\n/// Worker node status\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum WorkerStatus {\n    Pending,  // Node is starting up\n    Ready,    // Node is ready to accept jobs\n    Running,  // Node is running jobs\n    Draining, // Node is being drained (no new jobs)\n    Offline,  // Node is offline\n    Failed,   // Node has failed\n}\n\nimpl WorkerStatus {\n    pub fn is_available(\u0026self) -\u003e bool {\n        matches!(self, WorkerStatus::Ready | WorkerStatus::Running)\n    }\n}\n\n/// Job definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Job {\n    pub metadata: JobMetadata,\n    pub spec: JobSpec,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct JobMetadata {\n    pub id: super::JobId,\n    pub name: String,\n    pub namespace: String,\n    pub labels: HashMap\u003cString, String\u003e,\n    pub created_at: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct JobSpec {\n    pub resource_requirements: Option\u003cResourceRequirements\u003e,\n    pub priority: JobPriority,\n    pub node_selector: Option\u003cNodeSelector\u003e,\n    pub affinity: Option\u003cNodeAffinity\u003e,\n    pub tolerations: Vec\u003cToleration\u003e,\n    pub max_retries: u32,\n}\n\n/// Resource requirements for a job\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct ResourceRequirements {\n    pub cpu_cores: Option\u003cf64\u003e,\n    pub memory_bytes: Option\u003cu64\u003e,\n    pub gpu_count: Option\u003cu32\u003e,\n    pub ephemeral_storage: Option\u003cu64\u003e,\n}\n\nimpl ResourceRequirements {\n    /// Check if requirements are valid\n    pub fn is_valid(\u0026self) -\u003e bool {\n        if let Some(cpu) = self.cpu_cores {\n            if cpu \u003c= 0.0 {\n                return false;\n            }\n        }\n\n        if let Some(memory) = self.memory_bytes {\n            if memory == 0 {\n                return false;\n            }\n        }\n\n        if let Some(gpu) = self.gpu_count {\n            if gpu == 0 {\n                return false;\n            }\n        }\n\n        true\n    }\n}\n\n/// Node selector for job placement\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct NodeSelector {\n    pub labels: HashMap\u003cString, String\u003e,\n}\n\n/// Node affinity rules\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct NodeAffinity {\n    pub required_during_scheduling: Vec\u003cLabelSelector\u003e,\n    pub preferred_during_scheduling: Vec\u003cWeightedLabelSelector\u003e,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct LabelSelector {\n    pub key: String,\n    pub operator: LabelSelectorOperator,\n    pub values: Option\u003cVec\u003cString\u003e\u003e,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct WeightedLabelSelector {\n    pub selector: LabelSelector,\n    pub weight: i32,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum LabelSelectorOperator {\n    In,\n    NotIn,\n    Exists,\n    DoesNotExist,\n}\n\n/// Taints for node dedication\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct Taint {\n    pub key: String,\n    pub operator: TaintOperator,\n    pub value: String,\n    pub effect: TaintEffect,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum TaintOperator {\n    Equal,\n    NotEqual,\n    Exists,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum TaintEffect {\n    NoSchedule,\n    PreferNoSchedule,\n    NoExecute,\n}\n\n/// Tolerations to override taints\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct Toleration {\n    pub key: String,\n    pub operator: TaintOperator,\n    pub value: String,\n    pub effect: TaintEffect,\n    pub toleration_seconds: Option\u003ci64\u003e,\n}\n\n/// Backend-specific information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum BackendSpecific {\n    Kubernetes(KubernetesNodeSpecific),\n    Docker(DockerNodeSpecific),\n    CloudVm(CloudVmNodeSpecific),\n    BareMetal(BareMetalNodeSpecific),\n    Serverless(ServerlessNodeSpecific),\n    Hpc(HpcNodeSpecific),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KubernetesNodeSpecific {\n    pub node_name: String,\n    pub namespace: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DockerNodeSpecific {\n    pub host: String,\n    pub socket_path: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CloudVmNodeSpecific {\n    pub instance_id: String,\n    pub region: String,\n    pub zone: String,\n    pub instance_type: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BareMetalNodeSpecific {\n    pub hostname: String,\n    pub ip_address: String,\n    pub rack_id: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ServerlessNodeSpecific {\n    pub provider: String,\n    pub runtime: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct HpcNodeSpecific {\n    pub cluster_name: String,\n    pub partition: String,\n}\n\n/// Node location information\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct NodeLocation {\n    pub region: Option\u003cString\u003e,\n    pub zone: Option\u003cString\u003e,\n    pub datacenter: Option\u003cString\u003e,\n    pub rack: Option\u003cString\u003e,\n}\n\n/// Scoring weights for worker selection\n#[derive(Debug, Clone)]\npub struct ScoringWeights {\n    pub resource_fit: f64,\n    pub location: f64,\n    pub backend: f64,\n    pub affinity: f64,\n    pub taints: f64,\n}\n\nimpl Default for ScoringWeights {\n    fn default() -\u003e Self {\n        Self {\n            resource_fit: 0.40, // 40%\n            location: 0.25,     // 25%\n            backend: 0.15,      // 15%\n            affinity: 0.15,     // 15%\n            taints: 0.05,       // 5%\n        }\n    }\n}\n\nimpl ScoringWeights {\n    pub fn new(resource_fit: f64, location: f64, backend: f64, affinity: f64, taints: f64) -\u003e Self {\n        Self {\n            resource_fit,\n            location,\n            backend,\n            affinity,\n            taints,\n        }\n    }\n}\n\n/// Scored worker with score\n#[derive(Debug, Clone)]\npub struct ScoredWorker {\n    pub node: WorkerNode,\n    pub score: f64,\n    pub reasons: Vec\u003cString\u003e,\n}\n\nimpl ScoredWorker {\n    pub fn new(node: WorkerNode, score: f64) -\u003e Self {\n        Self {\n            node,\n            score,\n            reasons: vec![],\n        }\n    }\n\n    pub fn with_reason(mut self, reason: String) -\u003e Self {\n        self.reasons.push(reason);\n        self\n    }\n}\n\n/// Worker selection strategies\n#[derive(Debug, Clone)]\npub enum WorkerSelectionStrategy {\n    LeastLoaded,\n    ResourceBalance,\n    BinPacking,\n    RoundRobin,\n    Custom(String), // Plugin-based strategy\n}\n\n/// Worker node representation\n#[derive(Debug, Clone)]\npub struct WorkerNode {\n    pub id: super::WorkerId,\n    pub backend_type: BackendType,\n    pub status: WorkerStatus,\n    pub resources: super::backend::ComputeResource,\n    pub labels: HashMap\u003cString, String\u003e,\n    pub taints: Vec\u003cTaint\u003e,\n    pub backend_specific: BackendSpecific,\n    pub location: NodeLocation,\n}\n\nimpl WorkerNode {\n    /// Check if node matches job requirements\n    pub fn matches_requirements(\u0026self, job: \u0026Job) -\u003e bool {\n        // Check node selector\n        if let Some(selector) = \u0026job.spec.node_selector {\n            if !self.matches_node_selector(selector) {\n                return false;\n            }\n        }\n\n        // Check taints and tolerations\n        if !self.has_matching_tolerations(\u0026job.spec.tolerations) {\n            return false;\n        }\n\n        // Check affinity rules (simplified)\n        if let Some(affinity) = \u0026job.spec.affinity {\n            if !self.matches_affinity(affinity) {\n                return false;\n            }\n        }\n\n        // Check resources\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            if !self.resources.has_resources(req) {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node matches node selector\n    fn matches_node_selector(\u0026self, selector: \u0026NodeSelector) -\u003e bool {\n        for (key, value) in \u0026selector.labels {\n            match self.labels.get(key) {\n                Some(node_value) =\u003e {\n                    if node_value != value {\n                        return false;\n                    }\n                }\n                None =\u003e return false,\n            }\n        }\n        true\n    }\n\n    /// Check if node has matching tolerations\n    fn has_matching_tolerations(\u0026self, tolerations: \u0026[Toleration]) -\u003e bool {\n        for taint in \u0026self.taints {\n            let has_matching_toleration = tolerations.iter().any(|tol| {\n                tol.key == taint.key \u0026\u0026 tol.value == taint.value \u0026\u0026 tol.effect == taint.effect\n            });\n\n            // If there's no matching toleration and effect is NoSchedule, reject\n            if !has_matching_toleration \u0026\u0026 taint.effect == TaintEffect::NoSchedule {\n                return false;\n            }\n        }\n\n        true\n    }\n\n    /// Check if node matches affinity rules (simplified)\n    fn matches_affinity(\u0026self, _affinity: \u0026NodeAffinity) -\u003e bool {\n        // Simplified: assume all nodes match for now\n        // In production: implement full affinity matching logic\n        true\n    }\n\n    /// Calculate score for this node based on job\n    pub fn calculate_score(\u0026self, job: \u0026Job, weights: \u0026ScoringWeights) -\u003e f64 {\n        let mut score = 0.0;\n\n        // Resource fit score (0-100)\n        if let Some(req) = \u0026job.spec.resource_requirements {\n            let utilization = self.resources.utilization(req);\n            let resource_score = 100.0 * (1.0 - utilization.abs()); // Lower utilization = higher score\n            score += resource_score * weights.resource_fit;\n        }\n\n        // Location score\n        let location_score = 100.0; // Simplified: all locations equal\n        score += location_score * weights.location;\n\n        // Backend score\n        let backend_score = if self.backend_type == BackendType::Kubernetes {\n            100.0\n        } else {\n            80.0\n        };\n        score += backend_score * weights.backend;\n\n        score\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_job_priority_comparison() {\n        assert!(JobPriority::Critical \u003e JobPriority::High);\n        assert!(JobPriority::High \u003e JobPriority::Medium);\n        assert!(JobPriority::Medium \u003e JobPriority::Low);\n        assert!(JobPriority::Low \u003e JobPriority::Batch);\n    }\n\n    #[test]\n    fn test_job_priority_preemption() {\n        let critical = JobPriority::Critical;\n        let medium = JobPriority::Medium;\n\n        assert!(critical.can_preempt(\u0026medium));\n        assert!(!medium.can_preempt(\u0026critical));\n    }\n\n    #[test]\n    fn test_job_priority_value() {\n        assert_eq!(JobPriority::Critical.value(), 5);\n        assert_eq!(JobPriority::High.value(), 4);\n        assert_eq!(JobPriority::Medium.value(), 3);\n        assert_eq!(JobPriority::Low.value(), 2);\n        assert_eq!(JobPriority::Batch.value(), 1);\n    }\n\n    #[test]\n    fn test_worker_status_is_available() {\n        assert!(WorkerStatus::Ready.is_available());\n        assert!(WorkerStatus::Running.is_available());\n        assert!(!WorkerStatus::Pending.is_available());\n        assert!(!WorkerStatus::Draining.is_available());\n        assert!(!WorkerStatus::Offline.is_available());\n        assert!(!WorkerStatus::Failed.is_available());\n    }\n\n    #[test]\n    fn test_resource_requirements_is_valid() {\n        let valid = ResourceRequirements {\n            cpu_cores: Some(2.0),\n            memory_bytes: Some(4_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(valid.is_valid());\n\n        let invalid = ResourceRequirements {\n            cpu_cores: Some(0.0),\n            memory_bytes: Some(4_000_000_000),\n            gpu_count: Some(1),\n            ephemeral_storage: None,\n        };\n\n        assert!(!invalid.is_valid());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","correlation.rs"],"content":"//! Correlation module for distributed tracing\n\nuse serde::{Deserialize, Serialize};\nuse crate::Uuid;\n\n/// Correlation identifier for distributed tracing\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct CorrelationId(Uuid);\n\nimpl CorrelationId {\n    pub fn new() -\u003e Self {\n        Self(Uuid::new_v4())\n    }\n\n    pub fn from_uuid(uuid: Uuid) -\u003e Self {\n        Self(uuid)\n    }\n\n    pub fn as_uuid(\u0026self) -\u003e Uuid {\n        self.0\n    }\n}\n\nimpl Default for CorrelationId {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Trace context for distributed tracing\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct TraceContext {\n    pub trace_id: Uuid,\n    pub span_id: Uuid,\n    pub parent_span_id: Option\u003cUuid\u003e,\n}\n\nimpl TraceContext {\n    pub fn new() -\u003e Self {\n        Self {\n            trace_id: Uuid::new_v4(),\n            span_id: Uuid::new_v4(),\n            parent_span_id: None,\n        }\n    }\n\n    pub fn with_parent(mut self, parent: Uuid) -\u003e Self {\n        self.parent_span_id = Some(parent);\n        self\n    }\n}\n\nimpl Default for TraceContext {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","error.rs"],"content":"//! Error types shared across the system\n\nuse thiserror::Error;\n\n/// Base error type for the entire system\n#[derive(Error, Debug)]\npub enum DomainError {\n    #[error(\"validation error: {0}\")]\n    Validation(String),\n\n    #[error(\"invalid state transition from {from} to {to}\")]\n    InvalidStateTransition { from: String, to: String },\n\n    #[error(\"resource not found: {0}\")]\n    NotFound(String),\n\n    #[error(\"concurrency error: {0}\")]\n    Concurrency(String),\n\n    #[error(\"infrastructure error: {0}\")]\n    Infrastructure(String),\n\n    #[error(\"authorization error: {0}\")]\n    Authorization(String),\n\n    #[error(\"timeout: {0}\")]\n    Timeout(String),\n}\n\nimpl DomainError {\n    pub fn invalid_state_transition(from: \u0026str, to: \u0026str) -\u003e Self {\n        Self::InvalidStateTransition {\n            from: from.to_string(),\n            to: to.to_string(),\n        }\n    }\n}\n","traces":[{"line":31,"address":[5305016,5304816,5305010],"length":1,"stats":{"Line":0}},{"line":33,"address":[5304862],"length":1,"stats":{"Line":0}},{"line":34,"address":[5304901],"length":1,"stats":{"Line":0}}],"covered":0,"coverable":3},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","health_checks.rs"],"content":"//! Health check protocols and types\n\nuse chrono::{DateTime, Utc};\nuse serde::{Deserialize, Serialize};\n\n/// Health status\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum HealthStatus {\n    Healthy,\n    Degraded,\n    Unhealthy,\n}\n\n/// Health check result\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct HealthCheck {\n    pub component: String,\n    pub status: HealthStatus,\n    pub message: Option\u003cString\u003e,\n    pub timestamp: DateTime\u003cUtc\u003e,\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","job_definitions.rs"],"content":"//! Job definition types and schemas\n\nuse crate::Uuid;\nuse crate::error::DomainError;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n/// Job identifier\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct JobId(pub Uuid);\n\nimpl JobId {\n    pub fn new() -\u003e Self {\n        Self(Uuid::new_v4())\n    }\n\n    pub fn from_uuid(uuid: Uuid) -\u003e Self {\n        Self(uuid)\n    }\n\n    pub fn as_uuid(\u0026self) -\u003e Uuid {\n        self.0\n    }\n}\n\nimpl Default for JobId {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Resource requirements for a job\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct ResourceQuota {\n    pub cpu_m: u64,      // CPU in millicores\n    pub memory_mb: u64,  // Memory in MB\n    pub gpu: Option\u003cu8\u003e, // Optional GPU requirement\n}\n\nimpl ResourceQuota {\n    pub fn new(cpu_m: u64, memory_mb: u64) -\u003e Self {\n        Self {\n            cpu_m,\n            memory_mb,\n            gpu: None,\n        }\n    }\n\n    pub fn with_gpu(mut self, gpu: u8) -\u003e Self {\n        self.gpu = Some(gpu);\n        self\n    }\n}\n\n/// Job specification (immutable value object)\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct JobSpec {\n    pub name: String,\n    pub image: String,\n    pub command: Vec\u003cString\u003e,\n    pub resources: ResourceQuota,\n    pub timeout_ms: u64,\n    pub retries: u8,\n    pub env: HashMap\u003cString, String\u003e,\n    pub secret_refs: Vec\u003cString\u003e,\n}\n\nimpl JobSpec {\n    pub fn validate(\u0026self) -\u003e Result\u003c(), DomainError\u003e {\n        if self.name.trim().is_empty() {\n            return Err(DomainError::Validation(\n                \"job name cannot be empty\".to_string(),\n            ));\n        }\n\n        if self.image.trim().is_empty() {\n            return Err(DomainError::Validation(\"image cannot be empty\".to_string()));\n        }\n\n        if self.command.is_empty() {\n            return Err(DomainError::Validation(\n                \"command cannot be empty\".to_string(),\n            ));\n        }\n\n        if self.timeout_ms == 0 {\n            return Err(DomainError::Validation(\n                \"timeout must be greater than 0\".to_string(),\n            ));\n        }\n\n        Ok(())\n    }\n}\n\n/// Job state value object\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub struct JobState(String);\n\nimpl JobState {\n    pub const PENDING: \u0026'static str = \"PENDING\";\n    pub const SCHEDULED: \u0026'static str = \"SCHEDULED\";\n    pub const RUNNING: \u0026'static str = \"RUNNING\";\n    pub const SUCCESS: \u0026'static str = \"SUCCESS\";\n    pub const FAILED: \u0026'static str = \"FAILED\";\n    pub const CANCELLED: \u0026'static str = \"CANCELLED\";\n\n    pub fn new(state: String) -\u003e Result\u003cSelf, DomainError\u003e {\n        match state.as_str() {\n            Self::PENDING\n            | Self::SCHEDULED\n            | Self::RUNNING\n            | Self::SUCCESS\n            | Self::FAILED\n            | Self::CANCELLED =\u003e Ok(Self(state)),\n            _ =\u003e Err(DomainError::Validation(format!(\n                \"invalid job state: {}\",\n                state\n            ))),\n        }\n    }\n\n    pub fn can_transition_to(\u0026self, target: \u0026Self) -\u003e bool {\n        match (self.0.as_str(), target.0.as_str()) {\n            (Self::PENDING, Self::SCHEDULED) =\u003e true,\n            (Self::PENDING, Self::CANCELLED) =\u003e true,\n            (Self::SCHEDULED, Self::RUNNING) =\u003e true,\n            (Self::SCHEDULED, Self::CANCELLED) =\u003e true,\n            (Self::RUNNING, Self::SUCCESS) =\u003e true,\n            (Self::RUNNING, Self::FAILED) =\u003e true,\n            (Self::RUNNING, Self::CANCELLED) =\u003e true,\n            (Self::FAILED, Self::PENDING) =\u003e true, // For retry\n            (Self::FAILED, Self::CANCELLED) =\u003e true,\n            _ =\u003e false,\n        }\n    }\n\n    pub fn as_str(\u0026self) -\u003e \u0026str {\n        \u0026self.0\n    }\n}\n\nimpl From\u003cString\u003e for JobState {\n    fn from(s: String) -\u003e Self {\n        Self::new(s).expect(\"valid state\")\n    }\n}\n\n/// Job execution result\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct ExecResult {\n    pub exit_code: i32,\n    pub stdout: Option\u003cString\u003e,\n    pub stderr: Option\u003cString\u003e,\n}\n","traces":[{"line":13,"address":[5298560],"length":1,"stats":{"Line":0}},{"line":14,"address":[5298574],"length":1,"stats":{"Line":0}},{"line":17,"address":[5298624],"length":1,"stats":{"Line":0}},{"line":18,"address":[5298627],"length":1,"stats":{"Line":0}},{"line":21,"address":[5298656],"length":1,"stats":{"Line":0}},{"line":22,"address":[5298664],"length":1,"stats":{"Line":0}},{"line":27,"address":[5298688],"length":1,"stats":{"Line":0}},{"line":28,"address":[5298696],"length":1,"stats":{"Line":0}},{"line":41,"address":[5298720],"length":1,"stats":{"Line":0}},{"line":49,"address":[5298752],"length":1,"stats":{"Line":0}},{"line":50,"address":[5298761],"length":1,"stats":{"Line":0}},{"line":51,"address":[5298768],"length":1,"stats":{"Line":0}},{"line":69,"address":[5298800],"length":1,"stats":{"Line":0}},{"line":70,"address":[5298838],"length":1,"stats":{"Line":0}},{"line":71,"address":[5298939],"length":1,"stats":{"Line":0}},{"line":72,"address":[5298911],"length":1,"stats":{"Line":0}},{"line":76,"address":[5298874],"length":1,"stats":{"Line":0}},{"line":77,"address":[5299029],"length":1,"stats":{"Line":0}},{"line":80,"address":[5299009],"length":1,"stats":{"Line":0}},{"line":81,"address":[5299188],"length":1,"stats":{"Line":0}},{"line":82,"address":[5299157],"length":1,"stats":{"Line":0}},{"line":86,"address":[5299142],"length":1,"stats":{"Line":0}},{"line":87,"address":[5299308],"length":1,"stats":{"Line":0}},{"line":88,"address":[5299277],"length":1,"stats":{"Line":0}},{"line":92,"address":[5299399],"length":1,"stats":{"Line":0}},{"line":108,"address":[5299440,5300202],"length":1,"stats":{"Line":0}},{"line":109,"address":[5299470,5299554],"length":1,"stats":{"Line":0}},{"line":110,"address":[5299613,5299759],"length":1,"stats":{"Line":0}},{"line":116,"address":[5299958],"length":1,"stats":{"Line":0}},{"line":123,"address":[5300224],"length":1,"stats":{"Line":0}},{"line":124,"address":[5300243],"length":1,"stats":{"Line":0}},{"line":125,"address":[5300298,5300809,5300362],"length":1,"stats":{"Line":0}},{"line":126,"address":[5300771],"length":1,"stats":{"Line":0}},{"line":127,"address":[5300435,5300329,5300761],"length":1,"stats":{"Line":0}},{"line":128,"address":[5300723],"length":1,"stats":{"Line":0}},{"line":129,"address":[5300402,5300671,5300508],"length":1,"stats":{"Line":0}},{"line":130,"address":[5300638,5300713],"length":1,"stats":{"Line":0}},{"line":131,"address":[5300678],"length":1,"stats":{"Line":0}},{"line":132,"address":[5300475,5300631,5300552],"length":1,"stats":{"Line":0}},{"line":133,"address":[5300596],"length":1,"stats":{"Line":0}},{"line":134,"address":[5300545],"length":1,"stats":{"Line":0}},{"line":138,"address":[5300832],"length":1,"stats":{"Line":0}},{"line":139,"address":[5300837],"length":1,"stats":{"Line":0}},{"line":144,"address":[5300848],"length":1,"stats":{"Line":0}},{"line":145,"address":[5300862],"length":1,"stats":{"Line":0}}],"covered":0,"coverable":45},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","lib.rs"],"content":"//! Shared types and definitions for the Hodei CI/CD distributed system\n//!\n//! This crate contains common types, IDs, value objects, and error types\n//! used across all bounded contexts in the system.\n\npub mod correlation;\npub mod error;\npub mod health_checks;\npub mod job_definitions;\npub mod worker_messages;\n\npub use crate::error::DomainError;\npub use chrono::{DateTime, Utc};\npub use serde::{Deserialize, Serialize};\npub use uuid::{Uuid, uuid};\n\n// Re-export all types for easy importing\npub use crate::correlation::{CorrelationId, TraceContext};\npub use crate::job_definitions::{ExecResult, JobId, JobSpec, JobState, ResourceQuota};\n\n/// Tenant identifier for multi-tenancy support\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct TenantId(String);\n\nimpl TenantId {\n    pub fn new(id: String) -\u003e Self {\n        Self(id)\n    }\n\n    pub fn as_str(\u0026self) -\u003e \u0026str {\n        \u0026self.0\n    }\n}\n\nimpl From\u003cString\u003e for TenantId {\n    fn from(s: String) -\u003e Self {\n        TenantId::new(s)\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","crates","shared-types","src","worker_messages.rs"],"content":"//! Worker-related message types for distributed communication\n\nuse serde::{Deserialize, Serialize};\nuse uuid::Uuid;\n\n/// Worker identifier\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct WorkerId(pub Uuid);\n\nimpl WorkerId {\n    pub fn new() -\u003e Self {\n        Self(Uuid::new_v4())\n    }\n\n    pub fn from_uuid(uuid: Uuid) -\u003e Self {\n        Self(uuid)\n    }\n\n    pub fn as_uuid(\u0026self) -\u003e Uuid {\n        self.0\n    }\n}\n\nimpl Default for WorkerId {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Worker state\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\npub enum WorkerState {\n    Creating,\n    Available,\n    Running,\n    Unhealthy,\n    Draining,\n    Terminated,\n    Failed { reason: String },\n}\n\n/// Runtime specification for worker\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct RuntimeSpec {\n    pub image: String,\n    pub command: Option\u003cVec\u003cString\u003e\u003e,\n    pub resources: crate::job_definitions::ResourceQuota,\n    pub env: std::collections::HashMap\u003cString, String\u003e,\n    pub labels: std::collections::HashMap\u003cString, String\u003e,\n}\n\nimpl RuntimeSpec {\n    pub fn new(image: String) -\u003e Self {\n        Self {\n            image,\n            command: None,\n            resources: crate::job_definitions::ResourceQuota::new(100, 512),\n            env: std::collections::HashMap::new(),\n            labels: std::collections::HashMap::new(),\n        }\n    }\n}\n\n/// Worker capabilities for matching\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub struct WorkerCapabilities {\n    pub cpu_m: u64,\n    pub memory_mb: u64,\n    pub gpu: Option\u003cu8\u003e,\n    pub features: Vec\u003cString\u003e,\n}\n","traces":[{"line":11,"address":[5294720],"length":1,"stats":{"Line":1}},{"line":12,"address":[5294734],"length":1,"stats":{"Line":1}},{"line":15,"address":[5294784],"length":1,"stats":{"Line":0}},{"line":16,"address":[5294787],"length":1,"stats":{"Line":0}},{"line":19,"address":[5294816],"length":1,"stats":{"Line":0}},{"line":20,"address":[5294824],"length":1,"stats":{"Line":0}},{"line":25,"address":[5294848],"length":1,"stats":{"Line":0}},{"line":26,"address":[5294856],"length":1,"stats":{"Line":0}},{"line":53,"address":[5294880,5295210,5295232],"length":1,"stats":{"Line":0}},{"line":57,"address":[5294928],"length":1,"stats":{"Line":0}},{"line":58,"address":[5294995],"length":1,"stats":{"Line":0}},{"line":59,"address":[5295011],"length":1,"stats":{"Line":0}}],"covered":2,"coverable":12},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","examples","scheduler_worker_integration.rs"],"content":"//! Scheduler and Worker Lifecycle Integration Example\n//!\n//! This example demonstrates how to integrate the Kubernetes-style scheduler\n//! framework with the worker lifecycle management system for the Hodei Jobs\n//! distributed CI/CD platform.\n//!\n//! Run with: cargo run --example scheduler_worker_integration\n\nuse hodei_scheduler::backend::{ComputeResource, KubernetesBackend, SchedulerBackend};\nuse hodei_scheduler::integration::SchedulerWorkerIntegration;\nuse hodei_scheduler::types::{\n    BackendSpecific, BackendType, Job, JobMetadata, JobPriority, JobSpec, KubernetesNodeSpecific,\n    LabelSelector, LabelSelectorOperator, NodeAffinity, NodeLocation, NodeSelector,\n    ResourceRequirements, WeightedLabelSelector, WorkerNode,\n};\nuse hodei_shared_types::TenantId;\nuse hodei_worker_lifecycle::{Worker, WorkerCapabilities, WorkerManager};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::time::Duration;\nuse uuid::Uuid;\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n    tracing_subscriber::fmt::init();\n\n    println!(\" Starting Scheduler-Worker Lifecycle Integration Example\\n\");\n\n    // Step 1: Create Worker Lifecycle Manager\n    println!(\" Step 1: Creating Worker Lifecycle Manager\");\n    let worker_manager = Arc::new(WorkerManager::new(\n        None,                    // No coordinator needed for this example\n        Duration::from_secs(30), // Health check interval\n        Duration::from_secs(10), // Heartbeat timeout\n    ));\n\n    // Step 2: Create Scheduler Backend (Mock for example)\n    println!(\"  Step 2: Creating Mock Scheduler Backend\");\n    let backend = Arc::new(KubernetesBackend::new());\n\n    // Step 3: Create the Integration Coordinator\n    println!(\" Step 3: Creating Scheduler-Worker Integration Coordinator\");\n    let integration = SchedulerWorkerIntegration::new(backend.clone(), worker_manager.clone());\n\n    // Step 4: Start the integration\n    println!(\"  Starting integration coordinator...\");\n    integration.start().await?;\n    println!(\" Integration coordinator started successfully\\n\");\n\n    // Step 5: Register workers in lifecycle manager\n    println!(\" Step 5: Registering Workers\");\n    let worker_ids = register_workers(\u0026worker_manager).await?;\n    println!(\" Registered {} workers\\n\", worker_ids.len());\n\n    // Step 6: Create a sample job\n    println!(\" Step 6: Creating Sample Job\");\n    let job = create_sample_job();\n    println!(\n        \" Created job: {} (priority: {})\\n\",\n        job.metadata.name, job.spec.priority\n    );\n\n    // Step 7: Find suitable workers for the job\n    println!(\" Step 7: Finding Suitable Workers\");\n    match integration.find_suitable_workers(\u0026job).await {\n        Ok(workers) =\u003e {\n            println!(\" Found {} suitable workers:\", workers.len());\n            for worker in \u0026workers {\n                println!(\"   - Worker ID: {}\", worker.id);\n                println!(\n                    \"     CPU: {:.1} cores, Memory: {} bytes\",\n                    worker.resources.cpu_cores, worker.resources.memory_bytes\n                );\n            }\n            println!();\n\n            // Step 8: Bind job to a worker\n            if let Some(first_worker) = workers.first() {\n                println!(\" Step 8: Binding Job to Worker\");\n                let job_id = job.metadata.id;\n                let worker_id = first_worker.id;\n\n                integration.bind_job_to_worker(\u0026job_id, \u0026worker_id).await?;\n                println!(\n                    \" Successfully bound job {} to worker {}\\n\",\n                    job_id, worker_id\n                );\n\n                // Step 9: Verify binding\n                println!(\" Step 9: Verifying Job Binding\");\n                assert!(integration.is_job_bound(\u0026job_id));\n                assert_eq!(integration.get_job_worker(\u0026job_id), Some(worker_id));\n\n                let binding_map = integration.get_job_to_worker_map();\n                println!(\"   Active bindings: {}\", binding_map.len());\n                println!(\"   Job {} is bound to worker {}\\n\", job_id, worker_id);\n\n                // Step 10: Simulate job completion\n                println!(\" Step 10: Simulating Job Completion\");\n                integration\n                    .unbind_job_from_worker(\u0026job_id, \u0026worker_id)\n                    .await?;\n                println!(\" Job unbound from worker\\n\");\n            }\n        }\n        Err(e) =\u003e {\n            println!(\" No suitable workers found: {}\", e);\n        }\n    }\n\n    // Step 11: Test worker failure handling\n    println!(\" Step 11: Testing Worker Failure Handling\");\n    if let Some(first_worker_id) = worker_ids.first() {\n        println!(\"   Simulating failure for worker: {}\", first_worker_id);\n        integration.handle_worker_failed(*first_worker_id).await?;\n        println!(\" Worker failure handled successfully\\n\");\n    }\n\n    // Step 12: Get worker status\n    println!(\" Step 12: Getting Worker Status Summary\");\n    let worker_summaries = integration.get_workers_summary().await;\n    println!(\"   Total workers: {}\", worker_summaries.len());\n\n    for summary in worker_summaries {\n        println!(\n            \"   - Worker {}: state={}, load={:.2}, jobs={}\",\n            summary.worker_id, summary.state, summary.load, summary.jobs_running\n        );\n    }\n    println!();\n\n    // Step 13: Stop the integration\n    println!(\"  Step 13: Stopping Integration Coordinator\");\n    integration.stop().await?;\n    println!(\" Integration coordinator stopped successfully\\n\");\n\n    println!(\" Example completed successfully!\");\n\n    Ok(())\n}\n\n/// Register sample workers in the lifecycle manager\nasync fn register_workers(\n    worker_manager: \u0026Arc\u003cWorkerManager\u003e,\n) -\u003e Result\u003cVec\u003cUuid\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n    let mut worker_ids = Vec::new();\n\n    // Worker 1: High-capacity node\n    let worker1_id = Uuid::new_v4();\n    worker_manager.register_worker(Worker::new(\n        worker1_id,\n        WorkerCapabilities {\n            cpu_cores: 16,\n            memory_gb: 64,\n            has_gpu: true,\n            gpu_count: Some(2),\n            specialized_hardware: vec![\"nvme-ssd\".to_string()],\n            container_runtime: \"containerd\".to_string(),\n        },\n        10, // max_jobs\n        None,\n    ))?;\n    worker_ids.push(worker1_id);\n    println!(\"    Registered worker 1 (ID: {})\", worker1_id);\n\n    // Worker 2: Medium-capacity node\n    let worker2_id = Uuid::new_v4();\n    worker_manager.register_worker(Worker::new(\n        worker2_id,\n        WorkerCapabilities {\n            cpu_cores: 8,\n            memory_gb: 32,\n            has_gpu: false,\n            gpu_count: None,\n            specialized_hardware: vec![],\n            container_runtime: \"docker\".to_string(),\n        },\n        5, // max_jobs\n        None,\n    ))?;\n    worker_ids.push(worker2_id);\n    println!(\"    Registered worker 2 (ID: {})\", worker2_id);\n\n    // Worker 3: GPU node\n    let worker3_id = Uuid::new_v4();\n    worker_manager.register_worker(Worker::new(\n        worker3_id,\n        WorkerCapabilities {\n            cpu_cores: 4,\n            memory_gb: 16,\n            has_gpu: true,\n            gpu_count: Some(1),\n            specialized_hardware: vec![\"cuda\".to_string()],\n            container_runtime: \"docker\".to_string(),\n        },\n        3, // max_jobs\n        None,\n    ))?;\n    worker_ids.push(worker3_id);\n    println!(\"    Registered worker 3 (ID: {})\", worker3_id);\n\n    Ok(worker_ids)\n}\n\n/// Create a sample job with resource requirements\nfn create_sample_job() -\u003e Job {\n    Job {\n        metadata: JobMetadata {\n            id: Uuid::new_v4(),\n            name: \"ci-pipeline-build\".to_string(),\n            namespace: \"default\".to_string(),\n            labels: {\n                let mut labels = HashMap::new();\n                labels.insert(\"app\".to_string(), \"hodei-jobs\".to_string());\n                labels.insert(\"tier\".to_string(), \"ci\".to_string());\n                labels\n            },\n            created_at: chrono::Utc::now(),\n        },\n        spec: JobSpec {\n            resource_requirements: Some(ResourceRequirements {\n                cpu_cores: Some(4.0),\n                memory_bytes: Some(8_000_000_000), // 8 GB\n                gpu_count: Some(1),\n                ephemeral_storage: Some(20_000_000_000), // 20 GB\n            }),\n            priority: JobPriority::High,\n            node_selector: Some(NodeSelector {\n                labels: {\n                    let mut labels = HashMap::new();\n                    labels.insert(\"node-type\".to_string(), \"compute\".to_string());\n                    labels\n                },\n            }),\n            affinity: Some(NodeAffinity {\n                required_during_scheduling: vec![LabelSelector {\n                    key: \"rack\".to_string(),\n                    operator: LabelSelectorOperator::In,\n                    values: Some(vec![\"rack-a\".to_string(), \"rack-b\".to_string()]),\n                }],\n                preferred_during_scheduling: vec![WeightedLabelSelector {\n                    selector: LabelSelector {\n                        key: \"gpu\".to_string(),\n                        operator: LabelSelectorOperator::Exists,\n                        values: None,\n                    },\n                    weight: 10,\n                }],\n            }),\n            tolerations: vec![],\n            max_retries: 3,\n        },\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","aws_secrets.rs"],"content":"//! AWS Secrets Manager Provider Implementation\n//! \n//! This module provides a complete integration with AWS Secrets Manager for\n//! secure credential management and storage.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::time::Duration;\n\n/// AWS Secrets Manager configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AWSSecretsConfig {\n    pub region: String,\n    pub profile: Option\u003cString\u003e,\n    pub access_key_id: Option\u003cString\u003e,\n    pub secret_access_key: Option\u003cString\u003e,\n    pub session_token: Option\u003cString\u003e,\n    pub timeout: Duration,\n    pub max_retries: u32,\n}\n\n/// AWS Secrets Manager client wrapper\n#[derive(Debug)]\npub struct AWSSecretsClient {\n    client: aws_sdk_secretsmanager::Client,\n    config: AWSSecretsConfig,\n}\n\nimpl AWSSecretsClient {\n    /// Create new AWS Secrets Manager client\n    pub async fn new(config: AWSSecretsConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let aws_config = match (\u0026config.access_key_id, \u0026config.secret_access_key) {\n            (Some(key_id), Some(secret_key)) =\u003e {\n                aws_config::Config::builder()\n                    .region(config.region.as_ref().try_into().map_err(|_| CredentialError::Configuration {\n                        message: format!(\"Invalid AWS region: {}\", config.region)\n                    })?)\n                    .credentials_provider(aws_sdk_secretsmanager::config::ProvideCredentials::ClientCredentials {\n                        access_key_id: key_id.clone(),\n                        secret_access_key: secret_key.clone(),\n                        session_token: config.session_token.clone(),\n                    })\n                    .build()\n            }\n            _ =\u003e {\n                // Use default credential chain\n                aws_config::Config::builder()\n                    .region(config.region.as_ref().try_into().map_err(|_| CredentialError::Configuration {\n                        message: format!(\"Invalid AWS region: {}\", config.region)\n                    })?)\n                    .build()\n            }\n        };\n\n        let client = aws_sdk_secretsmanager::Client::from_conf(aws_config);\n        Ok(Self { client, config })\n    }\n\n    /// Get secret from AWS Secrets Manager\n    pub async fn get_secret(\u0026self, secret_id: \u0026str) -\u003e Result\u003cSecretString, CredentialError\u003e {\n        let request = self.client.get_secret_value()\n            .secret_id(secret_id);\n\n        match request.send().await {\n            Ok(output) =\u003e {\n                if let Some(secret_string) = output.secret_string() {\n                    Ok(secret_string.to_string())\n                } else if let Some(secret_binary) = output.secret_binary() {\n                    // Decode binary secret to string\n                    let bytes = secret_binary.as_ref();\n                    Ok(String::from_utf8_lossy(bytes).to_string())\n                } else {\n                    Err(CredentialError::NotFound { \n                        name: secret_id.to_string() \n                    })\n                }\n            }\n            Err(error) =\u003e {\n                if error.code() == Some(\"ResourceNotFoundException\") {\n                    Err(CredentialError::NotFound { \n                        name: secret_id.to_string() \n                    })\n                } else {\n                    Err(CredentialError::Network { \n                        message: format!(\"AWS Secrets Manager error: {}\", error) \n                    })\n                }\n            }\n        }\n    }\n\n    /// Put secret to AWS Secrets Manager\n    pub async fn put_secret(\u0026self, secret_id: \u0026str, secret_string: \u0026str, description: Option\u003c\u0026str\u003e) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut request = self.client.put_secret_value()\n            .secret_id(secret_id)\n            .secret_string(secret_string);\n\n        if let Some(desc) = description {\n            request = request.description(desc);\n        }\n\n        match request.send().await {\n            Ok(_) =\u003e Ok(()),\n            Err(error) =\u003e {\n                if error.code() == Some(\"ResourceNotFoundException\") {\n                    // Secret doesn't exist, create it\n                    let mut request = self.client.create_secret()\n                        .name(secret_id)\n                        .secret_string(secret_string);\n\n                    if let Some(desc) = description {\n                        request = request.description(desc);\n                    }\n\n                    match request.send().await {\n                        Ok(_) =\u003e Ok(()),\n                        Err(create_error) =\u003e {\n                            Err(CredentialError::Storage { \n                                message: format!(\"Failed to create secret: {}\", create_error) \n                            })\n                        }\n                    }\n                } else {\n                    Err(CredentialError::Storage { \n                        message: format!(\"AWS Secrets Manager put error: {}\", error) \n                    })\n                }\n            }\n        }\n    }\n\n    /// List secrets in AWS Secrets Manager\n    pub async fn list_secrets(\u0026self) -\u003e Result\u003cVec\u003cString\u003e, CredentialError\u003e {\n        let mut secrets = Vec::new();\n        let mut next_token = None;\n\n        loop {\n            let mut request = self.client.list_secrets();\n\n            if let Some(token) = next_token {\n                request = request.next_token(token);\n            }\n\n            match request.send().await {\n                Ok(output) =\u003e {\n                    if let Some(secret_list) = output.secret_list() {\n                        for secret in secret_list {\n                            if let Some(name) = secret.name() {\n                                secrets.push(name.to_string());\n                            }\n                        }\n                    }\n\n                    if output.next_token().is_some() {\n                        next_token = output.next_token().map(|t| t.to_string());\n                    } else {\n                        break;\n                    }\n                }\n                Err(error) =\u003e {\n                    return Err(CredentialError::Network { \n                        message: format!(\"AWS list secrets error: {}\", error) \n                    });\n                }\n            }\n        }\n\n        Ok(secrets)\n    }\n\n    /// Delete secret from AWS Secrets Manager\n    pub async fn delete_secret(\u0026self, secret_id: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let request = self.client.delete_secret()\n            .secret_id(secret_id);\n\n        match request.send().await {\n            Ok(_) =\u003e Ok(()),\n            Err(error) =\u003e {\n                if error.code() == Some(\"ResourceNotFoundException\") {\n                    // Secret doesn't exist, consider it already deleted\n                    Ok(())\n                } else {\n                    Err(CredentialError::Storage { \n                        message: format!(\"AWS delete secret error: {}\", error) \n                    })\n                }\n            }\n        }\n    }\n\n    /// Rotate secret in AWS Secrets Manager\n    pub async fn rotate_secret(\u0026self, secret_id: \u0026str, new_secret_string: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let request = self.client.rotate_secret()\n            .secret_id(secret_id)\n            .client_request_token(format!(\"rotation_{}\", chrono::Utc::now().timestamp_millis()));\n\n        match request.send().await {\n            Ok(_) =\u003e Ok(()),\n            Err(error) =\u003e {\n                Err(CredentialError::RotationFailed { \n                    name: secret_id.to_string() \n                })\n            }\n        }\n    }\n\n    /// Get secret versions\n    pub async fn list_secret_versions(\u0026self, secret_id: \u0026str) -\u003e Result\u003cVec\u003cSecretVersionInfo\u003e, CredentialError\u003e {\n        let request = self.client.list_secret_version_ids()\n            .secret_id(secret_id);\n\n        match request.send().await {\n            Ok(output) =\u003e {\n                let mut versions = Vec::new();\n                if let Some(version_ids) = output.secret_version_ids() {\n                    for version in version_ids {\n                        let version_info = SecretVersionInfo {\n                            version_id: version.version_id().unwrap_or(\"unknown\").to_string(),\n                            is_current_version: version.is_current_version().unwrap_or(false),\n                            created_date: version.created_date()\n                                .map(|d| chrono::DateTime::from_utc(\n                                    chrono::NaiveDateTime::from_timestamp_opt(d.as_secs(), 0).unwrap_or_default(),\n                                    chrono::Utc\n                                )),\n                        };\n                        versions.push(version_info);\n                    }\n                }\n                Ok(versions)\n            }\n            Err(error) =\u003e {\n                Err(CredentialError::Network { \n                    message: format!(\"AWS list secret versions error: {}\", error) \n                })\n            }\n        }\n    }\n}\n\n/// Secret version information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SecretVersionInfo {\n    pub version_id: String,\n    pub is_current_version: bool,\n    pub created_date: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\n/// AWS Secrets Manager Credential Provider\n#[derive(Debug)]\npub struct AWSSecretsManagerProvider {\n    client: AWSSecretsClient,\n}\n\nimpl AWSSecretsManagerProvider {\n    /// Create new AWS Secrets Manager Provider\n    pub async fn new(config: AWSSecretsConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = AWSSecretsClient::new(config).await?;\n        Ok(Self { client })\n    }\n\n    /// Convert AWS secret string to Credential format\n    fn aws_secret_to_credential(\u0026self, secret_string: \u0026str, secret_id: \u0026str) -\u003e Result\u003cHashMap\u003cString, String\u003e, CredentialError\u003e {\n        // Try to parse as JSON first\n        if let Ok(json_value) = serde_json::from_str::\u003cserde_json::Value\u003e(secret_string) {\n            let mut values = HashMap::new();\n            \n            if json_value.is_object() {\n                for (key, value) in json_value.as_object().unwrap() {\n                    if let Some(value_str) = value.as_str() {\n                        values.insert(key.clone(), value_str.to_string());\n                    } else if value.is_number() {\n                        values.insert(key.clone(), value.to_string());\n                    } else if value.is_boolean() {\n                        values.insert(key.clone(), value.to_string());\n                    } else {\n                        values.insert(key.clone(), serde_json::to_string(value).unwrap_or_else(|_| \"null\".to_string()));\n                    }\n                }\n            } else {\n                // Treat as a single value with a default key\n                values.insert(\"value\".to_string(), secret_string.to_string());\n            }\n            \n            Ok(values)\n        } else {\n            // Not JSON, treat as plain text\n            Ok(HashMap::from([(\"value\".to_string(), secret_string.to_string())]))\n        }\n    }\n\n    /// Convert Credential to AWS secret string\n    fn credential_to_aws_secret(\u0026self, credential: \u0026Credential) -\u003e String {\n        if credential.values.len() == 1 {\n            // Single value, return as plain text\n            credential.values.values().next().unwrap().clone()\n        } else {\n            // Multiple values, return as JSON\n            serde_json::to_string(\u0026credential.values).unwrap_or_else(|_| \"{}\".to_string())\n        }\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for AWSSecretsManagerProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        // AWS doesn't support getting specific versions via the basic API\n        // This would need additional implementation for version-specific access\n        let secret_id = format!(\"secret/{}\", name); // Assuming secrets are stored under \"secret/\" prefix\n        \n        let secret_string = self.client.get_secret(\u0026secret_id).await?;\n        let values = self.aws_secret_to_credential(\u0026secret_string, \u0026secret_id)?;\n\n        let credential = Credential {\n            name: name.to_string(),\n            values,\n            metadata: HashMap::new(),\n            created_at: chrono::Utc::now(),\n            updated_at: chrono::Utc::now(),\n            expires_at: None,\n            rotation_enabled: false,\n            access_policy: None,\n        };\n\n        let version = version.unwrap_or(\"current\").to_string();\n        \n        Ok(VersionedCredential {\n            credential,\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", credential.name);\n        let secret_string = self.credential_to_aws_secret(credential);\n        \n        self.client.put_secret(\u0026secret_id, \u0026secret_string, Some(\u0026format!(\"Secret for {}\", credential.name))).await?;\n        \n        let version = format!(\"v{}_{}\", chrono::Utc::now().timestamp_millis(), chrono::Utc::now().timestamp_nanos());\n        \n        Ok(VersionedCredential {\n            credential: credential.clone(),\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        let versions = self.client.list_secret_versions(\u0026secret_id).await?;\n        \n        let mut versioned_credentials = Vec::new();\n        let secret_string = self.client.get_secret(\u0026secret_id).await?;\n        let values = self.aws_secret_to_credential(\u0026secret_string, \u0026secret_id)?;\n        \n        for version_info in versions {\n            versioned_credentials.push(VersionedCredential {\n                credential: Credential {\n                    name: name.to_string(),\n                    values: values.clone(),\n                    metadata: HashMap::new(),\n                    created_at: version_info.created_date.unwrap_or(chrono::Utc::now()),\n                    updated_at: chrono::Utc::now(),\n                    expires_at: None,\n                    rotation_enabled: false,\n                    access_policy: None,\n                },\n                version: version_info.version_id,\n                version_created_at: version_info.created_date.unwrap_or(chrono::Utc::now()),\n                is_active: version_info.is_current_version,\n            });\n        }\n        \n        Ok(versioned_credentials)\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let secrets = self.client.list_secrets().await?;\n        \n        let mut credentials = Vec::new();\n        for secret_name in secrets {\n            if let Ok(version) = self.get_credential(\u0026secret_name, None).await {\n                credentials.push(version);\n            }\n        }\n        \n        Ok(credentials)\n    }\n\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        self.client.delete_secret(\u0026secret_id).await?;\n        Ok(())\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        match self.client.get_secret(\u0026secret_id).await {\n            Ok(_) =\u003e Ok(true),\n            Err(CredentialError::NotFound { .. }) =\u003e Ok(false),\n            Err(e) =\u003e Err(e),\n        }\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        let secret_id = format!(\"secret/{}\", name);\n        let current = self.get_credential(name, None).await?;\n        \n        // Create rotated version\n        let mut rotated_credential = current.credential.clone();\n        rotated_credential.updated_at = chrono::Utc::now();\n        \n        match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                rotated_credential.expires_at = Some(chrono::Utc::now() + time_based.rotation_interval);\n            }\n            RotationStrategy::EventBased(_) =\u003e {\n                rotated_credential.expires_at = Some(chrono::Utc::now() + chrono::Duration::days(30));\n            }\n            RotationStrategy::Manual =\u003e {\n                // Keep current expiry\n            }\n        }\n\n        // Use AWS Secrets Manager rotation API\n        let new_secret_string = self.credential_to_aws_secret(\u0026rotated_credential);\n        self.client.rotate_secret(\u0026secret_id, \u0026new_secret_string).await?;\n        \n        Ok(RotationResult {\n            success: true,\n            rotated_credential: Some(name.to_string()),\n            old_credential_version: Some(current.version),\n            new_credential_version: Some(\"rotated\".to_string()),\n            rotated_at: chrono::Utc::now(),\n            error_message: None,\n        })\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        // AWS Secrets Manager doesn't have built-in access policies in the secret itself\n        // This would need to be managed via IAM policies and AWS Verified Permissions\n        Ok(None)\n    }\n\n    async fn set_access_policy(\u0026self, _name: \u0026str, _policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        // Not applicable for AWS Secrets Manager\n        Ok(())\n    }\n\n    async fn has_permission(\u0026self, _name: \u0026str, _subject: \u0026str, _permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        // Permission checking would be done via AWS IAM/Verified Permissions\n        // For now, return true\n        Ok(true)\n    }\n\n    async fn audit_operation(\u0026self, _event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // In production, you would integrate with AWS CloudTrail for auditing\n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        let credentials = self.list_credentials().await?;\n        \n        Ok(CredentialStatistics {\n            total_credentials: credentials.len() as u64,\n            active_credentials: credentials.len() as u64,\n            credential_versions: credentials.len() as u64,\n            rotation_statistics: RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                next_scheduled_rotation: None,\n            },\n            last_accessed: None,\n        })\n    }\n}\n\n/// Service Account credentials for AWS STS assume role\n#[derive(Debug, Clone)]\npub struct AWSCredentials {\n    pub access_key_id: String,\n    pub secret_access_key: String,\n    pub session_token: Option\u003cString\u003e,\n}\n\n/// AWS STS Service for assuming roles\npub struct AWSSTSService {\n    client: aws_sdk_sts::Client,\n}\n\nimpl AWSSTSService {\n    /// Create new AWS STS service\n    pub fn new() -\u003e Result\u003cSelf, CredentialError\u003e {\n        let aws_config = aws_config::Config::builder()\n            .region(\"us-east-1\")\n            .build();\n        let client = aws_sdk_sts::Client::from_conf(aws_config);\n        Ok(Self { client })\n    }\n\n    /// Assume role and get temporary credentials\n    pub async fn assume_role(\n        \u0026self,\n        role_arn: \u0026str,\n        role_session_name: \u0026str,\n        duration_seconds: Option\u003ci32\u003e,\n    ) -\u003e Result\u003cAWSCredentials, CredentialError\u003e {\n        let mut request = self.client.assume_role()\n            .role_arn(role_arn)\n            .role_session_name(role_session_name);\n\n        if let Some(duration) = duration_seconds {\n            request = request.duration_seconds(duration);\n        }\n\n        match request.send().await {\n            Ok(output) =\u003e {\n                if let Some(credentials) = output.credentials() {\n                    let access_key_id = credentials.access_key_id().unwrap_or(\"\").to_string();\n                    let secret_access_key = credentials.secret_access_key().unwrap_or(\"\").to_string();\n                    let session_token = credentials.session_token().map(|t| t.to_string());\n\n                    Ok(AWSCredentials {\n                        access_key_id,\n                        secret_access_key,\n                        session_token,\n                    })\n                } else {\n                    Err(CredentialError::InternalError)\n                }\n            }\n            Err(error) =\u003e {\n                Err(CredentialError::Network {\n                    message: format!(\"AWS STS assume role error: {}\", error)\n                })\n            }\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","keycloak.rs"],"content":"//! Keycloak Service Account Provider Implementation\n//! \n//! This module provides a complete integration with Keycloak for\n//! Service Account authentication and token management.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::time::Duration;\n\n/// Keycloak configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KeycloakConfig {\n    pub base_url: String,\n    pub realm: String,\n    pub client_id: String,\n    pub client_secret: Option\u003cString\u003e,\n    pub scopes: Vec\u003cString\u003e,\n    pub token_endpoint: Option\u003cString\u003e,\n    pub jwks_endpoint: Option\u003cString\u003e,\n    pub timeout: Duration,\n    pub cache_ttl: Duration,\n}\n\n/// Keycloak token information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KeycloakToken {\n    pub access_token: String,\n    pub refresh_token: Option\u003cString\u003e,\n    pub token_type: String,\n    pub expires_in: i64,\n    pub scope: Option\u003cString\u003e,\n    pub issued_at: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Keycloak User information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KeycloakUser {\n    pub id: String,\n    pub username: String,\n    pub email: Option\u003cString\u003e,\n    pub first_name: Option\u003cString\u003e,\n    pub last_name: Option\u003cString\u003e,\n    pub enabled: bool,\n    pub email_verified: bool,\n}\n\n/// Keycloak Service Account client\n#[derive(Debug)]\npub struct KeycloakClient {\n    client: reqwest::Client,\n    config: KeycloakConfig,\n    token_cache: std::sync::RwLock\u003cHashMap\u003cString, (KeycloakToken, chrono::DateTime\u003cchrono::Utc\u003e)\u003e\u003e,\n}\n\nimpl KeycloakClient {\n    /// Create new Keycloak client\n    pub fn new(config: KeycloakConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = reqwest::Client::builder()\n            .timeout(config.timeout)\n            .danger_accept_invalid_certs(false)\n            .build()\n            .map_err(|e| CredentialError::Network { \n                message: format!(\"Failed to create Keycloak HTTP client: {}\", e) \n            })?;\n\n        Ok(Self {\n            client,\n            config,\n            token_cache: std::sync::RwLock::new(HashMap::new()),\n        })\n    }\n\n    /// Get cached token for client credentials\n    async fn get_cached_token(\u0026self, key: \u0026str) -\u003e Option\u003cKeycloakToken\u003e {\n        if let Ok(cache) = self.token_cache.read() {\n            if let Some((token, expires_at)) = cache.get(key) {\n                if chrono::Utc::now() \u003c *expires_at {\n                    return Some(token.clone());\n                }\n            }\n        }\n        None\n    }\n\n    /// Cache token\n    fn cache_token(\u0026self, key: String, token: KeycloakToken) {\n        let expires_at = token.issued_at + chrono::Duration::seconds(token.expires_in - 60); // 60s buffer\n        if let Ok(mut cache) = self.token_cache.write() {\n            cache.insert(key, (token, expires_at));\n        }\n    }\n\n    /// Get access token using client credentials flow\n    pub async fn get_access_token(\u0026self) -\u003e Result\u003cKeycloakToken, CredentialError\u003e {\n        let cache_key = format!(\"client_credentials:{}\", self.config.client_id);\n        \n        // Check cache first\n        if let Some(token) = self.get_cached_token(\u0026cache_key).await {\n            return Ok(token);\n        }\n\n        // Get token from Keycloak\n        let token_data = self.fetch_token().await?;\n        self.cache_token(cache_key, token_data.clone());\n\n        Ok(token_data)\n    }\n\n    /// Fetch token from Keycloak server\n    async fn fetch_token(\u0026self) -\u003e Result\u003cKeycloakToken, CredentialError\u003e {\n        let token_endpoint = self.get_token_endpoint().await?;\n        \n        let mut form_data = HashMap::from([\n            (\"grant_type\".to_string(), \"client_credentials\".to_string()),\n            (\"client_id\".to_string(), self.config.client_id.clone()),\n            (\"scope\".to_string(), self.config.scopes.join(\" \")),\n        ]);\n\n        if let Some(secret) = \u0026self.config.client_secret {\n            form_data.insert(\"client_secret\".to_string(), secret.clone());\n        }\n\n        let response = self.client\n            .post(\u0026token_endpoint)\n            .form(\u0026form_data)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get Keycloak token: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            return Err(CredentialError::Authentication {\n                message: format!(\"Keycloak authentication failed: HTTP {} - {}\", status, body)\n            });\n        }\n\n        let token_response: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Keycloak token response: {}\", e)\n            })?;\n\n        let token_data = KeycloakToken {\n            access_token: token_response.get(\"access_token\")\n                .and_then(|t| t.as_str())\n                .ok_or_else(|| CredentialError::InvalidFormat {\n                    name: \"access_token\".to_string()\n                })?.to_string(),\n            refresh_token: token_response.get(\"refresh_token\").and_then(|t| t.as_str()).map(|s| s.to_string()),\n            token_type: token_response.get(\"token_type\")\n                .and_then(|t| t.as_str())\n                .unwrap_or(\"Bearer\").to_string(),\n            expires_in: token_response.get(\"expires_in\")\n                .and_then(|e| e.as_i64())\n                .unwrap_or(3600),\n            scope: token_response.get(\"scope\").and_then(|s| s.as_str()).map(|s| s.to_string()),\n            issued_at: chrono::Utc::now(),\n        };\n\n        Ok(token_data)\n    }\n\n    /// Get Keycloak token endpoint\n    async fn get_token_endpoint(\u0026self) -\u003e Result\u003cString, CredentialError\u003e {\n        if let Some(endpoint) = \u0026self.config.token_endpoint {\n            return Ok(endpoint.clone());\n        }\n\n        // Discover token endpoint from realm info\n        let well_known_url = format!(\"{}/realms/{}/.well-known/openid-configuration\", \n                                   self.config.base_url, self.config.realm);\n        \n        let response = self.client\n            .get(\u0026well_known_url)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get Keycloak OIDC configuration: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to get Keycloak OIDC configuration: HTTP {}\", response.status())\n            });\n        }\n\n        let oidc_config: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Keycloak OIDC configuration: {}\", e)\n            })?;\n\n        let token_endpoint = oidc_config.get(\"token_endpoint\")\n            .and_then(|e| e.as_str())\n            .ok_or_else(|| CredentialError::Configuration {\n                message: \"Token endpoint not found in OIDC configuration\".to_string()\n            })?.to_string();\n\n        Ok(token_endpoint)\n    }\n\n    /// Get user information using access token\n    pub async fn get_user_info(\u0026self, access_token: \u0026str) -\u003e Result\u003cKeycloakUser, CredentialError\u003e {\n        let user_info_url = format!(\"{}/realms/{}/protocol/openid-connect/userinfo\", \n                                  self.config.base_url, self.config.realm);\n\n        let response = self.client\n            .get(\u0026user_info_url)\n            .bearer_auth(access_token)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get Keycloak user info: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Authentication {\n                message: format!(\"Failed to get user info: HTTP {}\", response.status())\n            });\n        }\n\n        let user_info: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Keycloak user info: {}\", e)\n            })?;\n\n        let user = KeycloakUser {\n            id: user_info.get(\"sub\")\n                .and_then(|s| s.as_str())\n                .ok_or_else(|| CredentialError::InvalidFormat { name: \"sub\".to_string() })?.to_string(),\n            username: user_info.get(\"preferred_username\")\n                .and_then(|u| u.as_str())\n                .unwrap_or(\"unknown\").to_string(),\n            email: user_info.get(\"email\").and_then(|e| e.as_str()).map(|s| s.to_string()),\n            first_name: user_info.get(\"given_name\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            last_name: user_info.get(\"family_name\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            enabled: true, // Would need to check user status via admin API\n            email_verified: user_info.get(\"email_verified\")\n                .and_then(|v| v.as_bool())\n                .unwrap_or(false),\n        };\n\n        Ok(user)\n    }\n\n    /// Get JWKS for token validation\n    pub async fn get_jwks(\u0026self) -\u003e Result\u003cserde_json::Value, CredentialError\u003e {\n        let jwks_url = self.get_jwks_endpoint().await?;\n\n        let response = self.client\n            .get(\u0026jwks_url)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get JWKS: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to get JWKS: HTTP {}\", response.status())\n            });\n        }\n\n        response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse JWKS: {}\", e)\n            })\n    }\n\n    /// Get JWKS endpoint\n    async fn get_jwks_endpoint(\u0026self) -\u003e Result\u003cString, CredentialError\u003e {\n        if let Some(endpoint) = \u0026self.config.jwks_endpoint {\n            return Ok(endpoint.clone());\n        }\n\n        // Discover JWKS endpoint from OIDC configuration\n        let well_known_url = format!(\"{}/realms/{}/.well-known/openid-configuration\", \n                                   self.config.base_url, self.config.realm);\n        \n        let response = self.client\n            .get(\u0026well_known_url)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get OIDC configuration for JWKS: {}\", e)\n            })?;\n\n        let oidc_config: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse OIDC configuration for JWKS: {}\", e)\n            })?;\n\n        let jwks_uri = oidc_config.get(\"jwks_uri\")\n            .and_then(|e| e.as_str())\n            .ok_or_else(|| CredentialError::Configuration {\n                message: \"JWKS URI not found in OIDC configuration\".to_string()\n            })?.to_string();\n\n        Ok(jwks_uri)\n    }\n\n    /// Validate and decode JWT token\n    pub async fn validate_token(\u0026self, token: \u0026str) -\u003e Result\u003cserde_json::Value, CredentialError\u003e {\n        // In a production environment, you would use a JWT library to validate the token\n        // This is a simplified implementation\n        let jwks = self.get_jwks().await?;\n        \n        // For now, just decode the base64 payload (NOT SECURE FOR PRODUCTION)\n        let parts: Vec\u003c\u0026str\u003e = token.split('.').collect();\n        if parts.len() != 3 {\n            return Err(CredentialError::InvalidFormat { name: \"jwt\".to_string() });\n        }\n\n        let payload_b64 = parts[1];\n        let payload_bytes = base64::Engine::decode(\u0026base64::engine::general_purpose::URL_SAFE_NO_PAD, payload_b64)\n            .map_err(|_| CredentialError::InvalidFormat { name: \"jwt_payload\".to_string() })?;\n\n        let payload: serde_json::Value = serde_json::from_slice(\u0026payload_bytes)\n            .map_err(|_| CredentialError::InvalidFormat { name: \"jwt_payload\".to_string() })?;\n\n        Ok(payload)\n    }\n}\n\n/// Keycloak Service Account Provider\n#[derive(Debug)]\npub struct KeycloakServiceAccountProvider {\n    client: Arc\u003cKeycloakClient\u003e,\n}\n\nimpl KeycloakServiceAccountProvider {\n    /// Create new Keycloak Service Account Provider\n    pub fn new(config: KeycloakConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = KeycloakClient::new(config)?;\n        Ok(Self {\n            client: Arc::new(client),\n        })\n    }\n\n    /// Convert Keycloak token to credential format\n    fn keycloak_token_to_credential(\u0026self, token: \u0026KeycloakToken) -\u003e HashMap\u003cString, String\u003e {\n        let mut values = HashMap::new();\n        values.insert(\"access_token\".to_string(), token.access_token.clone());\n        values.insert(\"token_type\".to_string(), token.token_type.clone());\n        values.insert(\"expires_in\".to_string(), token.expires_in.to_string());\n        \n        if let Some(refresh_token) = \u0026token.refresh_token {\n            values.insert(\"refresh_token\".to_string(), refresh_token.clone());\n        }\n        \n        if let Some(scope) = \u0026token.scope {\n            values.insert(\"scope\".to_string(), scope.clone());\n        }\n        \n        values.insert(\"issued_at\".to_string(), token.issued_at.to_rfc3339());\n        values\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for KeycloakServiceAccountProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Err(CredentialError::NotFound { name: name.to_string() });\n        }\n\n        let token = self.client.get_access_token().await?;\n        let values = self.keycloak_token_to_credential(\u0026token);\n\n        let credential = Credential {\n            name: name.to_string(),\n            values,\n            metadata: HashMap::from([\n                (\"client_id\".to_string(), self.client.config.client_id.clone()),\n                (\"realm\".to_string(), self.client.config.realm.clone()),\n                (\"scopes\".to_string(), self.client.config.scopes.join(\" \")),\n            ]),\n            created_at: token.issued_at,\n            updated_at: chrono::Utc::now(),\n            expires_at: Some(token.issued_at + chrono::Duration::seconds(token.expires_in)),\n            rotation_enabled: true,\n            access_policy: Some(AccessPolicy {\n                allowed_subjects: vec![\"system\".to_string()],\n                read_permissions: vec![\"read\".to_string()],\n                write_permissions: vec![],\n                rotation_permissions: vec![\"rotate\".to_string()],\n            }),\n        };\n\n        let version = version.unwrap_or(\"current\").to_string();\n        \n        Ok(VersionedCredential {\n            credential,\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn put_credential(\u0026self, _credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        Err(CredentialError::PermissionDenied {\n            name: \"Keycloak tokens are read-only\".to_string()\n        })\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Err(CredentialError::NotFound { name: name.to_string() });\n        }\n\n        let token = self.client.get_access_token().await?;\n        let values = self.keycloak_token_to_credential(\u0026token);\n\n        let credential = Credential {\n            name: name.to_string(),\n            values,\n            metadata: HashMap::from([\n                (\"client_id\".to_string(), self.client.config.client_id.clone()),\n                (\"realm\".to_string(), self.client.config.realm.clone()),\n                (\"scopes\".to_string(), self.client.config.scopes.join(\" \")),\n            ]),\n            created_at: token.issued_at,\n            updated_at: chrono::Utc::now(),\n            expires_at: Some(token.issued_at + chrono::Duration::seconds(token.expires_in)),\n            rotation_enabled: true,\n            access_policy: Some(AccessPolicy {\n                allowed_subjects: vec![\"system\".to_string()],\n                read_permissions: vec![\"read\".to_string()],\n                write_permissions: vec![],\n                rotation_permissions: vec![\"rotate\".to_string()],\n            }),\n        };\n\n        Ok(vec![VersionedCredential {\n            credential,\n            version: \"current\".to_string(),\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        }])\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        // Keycloak Service Account Provider only manages service account tokens\n        Ok(vec![])\n    }\n\n    async fn delete_credential(\u0026self, _name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        // Cannot delete service account tokens\n        Err(CredentialError::PermissionDenied {\n            name: \"Keycloak service account tokens cannot be deleted\".to_string()\n        })\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        Ok(name == \"service_account_token\")\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Err(CredentialError::NotFound { name: name.to_string() });\n        }\n\n        match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                // Keycloak tokens expire automatically, rotation is handled by the server\n                // We just need to refresh our cache\n                let old_token = self.client.get_access_token().await?;\n                \n                // Wait for rotation interval\n                tokio::time::sleep(time_based.rotation_interval).await;\n                \n                // Get new token (will trigger cache refresh)\n                let _ = self.client.get_access_token().await?;\n                \n                Ok(RotationResult {\n                    success: true,\n                    rotated_credential: Some(name.to_string()),\n                    old_credential_version: Some(old_token.access_token.clone()),\n                    new_credential_version: Some(\"rotated\".to_string()),\n                    rotated_at: chrono::Utc::now(),\n                    error_message: None,\n                })\n            }\n            RotationStrategy::EventBased(_) | RotationStrategy::Manual =\u003e {\n                // Force cache refresh\n                {\n                    let mut cache = self.client.token_cache.write().map_err(|_| CredentialError::InternalError)?;\n                    cache.clear();\n                }\n                \n                let new_token = self.client.get_access_token().await?;\n                \n                Ok(RotationResult {\n                    success: true,\n                    rotated_credential: Some(name.to_string()),\n                    old_credential_version: Some(\"refreshed\".to_string()),\n                    new_credential_version: Some(new_token.access_token.clone()),\n                    rotated_at: chrono::Utc::now(),\n                    error_message: None,\n                })\n            }\n        }\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        if name == \"service_account_token\" {\n            Ok(Some(AccessPolicy {\n                allowed_subjects: vec![\"system\".to_string()],\n                read_permissions: vec![\"read\".to_string()],\n                write_permissions: vec![],\n                rotation_permissions: vec![\"rotate\".to_string()],\n            }))\n        } else {\n            Ok(None)\n        }\n    }\n\n    async fn set_access_policy(\u0026self, _name: \u0026str, _policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        // Cannot set policies on Keycloak service account tokens\n        Err(CredentialError::PermissionDenied {\n            name: \"Cannot modify Keycloak service account token policies\".to_string()\n        })\n    }\n\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        if name != \"service_account_token\" {\n            return Ok(false);\n        }\n\n        // Allow access if subject is \"system\" for read/rotate permissions\n        Ok(subject == \"system\" \u0026\u0026 (permission == \"read\" || permission == \"rotate\"))\n    }\n\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // In production, you would log this to Keycloak events or external audit system\n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        Ok(CredentialStatistics {\n            total_credentials: 1,\n            active_credentials: 1,\n            credential_versions: 1,\n            rotation_statistics: RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                next_scheduled_rotation: None,\n            },\n            last_accessed: Some(chrono::Utc::now()),\n        })\n    }\n}\n\n/// Keycloak User Provider for user authentication\n#[derive(Debug)]\npub struct KeycloakUserProvider {\n    client: Arc\u003cKeycloakClient\u003e,\n}\n\nimpl KeycloakUserProvider {\n    /// Create new Keycloak User Provider\n    pub fn new(config: KeycloakConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = KeycloakClient::new(config)?;\n        Ok(Self {\n            client: Arc::new(client),\n        })\n    }\n\n    /// Authenticate user with username and password\n    pub async fn authenticate_user(\u0026self, username: \u0026str, password: \u0026str) -\u003e Result\u003cKeycloakToken, CredentialError\u003e {\n        let token_endpoint = self.client.get_token_endpoint().await?;\n        \n        let mut form_data = HashMap::from([\n            (\"grant_type\".to_string(), \"password\".to_string()),\n            (\"client_id\".to_string(), self.client.config.client_id.clone()),\n            (\"username\".to_string(), username.to_string()),\n            (\"password\".to_string(), password.to_string()),\n            (\"scope\".to_string(), self.client.config.scopes.join(\" \")),\n        ]);\n\n        if let Some(secret) = \u0026self.client.config.client_secret {\n            form_data.insert(\"client_secret\".to_string(), secret.clone());\n        }\n\n        let response = self.client\n            .client\n            .post(\u0026token_endpoint)\n            .form(\u0026form_data)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to authenticate user: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Authentication {\n                message: format!(\"User authentication failed: HTTP {}\", response.status())\n            });\n        }\n\n        let token_response: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse user token response: {}\", e)\n            })?;\n\n        let token_data = KeycloakToken {\n            access_token: token_response.get(\"access_token\")\n                .and_then(|t| t.as_str())\n                .ok_or_else(|| CredentialError::InvalidFormat {\n                    name: \"access_token\".to_string()\n                })?.to_string(),\n            refresh_token: token_response.get(\"refresh_token\").and_then(|t| t.as_str()).map(|s| s.to_string()),\n            token_type: token_response.get(\"token_type\")\n                .and_then(|t| t.as_str())\n                .unwrap_or(\"Bearer\").to_string(),\n            expires_in: token_response.get(\"expires_in\")\n                .and_then(|e| e.as_i64())\n                .unwrap_or(3600),\n            scope: token_response.get(\"scope\").and_then(|s| s.as_str()).map(|s| s.to_string()),\n            issued_at: chrono::Utc::now(),\n        };\n\n        Ok(token_data)\n    }\n\n    /// Get user by ID\n    pub async fn get_user_by_id(\u0026self, user_id: \u0026str, access_token: \u0026str) -\u003e Result\u003cKeycloakUser, CredentialError\u003e {\n        let admin_api_url = format!(\"{}/admin/realms/{}/users/{}\", \n                                  self.client.config.base_url, self.client.config.realm, user_id);\n\n        let response = self.client\n            .client\n            .get(\u0026admin_api_url)\n            .bearer_auth(access_token)\n            .send()\n            .await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to get user by ID: {}\", e)\n            })?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::NotFound {\n                name: format!(\"User {}\", user_id)\n            });\n        }\n\n        let user_data: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse user data: {}\", e)\n            })?;\n\n        let user = KeycloakUser {\n            id: user_data.get(\"id\")\n                .and_then(|i| i.as_str())\n                .unwrap_or(\"\").to_string(),\n            username: user_data.get(\"username\")\n                .and_then(|u| u.as_str())\n                .unwrap_or(\"\").to_string(),\n            email: user_data.get(\"email\").and_then(|e| e.as_str()).map(|s| s.to_string()),\n            first_name: user_data.get(\"firstName\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            last_name: user_data.get(\"lastName\").and_then(|n| n.as_str()).map(|s| s.to_string()),\n            enabled: user_data.get(\"enabled\")\n                .and_then(|e| e.as_bool())\n                .unwrap_or(true),\n            email_verified: user_data.get(\"emailVerified\")\n                .and_then(|v| v.as_bool())\n                .unwrap_or(false),\n        };\n\n        Ok(user)\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","mod.rs"],"content":"//! Credential Management Module\n//! \n//! This module provides a complete credential management system including\n//! multiple credential providers and automatic rotation capabilities.\n\nuse crate::traits::*;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\n/// Credential Information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Credential {\n    pub name: String,\n    pub values: HashMap\u003cString, String\u003e,\n    pub metadata: HashMap\u003cString, String\u003e,\n    pub created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub updated_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub expires_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    pub rotation_enabled: bool,\n    pub access_policy: Option\u003cAccessPolicy\u003e,\n}\n\n/// Access Policy for credential\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AccessPolicy {\n    pub allowed_subjects: Vec\u003cString\u003e,\n    pub read_permissions: Vec\u003cString\u003e,\n    pub write_permissions: Vec\u003cString\u003e,\n    pub rotation_permissions: Vec\u003cString\u003e,\n}\n\n/// Secret Rotation Strategy\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RotationStrategy {\n    TimeBased(TimeBasedRotation),\n    EventBased(EventBasedRotation),\n    Manual,\n}\n\n/// Time-based rotation configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TimeBasedRotation {\n    pub rotation_interval: Duration,\n    pub rotation_time: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    pub grace_period: Duration,\n}\n\n/// Event-based rotation configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct EventBasedRotation {\n    pub triggers: Vec\u003cRotationTrigger\u003e,\n    pub max_concurrent_rotations: usize,\n}\n\n/// Rotation trigger types\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RotationTrigger {\n    CredentialAccessed,\n    UnauthorizedAccessAttempt,\n    KeyCompromised,\n    CertificateExpiry(Duration),\n    ComplianceRequirement,\n}\n\n/// Rotation result information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RotationResult {\n    pub success: bool,\n    pub rotated_credential: Option\u003cString\u003e,\n    pub old_credential_version: Option\u003cString\u003e,\n    pub new_credential_version: Option\u003cString\u003e,\n    pub rotated_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub error_message: Option\u003cString\u003e,\n}\n\n/// Credential Audit Event\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AuditEvent {\n    pub event_type: CredentialEventType,\n    pub credential_name: String,\n    pub subject: String,\n    pub timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub success: bool,\n    pub details: HashMap\u003cString, String\u003e,\n}\n\n/// Types of credential events\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum CredentialEventType {\n    CredentialCreated,\n    CredentialRead,\n    CredentialUpdated,\n    CredentialDeleted,\n    RotationStarted,\n    RotationCompleted,\n    RotationFailed,\n    AccessDenied,\n}\n\n/// Versioned Credential\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct VersionedCredential {\n    pub credential: Credential,\n    pub version: String,\n    pub version_created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    pub is_active: bool,\n}\n\n/// Credential Provider Trait\n#[async_trait::async_trait]\npub trait CredentialProvider: Send + Sync {\n    /// Get credential by name\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e;\n    \n    /// Put/update credential\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e;\n    \n    /// List all versions of a credential\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e;\n    \n    /// List all credentials\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e;\n    \n    /// Delete credential (soft delete)\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e;\n    \n    /// Check if credential exists\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e;\n    \n    /// Rotate credential using specified strategy\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e;\n    \n    /// Get access policy for credential\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e;\n    \n    /// Set access policy for credential\n    async fn set_access_policy(\u0026self, name: \u0026str, policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e;\n    \n    /// Check if subject has permission for credential\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e;\n    \n    /// Audit credential operation\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e;\n    \n    /// Get credential statistics\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e;\n}\n\n/// Credential statistics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CredentialStatistics {\n    pub total_credentials: u64,\n    pub active_credentials: u64,\n    pub credential_versions: u64,\n    pub rotation_statistics: RotationStatistics,\n    pub last_accessed: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\n/// Rotation statistics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RotationStatistics {\n    pub total_rotations: u64,\n    pub successful_rotations: u64,\n    pub failed_rotations: u64,\n    pub last_rotation: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    pub next_scheduled_rotation: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\n/// Simple in-memory credential provider\n#[derive(Debug)]\npub struct SimpleCredentialProvider {\n    credentials: std::sync::RwLock\u003cHashMap\u003cString, Vec\u003cVersionedCredential\u003e\u003e\u003e,\n    audit_log: std::sync::RwLock\u003cVec\u003cAuditEvent\u003e\u003e,\n    rotation_config: std::sync::RwLock\u003cHashMap\u003cString, RotationStrategy\u003e\u003e,\n}\n\nimpl SimpleCredentialProvider {\n    /// Create new simple credential provider\n    pub fn new() -\u003e Self {\n        Self {\n            credentials: std::sync::RwLock::new(HashMap::new()),\n            audit_log: std::sync::RwLock::new(Vec::new()),\n            rotation_config: std::sync::RwLock::new(HashMap::new()),\n        }\n    }\n\n    /// Generate a new version identifier\n    fn generate_version() -\u003e String {\n        format!(\"v{}_{}\", \n               chrono::Utc::now().timestamp_millis(),\n               uuid::Uuid::new_v4().hyphenated().to_string()[0..8].to_string())\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for SimpleCredentialProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        let target_version = match version {\n            Some(v) =\u003e {\n                versions.iter().find(|vc| vc.version == v)\n                    .cloned()\n                    .ok_or_else(|| CredentialError::VersionNotFound { name: name.to_string(), version: v.to_string() })?\n            }\n            None =\u003e {\n                versions.iter().find(|vc| vc.is_active)\n                    .cloned()\n                    .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?\n            }\n        };\n\n        // Audit the access\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::CredentialRead,\n            credential_name: name.to_string(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::new(),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        Ok(target_version)\n    }\n\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        let version = Self::generate_version();\n        let versioned_credential = VersionedCredential {\n            credential: credential.clone(),\n            version: version.clone(),\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        };\n\n        // Deactivate previous versions\n        if let Some(versions) = credentials.get_mut(credential.name.as_str()) {\n            for v in versions.iter_mut() {\n                v.is_active = false;\n            }\n        } else {\n            credentials.insert(credential.name.clone(), Vec::new());\n        }\n\n        // Add new version\n        let versions = credentials.get_mut(credential.name.as_str()).unwrap();\n        versions.push(versioned_credential.clone());\n\n        // Audit the creation\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::CredentialCreated,\n            credential_name: credential.name.clone(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::from([\n                (\"version\".to_string(), version),\n            ]),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        Ok(versioned_credential)\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        Ok(versions.clone())\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let mut active_credentials = Vec::new();\n        for versions in credentials.values() {\n            if let Some(active) = versions.iter().find(|vc| vc.is_active) {\n                active_credentials.push(active.clone());\n            }\n        }\n        \n        Ok(active_credentials)\n    }\n\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        if let Some(versions) = credentials.get_mut(name) {\n            // Soft delete by deactivating all versions\n            for version in versions.iter_mut() {\n                version.is_active = false;\n            }\n            \n            // Audit the deletion\n            let audit_event = AuditEvent {\n                event_type: CredentialEventType::CredentialDeleted,\n                credential_name: name.to_string(),\n                subject: \"system\".to_string(),\n                timestamp: chrono::Utc::now(),\n                success: true,\n                details: HashMap::new(),\n            };\n            let _ = self.audit_operation(\u0026audit_event).await;\n            \n            Ok(())\n        } else {\n            Err(CredentialError::NotFound { name: name.to_string() })\n        }\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        Ok(credentials.contains_key(name))\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get_mut(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        // Get current active credential\n        let current_active = versions.iter()\n            .find(|vc| vc.is_active)\n            .cloned()\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n\n        // Audit rotation start\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::RotationStarted,\n            credential_name: name.to_string(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::new(),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        // Deactivate current version\n        for version in versions.iter_mut() {\n            if version.is_active {\n                version.is_active = false;\n                break;\n            }\n        }\n\n        // Create new version with updated expiry\n        let mut new_credential = current_active.credential.clone();\n        let new_expires_at = match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                Some(chrono::Utc::now() + time_based.rotation_interval)\n            }\n            RotationStrategy::EventBased(_) =\u003e {\n                // For event-based, set a reasonable default expiry\n                Some(chrono::Utc::now() + chrono::Duration::days(30))\n            }\n            RotationStrategy::Manual =\u003e {\n                new_credential.expires_at\n            }\n        };\n\n        new_credential.expires_at = new_expires_at;\n        new_credential.updated_at = chrono::Utc::now();\n\n        let new_version = Self::generate_version();\n        let new_versioned_credential = VersionedCredential {\n            credential: new_credential,\n            version: new_version.clone(),\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        };\n\n        versions.push(new_versioned_credential);\n\n        // Audit rotation completion\n        let audit_event = AuditEvent {\n            event_type: CredentialEventType::RotationCompleted,\n            credential_name: name.to_string(),\n            subject: \"system\".to_string(),\n            timestamp: chrono::Utc::now(),\n            success: true,\n            details: HashMap::from([\n                (\"old_version\".to_string(), current_active.version),\n                (\"new_version\".to_string(), new_version),\n            ]),\n        };\n        let _ = self.audit_operation(\u0026audit_event).await;\n\n        Ok(RotationResult {\n            success: true,\n            rotated_credential: Some(name.to_string()),\n            old_credential_version: Some(current_active.version),\n            new_credential_version: Some(new_version),\n            rotated_at: chrono::Utc::now(),\n            error_message: None,\n        })\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        let active = versions.iter()\n            .find(|vc| vc.is_active)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        Ok(active.credential.access_policy.clone())\n    }\n\n    async fn set_access_policy(\u0026self, name: \u0026str, policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut credentials = self.credentials.write().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get_mut(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        // Update active version's access policy\n        for version in versions.iter_mut() {\n            if version.is_active {\n                version.credential.access_policy = Some(policy.clone());\n                version.credential.updated_at = chrono::Utc::now();\n                break;\n            }\n        }\n\n        Ok(())\n    }\n\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let versions = credentials.get(name)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        let active = versions.iter()\n            .find(|vc| vc.is_active)\n            .ok_or_else(|| CredentialError::NotFound { name: name.to_string() })?;\n        \n        if let Some(policy) = \u0026active.credential.access_policy {\n            Ok(policy.allowed_subjects.contains(subject) \u0026\u0026\n               match permission {\n                   \"read\" =\u003e policy.read_permissions.is_empty() || policy.read_permissions.contains(permission),\n                   \"write\" =\u003e policy.write_permissions.is_empty() || policy.write_permissions.contains(permission),\n                   \"rotate\" =\u003e policy.rotation_permissions.is_empty() || policy.rotation_permissions.contains(permission),\n                   _ =\u003e false,\n               })\n        } else {\n            // Default: allow access to all subjects\n            Ok(true)\n        }\n    }\n\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut audit_log = self.audit_log.write().map_err(|_| CredentialError::InternalError)?;\n        audit_log.push(event.clone());\n        \n        // Limit audit log size to prevent memory issues\n        if audit_log.len() \u003e 10000 {\n            audit_log.drain(0..5000);\n        }\n        \n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        let credentials = self.credentials.read().map_err(|_| CredentialError::InternalError)?;\n        \n        let mut total_credentials = 0u64;\n        let mut active_credentials = 0u64;\n        let mut total_versions = 0u64;\n        let mut last_accessed = None::\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e;\n        \n        for versions in credentials.values() {\n            total_credentials += 1;\n            total_versions += versions.len() as u64;\n            \n            for version in versions {\n                if version.is_active {\n                    active_credentials += 1;\n                }\n                if last_accessed.is_none() || version.version_created_at \u003e last_accessed.unwrap() {\n                    last_accessed = Some(version.version_created_at);\n                }\n            }\n        }\n\n        let audit_log = self.audit_log.read().map_err(|_| CredentialError::InternalError)?;\n        let rotations = audit_log.iter()\n            .filter(|event| matches!(event.event_type, CredentialEventType::RotationCompleted))\n            .collect::\u003cVec\u003c_\u003e\u003e();\n\n        Ok(CredentialStatistics {\n            total_credentials,\n            active_credentials,\n            credential_versions: total_versions,\n            rotation_statistics: RotationStatistics {\n                total_rotations: rotations.len() as u64,\n                successful_rotations: rotations.iter().filter(|e| e.success).count() as u64,\n                failed_rotations: rotations.iter().filter(|e| !e.success).count() as u64,\n                last_rotation: rotations.last().map(|e| e.timestamp),\n                next_scheduled_rotation: None, // Would be calculated based on strategies\n            },\n            last_accessed,\n        })\n    }\n}\n\nimpl Default for SimpleCredentialProvider {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Extended error types for credentials\n#[derive(Debug, thiserror::Error)]\npub enum CredentialError {\n    #[error(\"Credential not found: {name}\")]\n    NotFound { name: String },\n    \n    #[error(\"Credential version not found: {name} version {version}\")]\n    VersionNotFound { name: String, version: String },\n    \n    #[error(\"Credential already exists: {name}\")]\n    AlreadyExists { name: String },\n    \n    #[error(\"Invalid credential format: {name}\")]\n    InvalidFormat { name: String },\n    \n    #[error(\"Permission denied for credential: {name}\")]\n    PermissionDenied { name: String },\n    \n    #[error(\"Credential limit exceeded: {name}\")]\n    LimitExceeded { name: String },\n    \n    #[error(\"Internal error\")]\n    InternalError,\n    \n    #[error(\"Network error: {message}\")]\n    Network { message: String },\n    \n    #[error(\"Storage error: {message}\")]\n    Storage { message: String },\n    \n    #[error(\"Encryption error: {message}\")]\n    Encryption { message: String },\n    \n    #[error(\"Rotation failed: {name}\")]\n    RotationFailed { name: String },\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","rotation.rs"],"content":"//! Automatic Credential Rotation System\n//! \n//! This module provides a complete automatic rotation system for credentials\n//! with support for time-based, event-based, and manual rotation strategies.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::{Mutex, RwLock};\nuse tokio::time::{interval, Duration};\n\n/// Rotation task state\n#[derive(Debug, Clone)]\nenum RotationTaskState {\n    Pending,\n    InProgress,\n    Completed,\n    Failed,\n    Cancelled,\n}\n\n/// Rotation task information\n#[derive(Debug, Clone)]\nstruct RotationTask {\n    id: uuid::Uuid,\n    credential_name: String,\n    strategy: RotationStrategy,\n    state: RotationTaskState,\n    created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    started_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    completed_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    result: Option\u003cRotationResult\u003e,\n    retry_count: u32,\n    max_retries: u32,\n}\n\n/// Rotation event trigger\n#[derive(Debug, Clone)]\npub struct RotationEvent {\n    pub event_type: RotationEventType,\n    pub credential_name: Option\u003cString\u003e,\n    pub metadata: HashMap\u003cString, String\u003e,\n    pub timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Types of rotation events\n#[derive(Debug, Clone)]\npub enum RotationEventType {\n    CredentialAccessed,\n    UnauthorizedAccess,\n    SecurityBreach,\n    ComplianceCheck,\n    ManualTrigger,\n    ScheduledRotation,\n    KeyRotation,\n    CertificateExpiryWarning,\n}\n\n/// Rotation statistics\n#[derive(Debug, Clone)]\nstruct RotationStatistics {\n    total_rotations: u64,\n    successful_rotations: u64,\n    failed_rotations: u64,\n    last_rotation: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    average_rotation_time: Duration,\n    active_tasks: u32,\n}\n\n/// Rotation engine configuration\n#[derive(Debug, Clone)]\npub struct RotationEngineConfig {\n    pub max_concurrent_rotations: usize,\n    pub default_retry_count: u32,\n    pub rotation_timeout: Duration,\n    pub audit_retention_days: u32,\n    pub enable_metrics: bool,\n    pub health_check_interval: Duration,\n}\n\nimpl Default for RotationEngineConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_concurrent_rotations: 5,\n            default_retry_count: 3,\n            rotation_timeout: Duration::from_secs(300), // 5 minutes\n            audit_retention_days: 90,\n            enable_metrics: true,\n            health_check_interval: Duration::from_secs(60),\n        }\n    }\n}\n\n/// Main rotation engine\n#[derive(Debug)]\npub struct RotationEngine {\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    tasks: Arc\u003cMutex\u003cHashMap\u003cuuid::Uuid, RotationTask\u003e\u003e\u003e,\n    config: RotationEngineConfig,\n    statistics: Arc\u003cRwLock\u003cRotationStatistics\u003e\u003e,\n    audit_log: Arc\u003cMutex\u003cVec\u003cRotationAuditEvent\u003e\u003e\u003e,\n    rotation_strategies: Arc\u003cRwLock\u003cHashMap\u003cString, RotationStrategy\u003e\u003e\u003e,\n    event_handlers: Arc\u003cRwLock\u003cVec\u003cBox\u003cdyn RotationEventHandler + Send + Sync\u003e\u003e\u003e\u003e,\n    health_status: Arc\u003cRwLock\u003cRotationHealthStatus\u003e\u003e,\n    shutdown_tx: Arc\u003cMutex\u003cOption\u003ctokio::sync::oneshot::Sender\u003c()\u003e\u003e\u003e\u003e,\n}\n\n/// Rotation health status\n#[derive(Debug, Clone)]\nstruct RotationHealthStatus {\n    is_healthy: bool,\n    last_health_check: chrono::DateTime\u003cchrono::Utc\u003e,\n    issues: Vec\u003cString\u003e,\n    running_tasks: u32,\n}\n\n/// Rotation audit event\n#[derive(Debug, Clone)]\nstruct RotationAuditEvent {\n    task_id: uuid::Uuid,\n    credential_name: String,\n    event_type: RotationAuditEventType,\n    details: HashMap\u003cString, String\u003e,\n    timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n#[derive(Debug, Clone)]\nenum RotationAuditEventType {\n    TaskCreated,\n    TaskStarted,\n    TaskCompleted,\n    TaskFailed,\n    TaskRetried,\n    TaskCancelled,\n    StrategyUpdated,\n    EventTriggered,\n}\n\nimpl RotationEngine {\n    /// Create new rotation engine\n    pub fn new(\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n        config: RotationEngineConfig,\n    ) -\u003e Self {\n        Self {\n            credential_provider,\n            tasks: Arc::new(Mutex::new(HashMap::new())),\n            config,\n            statistics: Arc::new(RwLock::new(RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                average_rotation_time: Duration::from_secs(0),\n                active_tasks: 0,\n            })),\n            audit_log: Arc::new(Mutex::new(Vec::new())),\n            rotation_strategies: Arc::new(RwLock::new(HashMap::new())),\n            event_handlers: Arc::new(RwLock::new(Vec::new())),\n            health_status: Arc::new(RwLock::new(RotationHealthStatus {\n                is_healthy: true,\n                last_health_check: chrono::Utc::now(),\n                issues: Vec::new(),\n                running_tasks: 0,\n            })),\n            shutdown_tx: Arc::new(Mutex::new(None)),\n        }\n    }\n\n    /// Start the rotation engine\n    pub async fn start(\u0026self) -\u003e Result\u003c(), CredentialError\u003e {\n        let (shutdown_tx, shutdown_rx) = tokio::sync::oneshot::channel();\n        \n        {\n            let mut tx = self.shutdown_tx.lock().await;\n            *tx = Some(shutdown_tx);\n        }\n\n        // Start background tasks\n        let rotation_engine = self.clone();\n        let rotation_task = tokio::spawn(async move {\n            rotation_engine.rotation_loop(shutdown_rx).await;\n        });\n\n        let health_engine = self.clone();\n        let health_task = tokio::spawn(async move {\n            health_engine.health_check_loop().await;\n        });\n\n        // Wait for completion\n        tokio::select! {\n            result = rotation_task =\u003e {\n                if let Err(e) = result {\n                    eprintln!(\"Rotation task failed: {:?}\", e);\n                }\n            }\n            result = health_task =\u003e {\n                if let Err(e) = result {\n                    eprintln!(\"Health check task failed: {:?}\", e);\n                }\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Stop the rotation engine gracefully\n    pub async fn stop(\u0026self) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut shutdown_tx = self.shutdown_tx.lock().await;\n        if let Some(tx) = shutdown_tx.take() {\n            let _ = tx.send(());\n        }\n        Ok(())\n    }\n\n    /// Clone with arc references\n    fn clone(\u0026self) -\u003e Self {\n        Self {\n            credential_provider: self.credential_provider.clone(),\n            tasks: self.tasks.clone(),\n            config: self.config.clone(),\n            statistics: self.statistics.clone(),\n            audit_log: self.audit_log.clone(),\n            rotation_strategies: self.rotation_strategies.clone(),\n            event_handlers: self.event_handlers.clone(),\n            health_status: self.health_status.clone(),\n            shutdown_tx: self.shutdown_tx.clone(),\n        }\n    }\n\n    /// Main rotation loop\n    async fn rotation_loop(\u0026self, shutdown_rx: tokio::sync::oneshot::Receiver\u003c()\u003e) {\n        let mut time_based_timer = interval(Duration::from_secs(60)); // Check every minute\n        let mut shutdown_signal = shutdown_rx;\n\n        loop {\n            tokio::select! {\n                _ = time_based_timer.tick() =\u003e {\n                    self.process_time_based_rotations().await;\n                    self.process_pending_tasks().await;\n                }\n                _ = \u0026mut shutdown_signal =\u003e {\n                    break;\n                }\n            }\n        }\n\n        // Graceful shutdown: wait for active tasks to complete\n        let timeout = tokio::time::sleep(Duration::from_secs(60));\n        tokio::select! {\n            _ = timeout =\u003e {\n                // Force shutdown\n            }\n            _ = self.wait_for_active_tasks() =\u003e {\n                // All tasks completed\n            }\n        }\n    }\n\n    /// Health check loop\n    async fn health_check_loop(\u0026self) {\n        let mut timer = interval(self.config.health_check_interval);\n        \n        loop {\n            timer.tick().await;\n            \n            // Check engine health\n            let active_tasks = self.get_active_task_count().await;\n            let mut issues = Vec::new();\n            \n            if active_tasks \u003e self.config.max_concurrent_rotations {\n                issues.push(format!(\"Too many active tasks: {}\", active_tasks));\n            }\n            \n            // Update health status\n            {\n                let mut status = self.health_status.write().await;\n                status.is_healthy = issues.is_empty();\n                status.last_health_check = chrono::Utc::now();\n                status.issues = issues.clone();\n                status.running_tasks = active_tasks;\n            }\n        }\n    }\n\n    /// Process time-based rotations\n    async fn process_time_based_rotations(\u0026self) {\n        let strategies = self.rotation_strategies.read().await;\n        let now = chrono::Utc::now();\n\n        for (credential_name, strategy) in strategies.iter() {\n            if let RotationStrategy::TimeBased(time_based) = strategy {\n                // Check if rotation is due\n                if let Ok(current_credential) = self.credential_provider.get_credential(credential_name, None).await {\n                    let next_rotation = current_credential.credential.updated_at + time_based.rotation_interval;\n                    \n                    if now \u003e= next_rotation {\n                        let _ = self.schedule_rotation(credential_name, strategy.clone()).await;\n                    }\n                }\n            }\n        }\n    }\n\n    /// Process pending rotation tasks\n    async fn process_pending_tasks(\u0026self) {\n        let mut tasks = self.tasks.lock().await;\n        \n        // Collect pending tasks\n        let pending_tasks: Vec\u003cuuid::Uuid\u003e = tasks.values()\n            .filter(|task| matches!(task.state, RotationTaskState::Pending))\n            .map(|task| task.id)\n            .collect();\n\n        // Process each pending task\n        for task_id in pending_tasks {\n            let active_count = tasks.values().filter(|t| matches!(t.state, RotationTaskState::InProgress)).count();\n            \n            if active_count \u003c self.config.max_concurrent_rotations {\n                if let Some(task) = tasks.get_mut(\u0026task_id) {\n                    task.state = RotationTaskState::InProgress;\n                    task.started_at = Some(chrono::Utc::now());\n                    \n                    // Start task execution in background\n                    let engine_clone = self.clone();\n                    let task_clone = task.clone();\n                    tokio::spawn(async move {\n                        engine_clone.execute_rotation_task(task_clone).await;\n                    });\n                }\n            }\n        }\n    }\n\n    /// Execute rotation task\n    async fn execute_rotation_task(\u0026self, task: RotationTask) {\n        let start_time = chrono::Utc::now();\n        \n        // Update task state\n        {\n            let mut tasks = self.tasks.lock().await;\n            if let Some(existing_task) = tasks.get_mut(\u0026task.id) {\n                existing_task.state = RotationTaskState::InProgress;\n                existing_task.started_at = Some(start_time);\n            }\n        }\n\n        // Audit task start\n        self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskStarted).await;\n\n        let result = self.perform_rotation(\u0026task.credential_name, \u0026task.strategy).await;\n\n        // Update task state and statistics\n        {\n            let mut tasks = self.tasks.lock().await;\n            let mut task_update = tasks.get_mut(\u0026task.id).unwrap();\n            \n            match result {\n                Ok(rotation_result) =\u003e {\n                    task_update.state = RotationTaskState::Completed;\n                    task_update.completed_at = Some(chrono::Utc::now());\n                    task_update.result = Some(rotation_result);\n                    \n                    // Update statistics\n                    self.update_statistics(true, start_time).await;\n                    \n                    // Audit completion\n                    self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskCompleted).await;\n                }\n                Err(error) =\u003e {\n                    task_update.retry_count += 1;\n                    \n                    if task_update.retry_count \u003c task_update.max_retries {\n                        task_update.state = RotationTaskState::Pending;\n                        self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskRetried).await;\n                    } else {\n                        task_update.state = RotationTaskState::Failed;\n                        task_update.completed_at = Some(chrono::Utc::now());\n                        self.update_statistics(false, start_time).await;\n                        self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskFailed).await;\n                    }\n                }\n            }\n        }\n    }\n\n    /// Perform actual rotation\n    async fn perform_rotation(\u0026self, credential_name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        match self.credential_provider.rotate_credential(credential_name, strategy).await {\n            Ok(result) =\u003e {\n                // Trigger event handlers\n                let event = RotationEvent {\n                    event_type: RotationEventType::ScheduledRotation,\n                    credential_name: Some(credential_name.to_string()),\n                    metadata: HashMap::new(),\n                    timestamp: chrono::Utc::now(),\n                };\n                \n                self.trigger_event_handlers(\u0026event).await;\n                \n                Ok(result)\n            }\n            Err(error) =\u003e {\n                // Trigger failure event handlers\n                let event = RotationEvent {\n                    event_type: RotationEventType::SecurityBreach,\n                    credential_name: Some(credential_name.to_string()),\n                    metadata: HashMap::from([\n                        (\"error\".to_string(), error.to_string()),\n                        (\"failure_type\".to_string(), \"rotation_failed\".to_string()),\n                    ]),\n                    timestamp: chrono::Utc::now(),\n                };\n                \n                self.trigger_event_handlers(\u0026event).await;\n                \n                Err(error)\n            }\n        }\n    }\n\n    /// Schedule rotation task\n    pub async fn schedule_rotation(\u0026self, credential_name: \u0026str, strategy: RotationStrategy) -\u003e Result\u003cuuid::Uuid, CredentialError\u003e {\n        let task = RotationTask {\n            id: uuid::Uuid::new_v4(),\n            credential_name: credential_name.to_string(),\n            strategy,\n            state: RotationTaskState::Pending,\n            created_at: chrono::Utc::now(),\n            started_at: None,\n            completed_at: None,\n            result: None,\n            retry_count: 0,\n            max_retries: self.config.default_retry_count,\n        };\n\n        {\n            let mut tasks = self.tasks.lock().await;\n            tasks.insert(task.id, task.clone());\n        }\n\n        // Audit task creation\n        self.audit_event(task.id.clone(), credential_name, RotationAuditEventType::TaskCreated).await;\n\n        Ok(task.id)\n    }\n\n    /// Trigger event-based rotation\n    pub async fn trigger_event_rotation(\u0026self, event: RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // Audit event\n        if let Some(ref credential_name) = event.credential_name {\n            self.audit_event(uuid::Uuid::new_v4(), credential_name, RotationAuditEventType::EventTriggered).await;\n        }\n\n        // Trigger event handlers\n        self.trigger_event_handlers(\u0026event).await;\n\n        // Schedule rotation if it's an event that requires it\n        if matches!(event.event_type, RotationEventType::UnauthorizedAccess | \n                   RotationEventType::SecurityBreach | \n                   RotationEventType::ManualTrigger) {\n            if let Some(credential_name) = \u0026event.credential_name {\n                // Use event-based strategy\n                let strategy = RotationStrategy::EventBased(EventBasedRotation {\n                    triggers: vec![event.event_type],\n                    max_concurrent_rotations: 1,\n                });\n                \n                let _ = self.schedule_rotation(credential_name, strategy).await;\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Update rotation strategy for a credential\n    pub async fn update_rotation_strategy(\u0026self, credential_name: \u0026str, strategy: RotationStrategy) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut strategies = self.rotation_strategies.write().await;\n        strategies.insert(credential_name.to_string(), strategy.clone());\n        \n        // Audit strategy update\n        self.audit_event(uuid::Uuid::new_v4(), credential_name, RotationAuditEventType::StrategyUpdated).await;\n\n        Ok(())\n    }\n\n    /// Add event handler\n    pub async fn add_event_handler(\u0026self, handler: Box\u003cdyn RotationEventHandler + Send + Sync\u003e) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut handlers = self.event_handlers.write().await;\n        handlers.push(handler);\n        Ok(())\n    }\n\n    /// Get rotation status\n    pub async fn get_status(\u0026self) -\u003e RotationEngineStatus {\n        let tasks = self.tasks.lock().await;\n        let statistics = self.statistics.read().await;\n        let health_status = self.health_status.read().await;\n\n        RotationEngineStatus {\n            total_tasks: tasks.len() as u32,\n            pending_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::Pending)).count() as u32,\n            active_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::InProgress)).count() as u32,\n            completed_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::Completed)).count() as u32,\n            failed_tasks: tasks.values().filter(|t| matches!(t.state, RotationTaskState::Failed)).count() as u32,\n            statistics: statistics.clone(),\n            health_status: health_status.clone(),\n            configured_credentials: self.rotation_strategies.read().await.len() as u32,\n        }\n    }\n\n    /// Get task details\n    pub async fn get_task(\u0026self, task_id: \u0026uuid::Uuid) -\u003e Option\u003cRotationTask\u003e {\n        let tasks = self.tasks.lock().await;\n        tasks.get(task_id).cloned()\n    }\n\n    /// List all tasks\n    pub async fn list_tasks(\u0026self, filter: Option\u003cRotationTaskState\u003e) -\u003e Vec\u003cRotationTask\u003e {\n        let tasks = self.tasks.lock().await;\n        let mut task_list: Vec\u003cRotationTask\u003e = tasks.values().cloned().collect();\n        \n        if let Some(state) = filter {\n            task_list.retain(|task| matches!(task.state, state));\n        }\n        \n        task_list\n    }\n\n    /// Cancel rotation task\n    pub async fn cancel_task(\u0026self, task_id: \u0026uuid::Uuid) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut tasks = self.tasks.lock().await;\n        if let Some(task) = tasks.get_mut(task_id) {\n            match task.state {\n                RotationTaskState::Pending | RotationTaskState::InProgress =\u003e {\n                    task.state = RotationTaskState::Cancelled;\n                    self.audit_event(task.id.clone(), \u0026task.credential_name, RotationAuditEventType::TaskCancelled).await;\n                    Ok(())\n                }\n                RotationTaskState::Completed | RotationTaskState::Failed | RotationTaskState::Cancelled =\u003e {\n                    Err(CredentialError::PermissionDenied {\n                        name: \"Cannot cancel completed or failed task\".to_string()\n                    })\n                }\n            }\n        } else {\n            Err(CredentialError::NotFound {\n                name: format!(\"Task {}\", task_id)\n            })\n        }\n    }\n\n    // Helper methods\n    async fn get_active_task_count(\u0026self) -\u003e u32 {\n        let tasks = self.tasks.lock().await;\n        tasks.values().filter(|t| matches!(t.state, RotationTaskState::InProgress)).count() as u32\n    }\n\n    async fn wait_for_active_tasks(\u0026self) {\n        loop {\n            let active_count = self.get_active_task_count().await;\n            if active_count == 0 {\n                break;\n            }\n            tokio::time::sleep(Duration::from_secs(1)).await;\n        }\n    }\n\n    async fn update_statistics(\u0026self, success: bool, start_time: chrono::DateTime\u003cchrono::Utc\u003e) {\n        let mut stats = self.statistics.write().await;\n        let duration = chrono::Utc::now() - start_time;\n        let duration_secs = duration.num_seconds() as u64;\n\n        stats.total_rotations += 1;\n        if success {\n            stats.successful_rotations += 1;\n        } else {\n            stats.failed_rotations += 1;\n        }\n        stats.last_rotation = Some(chrono::Utc::now());\n\n        // Update average time (simple moving average)\n        let current_avg = stats.average_rotation_time.as_secs();\n        let new_avg = (current_avg * (stats.total_rotations - 1) + duration_secs) / stats.total_rotations;\n        stats.average_rotation_time = Duration::from_secs(new_avg);\n    }\n\n    async fn audit_event(\u0026self, task_id: uuid::Uuid, credential_name: \u0026str, event_type: RotationAuditEventType) {\n        let audit_event = RotationAuditEvent {\n            task_id,\n            credential_name: credential_name.to_string(),\n            event_type,\n            details: HashMap::new(),\n            timestamp: chrono::Utc::now(),\n        };\n\n        let mut log = self.audit_log.lock().await;\n        log.push(audit_event);\n\n        // Limit audit log size\n        if log.len() \u003e 10000 {\n            log.drain(0..5000);\n        }\n    }\n\n    async fn trigger_event_handlers(\u0026self, event: \u0026RotationEvent) {\n        let handlers = self.event_handlers.read().await;\n        for handler in handlers.iter() {\n            let _ = handler.handle_event(event).await;\n        }\n    }\n}\n\n/// Rotation event handler trait\n#[async_trait]\npub trait RotationEventHandler: Send + Sync {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e;\n}\n\n/// Rotation engine status\n#[derive(Debug, Clone)]\npub struct RotationEngineStatus {\n    pub total_tasks: u32,\n    pub pending_tasks: u32,\n    pub active_tasks: u32,\n    pub completed_tasks: u32,\n    pub failed_tasks: u32,\n    pub statistics: RotationStatistics,\n    pub health_status: RotationHealthStatus,\n    pub configured_credentials: u32,\n}\n\n/// Built-in rotation event handlers\n\n/// Keycloak rotation event handler\npub struct KeycloakRotationHandler {\n    keycloak_client: Arc\u003csuper::keycloak::KeycloakClient\u003e,\n}\n\nimpl KeycloakRotationHandler {\n    pub fn new(keycloak_client: Arc\u003csuper::keycloak::KeycloakClient\u003e) -\u003e Self {\n        Self { keycloak_client }\n    }\n}\n\n#[async_trait::async_trait]\nimpl RotationEventHandler for KeycloakRotationHandler {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        match event.event_type {\n            RotationEventType::UnauthorizedAccess =\u003e {\n                // Trigger immediate token refresh\n                {\n                    let mut cache = self.keycloak_client.token_cache.write().map_err(|_| CredentialError::InternalError)?;\n                    cache.clear();\n                }\n                Ok(())\n            }\n            _ =\u003e Ok(()),\n        }\n    }\n}\n\n/// AWS Secrets Manager rotation event handler\npub struct AWSRotationHandler {\n    // Would include AWS-specific configuration\n    #[allow(dead_code)]\n    region: String,\n}\n\nimpl AWSRotationHandler {\n    pub fn new(region: String) -\u003e Self {\n        Self { region }\n    }\n}\n\n#[async_trait::async_trait]\nimpl RotationEventHandler for AWSRotationHandler {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        match event.event_type {\n            RotationEventType::ComplianceCheck =\u003e {\n                // Log compliance check event\n                println!(\"AWS compliance check triggered for credential: {:?}\", event.credential_name);\n                Ok(())\n            }\n            _ =\u003e Ok(()),\n        }\n    }\n}\n\n/// Vault rotation event handler\npub struct VaultRotationHandler {\n    // Would include Vault-specific configuration\n    #[allow(dead_code)]\n    vault_url: String,\n}\n\nimpl VaultRotationHandler {\n    pub fn new(vault_url: String) -\u003e Self {\n        Self { vault_url }\n    }\n}\n\n#[async_trait::async_trait]\nimpl RotationEventHandler for VaultRotationHandler {\n    async fn handle_event(\u0026self, event: \u0026RotationEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        match event.event_type {\n            RotationEventType::KeyRotation =\u003e {\n                // Trigger key rotation in Vault transit engine\n                println!(\"Vault key rotation triggered for credential: {:?}\", event.credential_name);\n                Ok(())\n            }\n            _ =\u003e Ok(()),\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","credentials","vault.rs"],"content":"//! HashiCorp Vault Provider Implementation\n//! \n//! This module provides a complete integration with HashiCorp Vault for\n//! secure credential management and storage.\n\nuse super::*;\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::time::Duration;\n\n/// Vault-specific configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct VaultConfig {\n    pub vault_url: String,\n    pub auth_method: VaultAuthMethod,\n    pub timeout: Duration,\n    pub max_retries: u32,\n    pub secrets_engine: String,\n    pub transit_engine: Option\u003cString\u003e,\n    pub kv_version: KVVersion,\n}\n\n/// Vault authentication methods\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum VaultAuthMethod {\n    Token { token: String },\n    AppRole { role_id: String, secret_id: String },\n    Kubernetes { jwt_path: String, role: String },\n    Azure { client_id: String, client_secret: String, tenant_id: String },\n}\n\n/// KV engine version\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum KVVersion {\n    V1,\n    V2,\n}\n\n/// Vault API client wrapper\n#[derive(Debug)]\npub struct VaultClient {\n    client: reqwest::Client,\n    config: VaultConfig,\n    token: Option\u003cString\u003e,\n    token_expiry: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n\nimpl VaultClient {\n    /// Create new Vault client\n    pub async fn new(config: VaultConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = reqwest::Client::builder()\n            .timeout(config.timeout)\n            .danger_accept_invalid_certs(false)\n            .build()\n            .map_err(|e| CredentialError::Network { \n                message: format!(\"Failed to create HTTP client: {}\", e) \n            })?;\n\n        let mut vault_client = Self {\n            client,\n            config: config.clone(),\n            token: None,\n            token_expiry: None,\n        };\n\n        // Authenticate using configured method\n        vault_client.authenticate().await?;\n\n        Ok(vault_client)\n    }\n\n    /// Authenticate with Vault\n    async fn authenticate(\u0026mut self) -\u003e Result\u003c(), CredentialError\u003e {\n        match \u0026self.config.auth_method {\n            VaultAuthMethod::Token { token } =\u003e {\n                self.token = Some(token.clone());\n                self.token_expiry = Some(chrono::Utc::now() + chrono::Duration::hours(24));\n            }\n            VaultAuthMethod::AppRole { role_id, secret_id } =\u003e {\n                let auth_data = HashMap::from([\n                    (\"role_id\", role_id.clone()),\n                    (\"secret_id\", secret_id.clone()),\n                ]);\n                \n                let response = self.client\n                    .post(\u0026format!(\"{}/v1/auth/approle/login\", self.config.vault_url))\n                    .json(\u0026auth_data)\n                    .send()\n                    .await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Vault authentication failed: {}\", e)\n                    })?;\n\n                if !response.status().is_success() {\n                    return Err(CredentialError::Authentication {\n                        message: \"Failed to authenticate with Vault\".to_string(),\n                    });\n                }\n\n                let auth_response: serde_json::Value = response.json().await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Failed to parse Vault auth response: {}\", e)\n                    })?;\n\n                if let Some(auth) = auth_response.get(\"auth\") {\n                    if let Some(token) = auth.get(\"client_token\").and_then(|t| t.as_str()) {\n                        if let Some(lease_duration) = auth.get(\"lease_duration\").and_then(|d| d.as_u64()) {\n                            self.token_expiry = Some(chrono::Utc::now() + chrono::Duration::seconds(lease_duration as i64));\n                        }\n                        self.token = Some(token.to_string());\n                    }\n                }\n            }\n            VaultAuthMethod::Kubernetes { jwt_path, role } =\u003e {\n                let jwt = tokio::fs::read_to_string(jwt_path).await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Failed to read JWT file: {}\", e)\n                    })?;\n\n                let auth_data = HashMap::from([\n                    (\"role\", role.clone()),\n                    (\"jwt\", jwt.trim().to_string()),\n                ]);\n\n                let response = self.client\n                    .post(\u0026format!(\"{}/v1/auth/kubernetes/login\", self.config.vault_url))\n                    .json(\u0026auth_data)\n                    .send()\n                    .await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Vault Kubernetes authentication failed: {}\", e)\n                    })?;\n\n                if !response.status().is_success() {\n                    return Err(CredentialError::Authentication {\n                        message: \"Failed to authenticate with Vault via Kubernetes\".to_string(),\n                    });\n                }\n\n                let auth_response: serde_json::Value = response.json().await\n                    .map_err(|e| CredentialError::Network {\n                        message: format!(\"Failed to parse Vault auth response: {}\", e)\n                    })?;\n\n                if let Some(auth) = auth_response.get(\"auth\") {\n                    if let Some(token) = auth.get(\"client_token\").and_then(|t| t.as_str()) {\n                        if let Some(lease_duration) = auth.get(\"lease_duration\").and_then(|d| d.as_u64()) {\n                            self.token_expiry = Some(chrono::Utc::now() + chrono::Duration::seconds(lease_duration as i64));\n                        }\n                        self.token = Some(token.to_string());\n                    }\n                }\n            }\n            _ =\u003e {\n                return Err(CredentialError::Configuration {\n                    message: \"Unsupported authentication method\".to_string(),\n                });\n            }\n        }\n\n        if self.token.is_none() {\n            return Err(CredentialError::Authentication {\n                message: \"Failed to obtain Vault token\".to_string(),\n            });\n        }\n\n        Ok(())\n    }\n\n    /// Ensure we have a valid token\n    async fn ensure_authenticated(\u0026mut self) -\u003e Result\u003c(), CredentialError\u003e {\n        if let Some(expiry) = self.token_expiry {\n            if chrono::Utc::now() \u003e expiry - chrono::Duration::minutes(5) {\n                self.authenticate().await?;\n            }\n        }\n        Ok(())\n    }\n\n    /// Make authenticated request to Vault\n    async fn request(\u0026mut self, method: reqwest::Method, path: \u0026str, body: Option\u003cserde_json::Value\u003e) -\u003e Result\u003creqwest::Response, CredentialError\u003e {\n        self.ensure_authenticated().await?;\n\n        let url = if path.starts_with(\"/v1/\") {\n            format!(\"{}{}\", self.config.vault_url, path)\n        } else {\n            format!(\"{}/v1/{}\", self.config.vault_url, path)\n        };\n\n        let mut request = self.client.request(method, \u0026url);\n        \n        if let Some(token) = \u0026self.token {\n            request = request.bearer_auth(token);\n        }\n\n        if let Some(body) = body {\n            request = request.json(\u0026body);\n        }\n\n        request.send().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Vault API request failed: {}\", e)\n            })\n    }\n\n    /// Get secret from Vault\n    pub async fn get_secret(\u0026mut self, path: \u0026str) -\u003e Result\u003cserde_json::Value, CredentialError\u003e {\n        let response = self.request(reqwest::Method::GET, path, None).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to get secret from Vault: HTTP {}\", response.status())\n            });\n        }\n\n        response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Vault response: {}\", e)\n            })\n    }\n\n    /// Put secret to Vault\n    pub async fn put_secret(\u0026mut self, path: \u0026str, secret_data: \u0026HashMap\u003cString, String\u003e) -\u003e Result\u003c(), CredentialError\u003e {\n        let secret_payload = match self.config.kv_version {\n            KVVersion::V1 =\u003e {\n                serde_json::to_value(secret_data).map_err(|e| CredentialError::Storage {\n                    message: format!(\"Failed to serialize secret data: {}\", e)\n                })?\n            }\n            KVVersion::V2 =\u003e {\n                serde_json::json!({ \"data\": secret_data })\n            }\n        };\n\n        let response = self.request(reqwest::Method::PUT, path, Some(secret_payload)).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to put secret to Vault: HTTP {}\", response.status())\n            });\n        }\n\n        Ok(())\n    }\n\n    /// List secrets at path\n    pub async fn list_secrets(\u0026mut self, path: \u0026str) -\u003e Result\u003cVec\u003cString\u003e, CredentialError\u003e {\n        let list_path = if self.config.kv_version == KVVersion::V2 {\n            format!(\"{}/metadata\", path)\n        } else {\n            path.to_string()\n        };\n\n        let response = self.request(reqwest::Method::GET, \u0026format!(\"{}?list=1\", list_path), None).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to list secrets from Vault: HTTP {}\", response.status())\n            });\n        }\n\n        let response_data: serde_json::Value = response.json().await\n            .map_err(|e| CredentialError::Network {\n                message: format!(\"Failed to parse Vault list response: {}\", e)\n            })?;\n\n        let mut keys = Vec::new();\n        if let Some(data) = response_data.get(\"data\") {\n            if let Some(key_list) = data.get(\"keys\").and_then(|k| k.as_array()) {\n                for key in key_list {\n                    if let Some(key_str) = key.as_str() {\n                        keys.push(key_str.to_string());\n                    }\n                }\n            }\n        }\n\n        Ok(keys)\n    }\n\n    /// Delete secret from Vault\n    pub async fn delete_secret(\u0026mut self, path: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let delete_path = match self.config.kv_version {\n            KVVersion::V1 =\u003e path.to_string(),\n            KVVersion::V2 =\u003e format!(\"{}/metadata\", path),\n        };\n\n        let response = self.request(reqwest::Method::DELETE, \u0026delete_path, None).await?;\n        \n        if !response.status().is_success() {\n            return Err(CredentialError::Network {\n                message: format!(\"Failed to delete secret from Vault: HTTP {}\", response.status())\n            });\n        }\n\n        Ok(())\n    }\n}\n\n/// HashiCorp Vault Credential Provider\n#[derive(Debug)]\npub struct HashiCorpVaultProvider {\n    client: Arc\u003cstd::sync::Mutex\u003cVaultClient\u003e\u003e,\n    config: VaultConfig,\n}\n\nimpl HashiCorpVaultProvider {\n    /// Create new HashiCorp Vault Provider\n    pub async fn new(config: VaultConfig) -\u003e Result\u003cSelf, CredentialError\u003e {\n        let client = VaultClient::new(config.clone()).await?;\n        Ok(Self {\n            client: Arc::new(std::sync::Mutex::new(client)),\n            config,\n        })\n    }\n\n    /// Convert secret data from Vault format to Credential format\n    fn vault_data_to_credential(\u0026self, vault_data: \u0026serde_json::Value) -\u003e Result\u003cHashMap\u003cString, String\u003e, CredentialError\u003e {\n        let mut values = HashMap::new();\n\n        if let Some(data) = vault_data.get(\"data\") {\n            if data.is_object() {\n                for (key, value) in data.as_object().unwrap() {\n                    if let Some(value_str) = value.as_str() {\n                        values.insert(key.clone(), value_str.to_string());\n                    } else if value.is_number() {\n                        values.insert(key.clone(), value.to_string());\n                    } else if value.is_boolean() {\n                        values.insert(key.clone(), value.to_string());\n                    } else {\n                        values.insert(key.clone(), serde_json::to_string(value).unwrap_or_else(|_| \"null\".to_string()));\n                    }\n                }\n            }\n        } else {\n            // For V1, the root object is the data\n            if vault_data.is_object() {\n                for (key, value) in vault_data.as_object().unwrap() {\n                    if let Some(value_str) = value.as_str() {\n                        values.insert(key.clone(), value_str.to_string());\n                    } else if value.is_number() {\n                        values.insert(key.clone(), value.to_string());\n                    } else if value.is_boolean() {\n                        values.insert(key.clone(), value.to_string());\n                    } else {\n                        values.insert(key.clone(), serde_json::to_string(value).unwrap_or_else(|_| \"null\".to_string()));\n                    }\n                }\n            }\n        }\n\n        Ok(values)\n    }\n\n    /// Convert Credential to Vault format\n    fn credential_to_vault_data(\u0026self, credential: \u0026Credential) -\u003e HashMap\u003cString, String\u003e {\n        let mut vault_data = HashMap::new();\n        \n        // Copy all values\n        for (key, value) in \u0026credential.values {\n            vault_data.insert(key.clone(), value.clone());\n        }\n        \n        // Add metadata\n        if let Some(policy) = \u0026credential.access_policy {\n            vault_data.insert(\"rotation_enabled\".to_string(), credential.rotation_enabled.to_string());\n            vault_data.insert(\"expires_at\".to_string(), \n                credential.expires_at.map(|e| e.to_rfc3339()).unwrap_or_else(|| \"null\".to_string()));\n            \n            // Store access policy as JSON\n            let policy_json = serde_json::to_string(policy)\n                .unwrap_or_else(|_| \"{}\".to_string());\n            vault_data.insert(\"access_policy\".to_string(), policy_json);\n        }\n        \n        vault_data\n    }\n\n    /// Get secret path in Vault\n    fn get_secret_path(\u0026self, name: \u0026str) -\u003e String {\n        format!(\"{}/{}\", self.config.secrets_engine, name)\n    }\n}\n\n#[async_trait::async_trait]\nimpl CredentialProvider for HashiCorpVaultProvider {\n    async fn get_credential(\u0026self, name: \u0026str, version: Option\u003c\u0026str\u003e) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let path = self.get_secret_path(name);\n        let vault_data = client.get_secret(\u0026path).await?;\n        \n        let values = self.vault_data_to_credential(\u0026vault_data)?;\n        \n        // Extract metadata\n        let mut credential_values = HashMap::new();\n        let mut metadata = HashMap::new();\n        let mut rotation_enabled = false;\n        let mut expires_at = None;\n        let mut access_policy = None;\n\n        for (key, value) in values {\n            match key.as_str() {\n                \"rotation_enabled\" =\u003e rotation_enabled = value.parse().unwrap_or(false),\n                \"expires_at\" =\u003e {\n                    if value != \"null\" {\n                        expires_at = chrono::DateTime::parse_from_rfc3339(\u0026value)\n                            .ok()\n                            .map(|dt| dt.with_timezone(\u0026chrono::Utc));\n                    }\n                }\n                \"access_policy\" =\u003e {\n                    if value != \"{}\" {\n                        access_policy = serde_json::from_str(\u0026value).ok();\n                    }\n                }\n                _ =\u003e credential_values.insert(key.clone(), value),\n            }\n        }\n\n        let credential = Credential {\n            name: name.to_string(),\n            values: credential_values,\n            metadata,\n            created_at: chrono::Utc::now(),\n            updated_at: chrono::Utc::now(),\n            expires_at,\n            rotation_enabled,\n            access_policy,\n        };\n\n        let version = version.unwrap_or(\"current\").to_string();\n        \n        Ok(VersionedCredential {\n            credential,\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn put_credential(\u0026self, credential: \u0026Credential) -\u003e Result\u003cVersionedCredential, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let path = self.get_secret_path(\u0026credential.name);\n        let vault_data = self.credential_to_vault_data(credential);\n        \n        client.put_secret(\u0026path, \u0026vault_data).await?;\n        \n        let version = format!(\"v{}_{}\", chrono::Utc::now().timestamp_millis(), chrono::Utc::now().timestamp_nanos());\n        \n        Ok(VersionedCredential {\n            credential: credential.clone(),\n            version,\n            version_created_at: chrono::Utc::now(),\n            is_active: true,\n        })\n    }\n\n    async fn list_credential_versions(\u0026self, name: \u0026str) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        // Note: Vault V1 doesn't support version listing\n        // This would need to be implemented differently for V1 vs V2\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        // For simplicity, return current version only\n        let current = self.get_credential(name, None).await?;\n        Ok(vec![current])\n    }\n\n    async fn list_credentials(\u0026self) -\u003e Result\u003cVec\u003cVersionedCredential\u003e, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let secrets = client.list_secrets(\u0026self.config.secrets_engine).await?;\n        \n        let mut credentials = Vec::new();\n        for secret_name in secrets {\n            if let Ok(version) = self.get_credential(\u0026secret_name, None).await {\n                credentials.push(version);\n            }\n        }\n        \n        Ok(credentials)\n    }\n\n    async fn delete_credential(\u0026self, name: \u0026str) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let path = self.get_secret_path(name);\n        client.delete_secret(\u0026path).await?;\n        \n        Ok(())\n    }\n\n    async fn credential_exists(\u0026self, name: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        // Simple existence check by attempting to read\n        match self.get_credential(name, None).await {\n            Ok(_) =\u003e Ok(true),\n            Err(CredentialError::NotFound { .. }) =\u003e Ok(false),\n            Err(e) =\u003e Err(e),\n        }\n    }\n\n    async fn rotate_credential(\u0026self, name: \u0026str, strategy: \u0026RotationStrategy) -\u003e Result\u003cRotationResult, CredentialError\u003e {\n        // Get current credential\n        let current = self.get_credential(name, None).await?;\n        \n        // Create rotated version\n        let mut rotated_credential = current.credential.clone();\n        rotated_credential.updated_at = chrono::Utc::now();\n        \n        match strategy {\n            RotationStrategy::TimeBased(time_based) =\u003e {\n                rotated_credential.expires_at = Some(chrono::Utc::now() + time_based.rotation_interval);\n            }\n            RotationStrategy::EventBased(_) =\u003e {\n                // For event-based, rotate immediately\n                rotated_credential.expires_at = Some(chrono::Utc::now() + chrono::Duration::days(30));\n            }\n            RotationStrategy::Manual =\u003e {\n                // Keep current expiry\n            }\n        }\n\n        // Store rotated credential\n        let rotated_version = self.put_credential(\u0026rotated_credential).await?;\n        \n        Ok(RotationResult {\n            success: true,\n            rotated_credential: Some(name.to_string()),\n            old_credential_version: Some(current.version),\n            new_credential_version: Some(rotated_version.version),\n            rotated_at: chrono::Utc::now(),\n            error_message: None,\n        })\n    }\n\n    async fn get_access_policy(\u0026self, name: \u0026str) -\u003e Result\u003cOption\u003cAccessPolicy\u003e, CredentialError\u003e {\n        let version = self.get_credential(name, None).await?;\n        Ok(version.credential.access_policy)\n    }\n\n    async fn set_access_policy(\u0026self, name: \u0026str, policy: \u0026AccessPolicy) -\u003e Result\u003c(), CredentialError\u003e {\n        let mut version = self.get_credential(name, None).await?;\n        version.credential.access_policy = Some(policy.clone());\n        version.credential.updated_at = chrono::Utc::now();\n        \n        let _ = self.put_credential(\u0026version.credential).await?;\n        \n        Ok(())\n    }\n\n    async fn has_permission(\u0026self, name: \u0026str, subject: \u0026str, permission: \u0026str) -\u003e Result\u003cbool, CredentialError\u003e {\n        let version = self.get_credential(name, None).await?;\n        \n        if let Some(policy) = \u0026version.credential.access_policy {\n            Ok(policy.allowed_subjects.contains(subject))\n        } else {\n            // Default policy: allow all access\n            Ok(true)\n        }\n    }\n\n    async fn audit_operation(\u0026self, event: \u0026AuditEvent) -\u003e Result\u003c(), CredentialError\u003e {\n        // In a production environment, you would log this to a secure audit system\n        // For now, just return success\n        Ok(())\n    }\n\n    async fn get_statistics(\u0026self) -\u003e Result\u003cCredentialStatistics, CredentialError\u003e {\n        let credentials = self.list_credentials().await?;\n        \n        Ok(CredentialStatistics {\n            total_credentials: credentials.len() as u64,\n            active_credentials: credentials.len() as u64,\n            credential_versions: credentials.len() as u64,\n            rotation_statistics: RotationStatistics {\n                total_rotations: 0,\n                successful_rotations: 0,\n                failed_rotations: 0,\n                last_rotation: None,\n                next_scheduled_rotation: None,\n            },\n            last_accessed: None,\n        })\n    }\n}\n\n/// Transit encryption provider for Vault\npub struct VaultTransitProvider {\n    client: Arc\u003cstd::sync::Mutex\u003cVaultClient\u003e\u003e,\n    key_name: String,\n}\n\nimpl VaultTransitProvider {\n    /// Create new Transit provider\n    pub fn new(client: Arc\u003cstd::sync::Mutex\u003cVaultClient\u003e\u003e, key_name: String) -\u003e Self {\n        Self { client, key_name }\n    }\n\n    /// Encrypt data using Vault Transit\n    pub async fn encrypt(\u0026self, plaintext: \u0026[u8]) -\u003e Result\u003cVec\u003cu8\u003e, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let payload = HashMap::from([\n            (\"plaintext\", base64::Engine::encode(\u0026base64::engine::general_purpose::STANDARD, plaintext))\n        ]);\n\n        let response = client.request(\n            reqwest::Method::POST,\n            \u0026format!(\"v1/transit/encrypt/{}\", self.key_name),\n            Some(serde_json::to_value(\u0026payload).unwrap())\n        ).await?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Encryption {\n                message: \"Failed to encrypt data\".to_string()\n            });\n        }\n\n        let response_data: serde_json::Value = response.json().await\n            .map_err(|_| CredentialError::Encryption {\n                message: \"Failed to parse encryption response\".to_string()\n            })?;\n\n        if let Some(ciphertext) = response_data.get(\"data\").and_then(|d| d.get(\"ciphertext\")) {\n            if let Some(ciphertext_str) = ciphertext.as_str() {\n                return Ok(base64::Engine::decode(\u0026base64::engine::general_purpose::STANDARD, ciphertext_str).unwrap());\n            }\n        }\n\n        Err(CredentialError::Encryption {\n            message: \"Invalid encryption response format\".to_string()\n        })\n    }\n\n    /// Decrypt data using Vault Transit\n    pub async fn decrypt(\u0026self, ciphertext: \u0026[u8]) -\u003e Result\u003cVec\u003cu8\u003e, CredentialError\u003e {\n        let mut client = self.client.lock().map_err(|_| CredentialError::InternalError)?;\n        \n        let ciphertext_b64 = base64::Engine::encode(\u0026base64::engine::general_purpose::STANDARD, ciphertext);\n        let payload = HashMap::from([\n            (\"ciphertext\", ciphertext_b64)\n        ]);\n\n        let response = client.request(\n            reqwest::Method::POST,\n            \u0026format!(\"v1/transit/decrypt/{}\", self.key_name),\n            Some(serde_json::to_value(\u0026payload).unwrap())\n        ).await?;\n\n        if !response.status().is_success() {\n            return Err(CredentialError::Decryption {\n                message: \"Failed to decrypt data\".to_string()\n            });\n        }\n\n        let response_data: serde_json::Value = response.json().await\n            .map_err(|_| CredentialError::Decryption {\n                message: \"Failed to parse decryption response\".to_string()\n            })?;\n\n        if let Some(plaintext) = response_data.get(\"data\").and_then(|d| d.get(\"plaintext\")) {\n            if let Some(plaintext_str) = plaintext.as_str() {\n                return Ok(base64::Engine::decode(\u0026base64::engine::general_purpose::STANDARD, plaintext_str).unwrap());\n            }\n        }\n\n        Err(CredentialError::Decryption {\n            message: \"Invalid decryption response format\".to_string()\n        })\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","examples","advanced_usage.rs"],"content":"//! Advanced Usage Example\n//! \n//! This example demonstrates advanced Worker Manager features including:\n//! - Multiple providers (Kubernetes and Docker)\n//! - Complex credential management with rotation\n//! - Integration with NATS for event streaming\n//! - Keycloak service account integration\n//! - Health checks and monitoring\n\nuse worker_manager::*;\nuse std::collections::HashMap;\nuse tokio::time::{Duration, sleep};\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Initialize tracing\n    tracing_subscriber::fmt::init();\n\n    println!(\"Worker Manager Advanced Usage Example\");\n    println!(\"=====================================\");\n\n    // 1. Set up comprehensive credential providers\n    println!(\"\\n--- Setting Up Credential Providers ---\");\n    \n    // Simple in-memory provider for quick access\n    let simple_provider = Arc::new(credentials::SimpleCredentialProvider::new());\n    println!(\" Created SimpleCredentialProvider\");\n\n    // Set up sample credentials\n    setup_sample_credentials(\u0026simple_provider).await?;\n    println!(\" Configured sample credentials\");\n\n    // 2. Create Worker Manager with multiple providers\n    println!(\"\\n--- Creating Multi-Provider Worker Manager ---\");\n    \n    let mut worker_manager = WorkerManager::new(simple_provider.clone());\n\n    // Kubernetes Provider (if available)\n    match setup_kubernetes_provider(\u0026simple_provider).await {\n        Ok(provider) =\u003e {\n            worker_manager.register_provider(\"kubernetes\".to_string(), provider);\n            println!(\" Configured Kubernetes provider\");\n        }\n        Err(e) =\u003e {\n            println!(\" Kubernetes provider not available: {}\", e);\n        }\n    }\n\n    // Docker Provider\n    let docker_provider = Arc::new(providers::docker::DockerProvider::from_provider_config(\n        \u0026ProviderConfig::docker(),\n        simple_provider.clone()\n    )?);\n    worker_manager.register_provider(\"docker\".to_string(), docker_provider.clone());\n    worker_manager.set_default_provider(\"docker\".to_string());\n    println!(\" Configured Docker provider\");\n\n    // 3. Start rotation engine with advanced configuration\n    println!(\"\\n--- Starting Advanced Rotation Engine ---\");\n    \n    let mut rotation_config = credentials::rotation::RotationEngineConfig::default();\n    rotation_config.max_concurrent_rotations = 10;\n    rotation_config.rotation_timeout = Duration::from_secs(120);\n    rotation_config.enable_metrics = true;\n\n    worker_manager.start_rotation_engine(rotation_config).await?;\n    println!(\" Started advanced rotation engine\");\n\n    // 4. Set up rotation strategies for different credentials\n    println!(\"\\n--- Configuring Rotation Strategies ---\");\n    \n    // Time-based rotation for database credentials\n    let db_rotation_strategy = credentials::rotation::RotationStrategy::TimeBased(\n        credentials::rotation::TimeBasedRotation {\n            rotation_interval: Duration::from_secs(600), // 10 minutes\n            rotation_time: None,\n            grace_period: Duration::from_secs(30),\n        }\n    );\n\n    // Event-based rotation for API keys (triggered by security events)\n    let api_rotation_strategy = credentials::rotation::RotationStrategy::EventBased(\n        credentials::rotation::EventBasedRotation {\n            triggers: vec![\n                credentials::rotation::RotationTrigger::CredentialAccessed,\n                credentials::rotation::RotationTrigger::UnauthorizedAccessAttempt,\n            ],\n            max_concurrent_rotations: 3,\n        }\n    );\n\n    // Schedule rotations\n    let db_rotation_task = worker_manager.schedule_rotation(\"database-credentials\", db_rotation_strategy).await?;\n    let api_rotation_task = worker_manager.schedule_rotation(\"api-keys\", api_rotation_strategy).await?;\n    \n    println!(\" Scheduled rotation tasks:\");\n    println!(\"  Database credentials: {}\", db_rotation_task);\n    println!(\"  API keys: {}\", api_rotation_task);\n\n    // 5. Create workers with different providers\n    println!(\"\\n--- Creating Workers with Different Providers ---\");\n    \n    let mut workers = Vec::new();\n\n    // Worker 1: Database worker with secrets\n    let db_worker = create_database_worker().await?;\n    let db_provider_config = ProviderConfig::docker();\n    let db_worker_result = worker_manager.create_worker(\u0026db_worker, \u0026db_provider_config).await?;\n    workers.push(db_worker_result);\n    println!(\" Created database worker: {}\", workers[0].id);\n\n    // Worker 2: Web worker with API keys\n    let api_worker = create_api_worker().await?;\n    let api_provider_config = ProviderConfig::docker();\n    let api_worker_result = worker_manager.create_worker(\u0026api_worker, \u0026api_provider_config).await?;\n    workers.push(api_worker_result);\n    println!(\" Created API worker: {}\", workers[1].id);\n\n    // Worker 3: Kubernetes worker (if available)\n    if worker_manager.get_provider_by_name(\"kubernetes\").is_some() {\n        let k8s_worker = create_kubernetes_worker().await?;\n        let k8s_provider_config = ProviderConfig::kubernetes(\"default\".to_string());\n        let k8s_worker_result = worker_manager.create_worker(\u0026k8s_worker, \u0026k8s_provider_config).await?;\n        workers.push(k8s_worker_result);\n        println!(\" Created Kubernetes worker: {}\", workers[2].id);\n    }\n\n    // 6. Monitor worker lifecycle\n    println!(\"\\n--- Monitoring Worker Lifecycle ---\");\n    \n    for worker in \u0026workers {\n        println!(\"  Monitoring worker: {}\", worker.id);\n        \n        // Monitor for 30 seconds\n        for i in 1..=15 {\n            sleep(Duration::from_secs(2)).await;\n            \n            let status = worker_manager.get_worker_status(\u0026worker.id).await?;\n            println!(\"    Status {}: {:?}\", i, status);\n\n            // Test operations during monitoring\n            if i == 5 {\n                match worker_manager.get_provider().unwrap().get_logs(\u0026worker.id).await {\n                    Ok(logs) =\u003e {\n                        println!(\"     Got logs: {}\", logs.stream_id);\n                    }\n                    Err(e) =\u003e {\n                        println!(\"     Failed to get logs: {}\", e);\n                    }\n                }\n            }\n\n            if i == 10 {\n                match worker_manager.get_provider().unwrap().execute_command(\n                    \u0026worker.id, \n                    vec![\"echo\".to_string(), \"health-check\".to_string()],\n                    Some(Duration::from_secs(5))\n                ).await {\n                    Ok(result) =\u003e {\n                        println!(\"     Health check passed\");\n                    }\n                    Err(e) =\u003e {\n                        println!(\"     Health check failed: {}\", e);\n                    }\n                }\n            }\n\n            if matches!(status, WorkerState::Terminated) {\n                println!(\"     Worker completed\");\n                break;\n            }\n        }\n    }\n\n    // 7. Demonstrate event-driven rotations\n    println!(\"\\n--- Triggering Event-Based Rotations ---\");\n    \n    // Simulate security event\n    let security_event = credentials::rotation::RotationEvent {\n        event_type: credentials::rotation::RotationEventType::UnauthorizedAccessAttempt,\n        credential_name: Some(\"api-keys\".to_string()),\n        metadata: HashMap::from([\n            (\"source_ip\".to_string(), \"192.168.1.100\".to_string()),\n            (\"attempt_count\".to_string(), \"5\".to_string()),\n            (\"severity\".to_string(), \"high\".to_string()),\n        ]),\n        timestamp: chrono::Utc::now(),\n    };\n\n    // Trigger event through rotation engine\n    let rotation_engine = worker_manager.rotation_engine.read().await;\n    if let Some(engine) = rotation_engine.as_ref() {\n        engine.trigger_event_rotation(security_event).await?;\n        println!(\" Triggered event-based rotation\");\n    }\n\n    // 8. Test capacity management\n    println!(\"\\n--- Capacity Management ---\");\n    \n    let capacity = worker_manager.get_capacity().await?;\n    println!(\"  System capacity:\");\n    println!(\"    Active workers: {}\", capacity.active_workers);\n    println!(\"    CPU usage: {}m/{}m\", \n             capacity.used_resources.cpu_m, \n             capacity.total_resources.cpu_m);\n    println!(\"    Memory usage: {}MB/{}MB\", \n             capacity.used_resources.memory_mb, \n             capacity.total_resources.memory_mb);\n\n    // 9. Health checks and system status\n    println!(\"\\n--- Health Check and System Status ---\");\n    \n    let health_status = worker_manager.health_check().await?;\n    println!(\"  System health: {}\", if health_status.is_healthy { \"Healthy\" } else { \"Unhealthy\" });\n    println!(\"  Provider status:\");\n    for (name, healthy, error) in \u0026health_status.provider_details {\n        println!(\"    {}: {}\", name, if *healthy { \" Healthy\" } else { \" Failed\" });\n        if let Some(err) = error {\n            println!(\"      Error: {}\", err);\n        }\n    }\n\n    // 10. Rotation engine status\n    println!(\"\\n--- Rotation Engine Status ---\");\n    \n    let rotation_status = worker_manager.get_rotation_status().await?;\n    println!(\"  Rotation tasks:\");\n    println!(\"    Total: {}\", rotation_status.total_tasks);\n    println!(\"    Pending: {}\", rotation_status.pending_tasks);\n    println!(\"    Active: {}\", rotation_status.active_tasks);\n    println!(\"    Completed: {}\", rotation_status.completed_tasks);\n    println!(\"    Failed: {}\", rotation_status.failed_tasks);\n    println!(\"  Rotation statistics:\");\n    println!(\"    Total rotations: {}\", rotation_status.statistics.total_rotations);\n    println!(\"    Successful: {}\", rotation_status.statistics.successful_rotations);\n    println!(\"    Failed: {}\", rotation_status.statistics.failed_rotations);\n\n    // 11. Test manual rotation\n    println!(\"\\n--- Manual Rotation Test ---\");\n    \n    let manual_strategy = credentials::rotation::RotationStrategy::Manual;\n    let manual_task = worker_manager.schedule_rotation(\"database-credentials\", manual_strategy).await?;\n    println!(\" Scheduled manual rotation: {}\", manual_task);\n\n    // 12. Cleanup and shutdown\n    println!(\"\\n--- Cleanup and Shutdown ---\");\n    \n    // Wait a bit for rotations to complete\n    sleep(Duration::from_secs(5)).await;\n\n    // Terminate all workers\n    for worker in \u0026workers {\n        match worker_manager.terminate_worker(\u0026worker.id).await {\n            Ok(_) =\u003e println!(\" Terminated worker: {}\", worker.id),\n            Err(e) =\u003e println!(\" Failed to terminate worker {}: {}\", worker.id, e),\n        }\n    }\n\n    // Stop rotation engine\n    worker_manager.stop_rotation_engine().await?;\n    println!(\" Stopped rotation engine\");\n\n    println!(\"\\n--- Advanced Example Completed ---\");\n    println!(\" Demonstrated multi-provider support\");\n    println!(\" Showed advanced rotation strategies\");\n    println!(\" Tested event-driven rotations\");\n    println!(\" Validated system health monitoring\");\n    println!(\" Performed manual operations and cleanup\");\n\n    Ok(())\n}\n\nasync fn setup_sample_credentials(\n    provider: \u0026Arc\u003ccredentials::SimpleCredentialProvider\u003e\n) -\u003e Result\u003c(), CredentialError\u003e {\n    // Database credentials\n    let db_credential = credentials::Credential {\n        name: \"database-credentials\".to_string(),\n        values: HashMap::from([\n            (\"DB_HOST\".to_string(), \"prod-db.example.com\".to_string()),\n            (\"DB_PORT\".to_string(), \"5432\".to_string()),\n            (\"DB_USER\".to_string(), \"app_user\".to_string()),\n            (\"DB_PASSWORD\".to_string(), \"secure_password_123\".to_string()),\n            (\"DB_NAME\".to_string(), \"production_db\".to_string()),\n        ]),\n        metadata: HashMap::from([\n            (\"environment\".to_string(), \"production\".to_string()),\n            (\"team\".to_string(), \"backend\".to_string()),\n            (\"compliance_level\".to_string(), \"high\".to_string()),\n        ]),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(90)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"web-service\".to_string(), \"api-service\".to_string()],\n            read_permissions: vec![\"read\".to_string()],\n            write_permissions: vec![],\n            rotation_permissions: vec![\"rotate\".to_string(), \"manual-rotate\".to_string()],\n        }),\n    };\n\n    provider.put_credential(\u0026db_credential).await?;\n\n    // API keys\n    let api_credential = credentials::Credential {\n        name: \"api-keys\".to_string(),\n        values: HashMap::from([\n            (\"GOOGLE_API_KEY\".to_string(), \"AIzaSyD-9tSrke72PouQMnMX-a7u8Lm5Oz9kSu4\".to_string()),\n            (\"STRIPE_PUBLISHABLE_KEY\".to_string(), \"pk_test_51H...xyz\".to_string()),\n            (\"STRIPE_SECRET_KEY\".to_string(), \"sk_test_51H...abc\".to_string()),\n            (\"TWITTER_BEARER_TOKEN\".to_string(), \"Bearer_token_here\".to_string()),\n        ]),\n        metadata: HashMap::from([\n            (\"environment\".to_string(), \"production\".to_string()),\n            (\"team\".to_string(), \"frontend\".to_string()),\n            (\"sensitivity\".to_string(), \"high\".to_string()),\n        ]),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(60)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"frontend-service\".to_string(), \"mobile-app\".to_string()],\n            read_permissions: vec![\"read\".to_string()],\n            write_permissions: vec![],\n            rotation_permissions: vec![\"rotate\".to_string()],\n        }),\n    };\n\n    provider.put_credential(\u0026api_credential).await?;\n\n    // Service account credentials\n    let service_credential = credentials::Credential {\n        name: \"service-accounts\".to_string(),\n        values: HashMap::from([\n            (\"KAFKA_BROKER_USER\".to_string(), \"kafka-service\".to_string()),\n            (\"KAFKA_BROKER_PASSWORD\".to_string(), \"kafka_password_456\".to_string()),\n            (\"REDIS_USER\".to_string(), \"redis-admin\".to_string()),\n            (\"REDIS_PASSWORD\".to_string(), \"redis_secret_789\".to_string()),\n        ]),\n        metadata: HashMap::from([\n            (\"environment\".to_string(), \"production\".to_string()),\n            (\"team\".to_string(), \"infrastructure\".to_string()),\n            (\"sensitivity\".to_string(), \"critical\".to_string()),\n        ]),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(120)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"worker-service\".to_string(), \"scheduler\".to_string()],\n            read_permissions: vec![\"read\".to_string(), \"write\".to_string()],\n            write_permissions: vec![\"write\".to_string()],\n            rotation_permissions: vec![\"rotate\".to_string()],\n        }),\n    };\n\n    provider.put_credential(\u0026service_credential).await?;\n\n    Ok(())\n}\n\nasync fn setup_kubernetes_provider(\n    credential_provider: \u0026Arc\u003ccredentials::SimpleCredentialProvider\u003e\n) -\u003e Result\u003cArc\u003cproviders::kubernetes::KubernetesProvider\u003e, ProviderError\u003e {\n    // This would require actual Kubernetes cluster access in production\n    // For demo purposes, we'll try to create it but expect it to fail\n    let k8s_config = providers::kubernetes::KubernetesConfig {\n        namespace: \"default\".to_string(),\n        service_account: None,\n        node_selector: None,\n        tolerations: None,\n        node_affinity: None,\n        pod_disruption_budget: None,\n        termination_grace_period_seconds: Some(30),\n        image_pull_secrets: None,\n        priority_class: None,\n        security_context: None,\n    };\n\n    providers::kubernetes::KubernetesProvider::new(k8s_config, credential_provider.clone()).await\n        .map(|p| Arc::new(p))\n}\n\nasync fn create_database_worker() -\u003e Result\u003cRuntimeSpec, ProviderError\u003e {\n    let mut spec = RuntimeSpec::basic(\"postgres:15\".to_string());\n    spec.command = Some(vec![\n        \"postgres\".to_string()\n    ]);\n    spec.secret_refs = vec![\"database-credentials\".to_string()];\n    spec.ports = vec![5432];\n    spec.resources = ResourceQuota::basic(1000, 1024);\n    spec.labels.insert(\"type\".to_string(), \"database\".to_string());\n    spec.labels.insert(\"environment\".to_string(), \"production\".to_string());\n    \n    // Add environment variables\n    spec.env.insert(\"POSTGRES_DB\".to_string(), \"production_db\".to_string());\n    spec.env.insert(\"PGDATA\".to_string(), \"/var/lib/postgresql/data/pgdata\".to_string());\n\n    Ok(spec)\n}\n\nasync fn create_api_worker() -\u003e Result\u003cRuntimeSpec, ProviderError\u003e {\n    let mut spec = RuntimeSpec::basic(\"nginx:alpine\".to_string());\n    spec.command = Some(vec![\n        \"nginx\".to_string(), \"-g\".to_string(), \"daemon off;\".to_string()\n    ]);\n    spec.secret_refs = vec![\"api-keys\".to_string()];\n    spec.ports = vec![80, 443];\n    spec.resources = ResourceQuota::basic(500, 512);\n    spec.labels.insert(\"type\".to_string(), \"api\".to_string());\n    spec.labels.insert(\"environment\".to_string(), \"production\".to_string());\n    \n    // Add API-specific configuration\n    spec.env.insert(\"API_TIMEOUT\".to_string(), \"30\".to_string());\n    spec.env.insert(\"RATE_LIMIT\".to_string(), \"100\".to_string());\n\n    Ok(spec)\n}\n\nasync fn create_kubernetes_worker() -\u003e Result\u003cRuntimeSpec, ProviderError\u003e {\n    let mut spec = RuntimeSpec::basic(\"busybox:1.36\".to_string());\n    spec.command = Some(vec![\n        \"sh\".to_string(), \"-c\".to_string(), \n        \"echo 'Kubernetes worker started' \u0026\u0026 sleep 60\".to_string()\n    ]);\n    spec.secret_refs = vec![\"service-accounts\".to_string()];\n    spec.resources = ResourceQuota::basic(250, 256);\n    spec.labels.insert(\"type\".to_string(), \"k8s-demo\".to_string());\n    spec.labels.insert(\"environment\".to_string(), \"production\".to_string());\n\n    Ok(spec)\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","examples","basic_usage.rs"],"content":"//! Basic Usage Example\n//! \n//! This example demonstrates how to use the Worker Manager with basic configurations.\n//! Shows the complete lifecycle: creation, monitoring, and termination of workers.\n\nuse worker_manager::*;\nuse std::collections::HashMap;\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Initialize logging\n    tracing_subscriber::fmt::init();\n\n    println!(\"Worker Manager Basic Usage Example\");\n    println!(\"==================================\");\n\n    // 1. Create a simple credential provider (in-memory for demo)\n    let credential_provider = Arc::new(credentials::SimpleCredentialProvider::new());\n    println!(\" Created simple credential provider\");\n\n    // 2. Create Worker Manager\n    let mut worker_manager = WorkerManager::new(credential_provider.clone());\n    println!(\" Created Worker Manager\");\n\n    // 3. Set up Docker Provider\n    let docker_provider = Arc::new(providers::docker::DockerProvider::from_provider_config(\n        \u0026ProviderConfig::docker(),\n        credential_provider.clone()\n    )?);\n    worker_manager.register_provider(\"docker\".to_string(), docker_provider.clone());\n    worker_manager.set_default_provider(\"docker\".to_string());\n    println!(\" Configured Docker provider\");\n\n    // 4. Create sample credential\n    let mut sample_credential = credentials::Credential {\n        name: \"database-credentials\".to_string(),\n        values: HashMap::from([\n            (\"DB_HOST\".to_string(), \"localhost\".to_string()),\n            (\"DB_PORT\".to_string(), \"5432\".to_string()),\n            (\"DB_USER\".to_string(), \"admin\".to_string()),\n            (\"DB_PASSWORD\".to_string(), \"secret123\".to_string()),\n        ]),\n        metadata: HashMap::new(),\n        created_at: chrono::Utc::now(),\n        updated_at: chrono::Utc::now(),\n        expires_at: Some(chrono::Utc::now() + chrono::Duration::days(30)),\n        rotation_enabled: true,\n        access_policy: Some(credentials::AccessPolicy {\n            allowed_subjects: vec![\"worker-service\".to_string()],\n            read_permissions: vec![\"read\".to_string()],\n            write_permissions: vec![],\n            rotation_permissions: vec![\"rotate\".to_string()],\n        }),\n    };\n\n    // Store the credential\n    let stored_credential = credential_provider.put_credential(\u0026sample_credential).await?;\n    println!(\" Created sample credential: {}\", stored_credential.credential.name);\n\n    // 5. Create a RuntimeSpec for a worker\n    let mut runtime_spec = RuntimeSpec::basic(\"alpine:3.18\".to_string());\n    runtime_spec.command = Some(vec![\"/bin/sh\".to_string(), \"-lc\".to_string(), \n                                  \"echo 'Hello from Worker Manager!' \u0026\u0026 sleep 10\".to_string()]);\n    runtime_spec.secret_refs = vec![\"database-credentials\".to_string()];\n    runtime_spec.ports = vec![8080];\n    runtime_spec.resources = ResourceQuota::basic(500, 256);\n    runtime_spec.labels.insert(\"environment\".to_string(), \"development\".to_string());\n    runtime_spec.labels.insert(\"service\".to_string(), \"worker-demo\".to_string());\n\n    println!(\" Created RuntimeSpec for worker\");\n\n    // 6. Create ProviderConfig\n    let provider_config = ProviderConfig::docker();\n    println!(\" Created ProviderConfig\");\n\n    // 7. Create worker\n    println!(\"\\n--- Creating Worker ---\");\n    let worker = worker_manager.create_worker(\u0026runtime_spec, \u0026provider_config).await?;\n    println!(\" Worker created successfully\");\n    println!(\"  Worker ID: {}\", worker.id);\n    println!(\"  State: {:?}\", worker.state);\n    println!(\"  Provider: {}\", worker.provider_config.provider_name);\n\n    // 8. Monitor worker status\n    println!(\"\\n--- Monitoring Worker Status ---\");\n    for i in 1..=5 {\n        tokio::time::sleep(std::time::Duration::from_secs(2)).await;\n        \n        let status = worker_manager.get_worker_status(\u0026worker.id).await?;\n        println!(\"  Status check {}: {:?}\", i, status);\n\n        if matches!(status, WorkerState::Terminated) {\n            println!(\"   Worker completed successfully\");\n            break;\n        }\n    }\n\n    // 9. Test capacity information\n    println!(\"\\n--- System Capacity ---\");\n    let capacity = worker_manager.get_capacity().await?;\n    println!(\"  Active workers: {}\", capacity.active_workers);\n    println!(\"  Total CPU: {}m\", capacity.total_resources.cpu_m);\n    println!(\"  Total Memory: {}MB\", capacity.total_resources.memory_mb);\n    println!(\"  Available CPU: {}m\", capacity.available_resources.cpu_m);\n    println!(\"  Available Memory: {}MB\", capacity.available_resources.memory_mb);\n\n    // 10. Demonstrate rotation setup\n    println!(\"\\n--- Setting Up Credential Rotation ---\");\n    \n    // Start rotation engine\n    let rotation_config = credentials::rotation::RotationEngineConfig::default();\n    worker_manager.start_rotation_engine(rotation_config).await?;\n    println!(\" Started rotation engine\");\n\n    // Schedule time-based rotation\n    let rotation_strategy = credentials::rotation::RotationStrategy::TimeBased(\n        credentials::rotation::TimeBasedRotation {\n            rotation_interval: std::time::Duration::from_secs(300), // 5 minutes for demo\n            rotation_time: None,\n            grace_period: std::time::Duration::from_secs(60),\n        }\n    );\n\n    let rotation_task_id = worker_manager.schedule_rotation(\"database-credentials\", rotation_strategy).await?;\n    println!(\" Scheduled rotation task: {}\", rotation_task_id);\n\n    // 11. Get rotation status\n    tokio::time::sleep(std::time::Duration::from_secs(1)).await;\n    let rotation_status = worker_manager.get_rotation_status().await?;\n    println!(\"  Total tasks: {}\", rotation_status.total_tasks);\n    println!(\"  Active tasks: {}\", rotation_status.active_tasks);\n    println!(\"  Healthy providers: {}/{}\", rotation_status.health_status.running_tasks, \n             rotation_status.health_status.is_healthy);\n\n    // 12. Demonstrate provider switching\n    println!(\"\\n--- Provider Switching ---\");\n    let providers = worker_manager.list_providers();\n    println!(\"  Available providers: {:?}\", providers);\n\n    // 13. Health check\n    println!(\"\\n--- System Health Check ---\");\n    let health_status = worker_manager.health_check().await?;\n    println!(\"  Overall health: {}\", if health_status.is_healthy { \"Healthy\" } else { \"Unhealthy\" });\n    println!(\"  Healthy providers: {}/{}\", health_status.healthy_providers, health_status.total_providers);\n    println!(\"  Rotation engine: {}\", if health_status.rotation_engine_healthy { \"Running\" } else { \"Not running\" });\n\n    // 14. Clean up\n    println!(\"\\n--- Cleanup ---\");\n    worker_manager.stop_rotation_engine().await?;\n    println!(\" Stopped rotation engine\");\n    println!(\" Example completed successfully\");\n\n    println!(\"\\n--- Summary ---\");\n    println!(\" Created and managed worker lifecycle\");\n    println!(\" Configured credential storage and rotation\");\n    println!(\" Monitored system capacity and health\");\n    println!(\" Tested provider abstraction and switching\");\n    println!(\" Demonstrated production-ready patterns\");\n\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","lib.rs"],"content":"//! Worker Manager Abstraction Layer\n//! \n//! A comprehensive Rust-based worker management system providing abstractions\n//! for multiple infrastructure providers and credential management with automatic rotation.\n\n#![warn(missing_docs)]\n\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n// Re-export main traits and types\npub use crate::traits::*;\npub use crate::providers::{WorkerManagerProvider, KubernetesProvider, DockerProvider};\npub use crate::credentials::{CredentialProvider, RotationEngine, RotationEvent, RotationStrategy};\n\n// Provider modules\npub mod traits {\n    pub use worker_manager_traits::*;\n}\n\npub mod providers {\n    pub mod kubernetes {\n        pub use worker_manager_providers_kubernetes::*;\n    }\n    pub mod docker {\n        pub use worker_manager_providers_docker::*;\n    }\n    pub mod plugin {\n        pub mod registry {\n            pub use worker_manager_providers_plugin_registry::*;\n        }\n    }\n}\n\n// Credential management modules\npub mod credentials {\n    pub mod mod_rs {\n        pub use worker_manager_credentials_mod::*;\n    }\n    pub mod vault {\n        pub use worker_manager_credentials_vault::*;\n    }\n    pub mod aws_secrets {\n        pub use worker_manager_credentials_aws_secrets::*;\n    }\n    pub mod keycloak {\n        pub use worker_manager_credentials_keycloak::*;\n    }\n    pub mod rotation {\n        pub use worker_manager_credentials_rotation::*;\n    }\n}\n\n// Ephemeral worker management\npub mod ephemeral {\n    pub mod auto_scaling {\n        pub use worker_manager_ephemeral_auto_scaling::*;\n    }\n    pub mod health_checks {\n        pub use worker_manager_ephemeral_health_checks::*;\n    }\n    pub mod cost_optimization {\n        pub use worker_manager_ephemeral_cost_optimization::*;\n    }\n}\n\n// Security and networking\npub mod security {\n    pub mod rbac {\n        pub use worker_manager_security_rbac::*;\n    }\n    pub mod network_policies {\n        pub use worker_manager_security_network_policies::*;\n    }\n}\n\n/// Main Worker Manager\n#[derive(Debug)]\npub struct WorkerManager {\n    providers: HashMap\u003cString, Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e,\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    rotation_engine: Arc\u003cRwLock\u003cOption\u003crotation::RotationEngine\u003e\u003e\u003e,\n    current_provider: String,\n}\n\nimpl WorkerManager {\n    /// Create new Worker Manager\n    pub fn new(\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Self {\n        Self {\n            providers: HashMap::new(),\n            credential_provider,\n            rotation_engine: Arc::new(RwLock::new(None)),\n            current_provider: \"kubernetes\".to_string(),\n        }\n    }\n\n    /// Register a provider\n    pub fn register_provider(\n        \u0026mut self,\n        name: String,\n        provider: Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e,\n    ) {\n        self.providers.insert(name, provider);\n    }\n\n    /// Set default provider\n    pub fn set_default_provider(\u0026mut self, provider_name: String) {\n        if self.providers.contains_key(\u0026provider_name) {\n            self.current_provider = provider_name;\n        }\n    }\n\n    /// Get current provider\n    pub fn get_provider(\u0026self) -\u003e Option\u003cArc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e\u003e {\n        self.providers.get(\u0026self.current_provider).cloned()\n    }\n\n    /// Get specific provider\n    pub fn get_provider_by_name(\n        \u0026self,\n        name: \u0026str,\n    ) -\u003e Option\u003cArc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e {\n        self.providers.get(name).cloned()\n    }\n\n    /// Start rotation engine\n    pub async fn start_rotation_engine(\n        \u0026self,\n        config: credentials::rotation::RotationEngineConfig,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        let mut rotation_engine = self.rotation_engine.write().await;\n        \n        let engine = credentials::rotation::RotationEngine::new(\n            self.credential_provider.clone(),\n            config,\n        );\n        \n        // Start the rotation engine\n        let engine_clone = engine.clone();\n        tokio::spawn(async move {\n            if let Err(e) = engine_clone.start().await {\n                eprintln!(\"Rotation engine failed to start: {:?}\", e);\n            }\n        });\n        \n        *rotation_engine = Some(engine);\n        Ok(())\n    }\n\n    /// Stop rotation engine\n    pub async fn stop_rotation_engine(\u0026self) -\u003e Result\u003c(), ProviderError\u003e {\n        let rotation_engine = self.rotation_engine.read().await;\n        if let Some(engine) = rotation_engine.as_ref() {\n            engine.stop().await.map_err(|e| ProviderError::internal(\n                format!(\"Failed to stop rotation engine: {}\", e),\n                Some(\"rotation_engine\".to_string()),\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: Some(\"rotation_engine\".to_string()),\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        }\n        Ok(())\n    }\n\n    /// Create worker with default provider\n    pub async fn create_worker(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        config: \u0026ProviderConfig,\n    ) -\u003e Result\u003cWorker, ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.create_worker(spec, config).await\n    }\n\n    /// Terminate worker\n    pub async fn terminate_worker(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.terminate_worker(worker_id).await\n    }\n\n    /// Get worker status\n    pub async fn get_worker_status(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n    ) -\u003e Result\u003cWorkerState, ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.get_worker_status(worker_id).await\n    }\n\n    /// Get system capacity\n    pub async fn get_capacity(\u0026self) -\u003e Result\u003cCapacityInfo, ProviderError\u003e {\n        let provider = self.get_provider()\n            .ok_or_else(|| ProviderError::configuration(\n                \"No provider configured\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n        \n        provider.get_capacity().await\n    }\n\n    /// Schedule rotation\n    pub async fn schedule_rotation(\n        \u0026self,\n        credential_name: \u0026str,\n        strategy: credentials::rotation::RotationStrategy,\n    ) -\u003e Result\u003cuuid::Uuid, ProviderError\u003e {\n        let rotation_engine = self.rotation_engine.read().await;\n        if let Some(engine) = rotation_engine.as_ref() {\n            engine.schedule_rotation(credential_name, strategy)\n                .await\n                .map_err(|e| ProviderError::credentials(\n                    format!(\"Failed to schedule rotation: {}\", e),\n                    \"rotation_engine\".to_string()\n                ))\n        } else {\n            Err(ProviderError::configuration(\n                \"Rotation engine not started\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: Some(\"rotation_engine\".to_string()),\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))\n        }\n    }\n\n    /// Get rotation status\n    pub async fn get_rotation_status(\u0026self) -\u003e Result\u003ccredentials::rotation::RotationEngineStatus, ProviderError\u003e {\n        let rotation_engine = self.rotation_engine.read().await;\n        if let Some(engine) = rotation_engine.as_ref() {\n            Ok(engine.get_status().await)\n        } else {\n            Err(ProviderError::configuration(\n                \"Rotation engine not started\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: Some(\"rotation_engine\".to_string()),\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))\n        }\n    }\n\n    /// Get available providers\n    pub fn list_providers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.providers.keys().cloned().collect()\n    }\n\n    /// Health check\n    pub async fn health_check(\u0026self) -\u003e Result\u003cWorkerManagerHealthStatus, ProviderError\u003e {\n        let providers_status = futures::future::join_all(\n            self.providers.values().map(|provider| {\n                let name = self.providers.iter()\n                    .find_map(|(k, v)| if v.as_ref() == provider.as_ref() { Some(k.clone()) } else { None })\n                    .unwrap_or_else(|| \"unknown\".to_string());\n                \n                async move {\n                    match provider.get_capacity().await {\n                        Ok(_) =\u003e (name, true, None),\n                        Err(e) =\u003e (name, false, Some(e.to_string())),\n                    }\n                }\n            })\n        ).await;\n\n        let healthy_providers = providers_status.iter().filter(|(_, healthy, _)| *healthy).count();\n        let is_healthy = healthy_providers \u003e 0;\n\n        Ok(WorkerManagerHealthStatus {\n            is_healthy,\n            healthy_providers,\n            total_providers: self.providers.len(),\n            provider_details: providers_status.into_iter().map(|(name, healthy, error)| {\n                (name, healthy, error)\n            }).collect(),\n            rotation_engine_healthy: self.rotation_engine.read().await.is_some(),\n            timestamp: chrono::Utc::now(),\n        })\n    }\n}\n\n/// Worker Manager health status\n#[derive(Debug, Clone)]\npub struct WorkerManagerHealthStatus {\n    pub is_healthy: bool,\n    pub healthy_providers: usize,\n    pub total_providers: usize,\n    pub provider_details: Vec\u003c(String, bool, Option\u003cString\u003e)\u003e,\n    pub rotation_engine_healthy: bool,\n    pub timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Worker Manager Builder for easier setup\n#[derive(Debug)]\npub struct WorkerManagerBuilder {\n    credential_provider: Option\u003cArc\u003cdyn CredentialProvider + Send + Sync\u003e\u003e,\n    providers: HashMap\u003cString, Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e\u003e,\n    default_provider: Option\u003cString\u003e,\n}\n\nimpl WorkerManagerBuilder {\n    /// Create new builder\n    pub fn new() -\u003e Self {\n        Self {\n            credential_provider: None,\n            providers: HashMap::new(),\n            default_provider: None,\n        }\n    }\n\n    /// Set credential provider\n    pub fn credential_provider(mut self, provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e) -\u003e Self {\n        self.credential_provider = Some(provider);\n        self\n    }\n\n    /// Add provider\n    pub fn add_provider(\n        mut self,\n        name: String,\n        provider: Arc\u003cdyn WorkerManagerProvider + Send + Sync\u003e,\n    ) -\u003e Self {\n        self.providers.insert(name, provider);\n        if self.default_provider.is_none() {\n            self.default_provider = Some(name);\n        }\n        self\n    }\n\n    /// Set default provider\n    pub fn default_provider(mut self, name: String) -\u003e Self {\n        self.default_provider = Some(name);\n        self\n    }\n\n    /// Build Worker Manager\n    pub fn build(self) -\u003e Result\u003cWorkerManager, ProviderError\u003e {\n        let credential_provider = self.credential_provider\n            .ok_or_else(|| ProviderError::configuration(\n                \"Credential provider is required\".to_string(),\n                None,\n                crate::traits::ErrorContext {\n                    operation_id: None,\n                    worker_id: None,\n                    provider: None,\n                    configuration: None,\n                    timestamp: chrono::Utc::now(),\n                }\n            ))?;\n\n        let mut manager = WorkerManager::new(credential_provider);\n\n        for (name, provider) in self.providers {\n            manager.register_provider(name, provider);\n        }\n\n        if let Some(default) = self.default_provider {\n            manager.set_default_provider(default);\n        }\n\n        Ok(manager)\n    }\n}\n\nimpl Default for WorkerManagerBuilder {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Helper functions for creating common configurations\npub mod builders {\n    use super::*;\n\n    /// Create a simple in-memory provider setup for testing\n    pub async fn create_simple_setup() -\u003e Result\u003c(WorkerManager, Vec\u003cArc\u003cdyn CredentialProvider + Send + Sync\u003e\u003e), ProviderError\u003e {\n        use crate::providers::docker::DockerProvider;\n        use crate::providers::kubernetes::KubernetesProvider;\n        use crate::credentials::SimpleCredentialProvider;\n\n        let simple_credential = Arc::new(SimpleCredentialProvider::new());\n        \n        let mut k8s_config = crate::providers::kubernetes::KubernetesConfig::default();\n        k8s_config.namespace = \"default\".to_string();\n        \n        let k8s_provider = Arc::new(KubernetesProvider::from_provider_config(\n            \u0026ProviderConfig::kubernetes(\"default\".to_string()),\n            simple_credential.clone()\n        ).await?);\n\n        let docker_provider = Arc::new(DockerProvider::from_provider_config(\n            \u0026ProviderConfig::docker(),\n            simple_credential.clone()\n        ).await?);\n\n        let mut builder = WorkerManagerBuilder::new()\n            .credential_provider(simple_credential.clone())\n            .add_provider(\"kubernetes\".to_string(), k8s_provider)\n            .add_provider(\"docker\".to_string(), docker_provider)\n            .default_provider(\"docker\".to_string());\n\n        let manager = builder.build()?;\n        Ok((manager, vec![simple_credential]))\n    }\n\n    /// Create a production setup with multiple credential providers\n    pub async fn create_production_setup() -\u003e Result\u003c(WorkerManager, Vec\u003cArc\u003cdyn CredentialProvider + Send + Sync\u003e\u003e), ProviderError\u003e {\n        use crate::credentials::vault::HashiCorpVaultProvider;\n        use crate::credentials::aws_secrets::AWSSecretsManagerProvider;\n        use crate::credentials::keycloak::KeycloakServiceAccountProvider;\n\n        // This would require proper configuration in production\n        // For now, return a simple setup\n        create_simple_setup().await\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","providers","docker","mod.rs"],"content":"//! Docker Provider Implementation\n//! \n//! This module provides a complete implementation of the WorkerManagerProvider trait\n//! using Docker as the underlying infrastructure provider.\n\nuse crate::traits::*;\nuse crate::credentials::CredentialProvider;\nuse async_trait::async_trait;\nuse chrono::Utc;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\nuse uuid::Uuid;\n\n/// Docker-specific configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DockerConfig {\n    pub host: Option\u003cString\u003e,\n    pub network_mode: String,\n    pub registry_auth: Option\u003cString\u003e,\n    pub cpu_shares: Option\u003cu64\u003e,\n    pub memory_limit: Option\u003cu64\u003e,\n    pub ulimits: Option\u003cVec\u003cString\u003e\u003e,\n    pub log_driver: Option\u003cString\u003e,\n    pub log_options: Option\u003cHashMap\u003cString, String\u003e\u003e,\n}\n\nimpl Default for DockerConfig {\n    fn default() -\u003e Self {\n        Self {\n            host: None,\n            network_mode: \"bridge\".to_string(),\n            registry_auth: None,\n            cpu_shares: Some(1024),\n            memory_limit: Some(1024 * 1024 * 1024), // 1GB\n            ulimits: None,\n            log_driver: Some(\"json-file\".to_string()),\n            log_options: Some(HashMap::from([\n                (\"max-size\".to_string(), \"10m\".to_string()),\n                (\"max-file\".to_string(), \"3\".to_string()),\n            ])),\n        }\n    }\n}\n\n/// Container tracking information\n#[derive(Debug, Clone)]\nstruct DockerWorker {\n    id: WorkerId,\n    container_id: String,\n    network_name: Option\u003cString\u003e,\n    volume_mounts: Vec\u003cString\u003e,\n    created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    last_health_check: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Docker Provider Implementation\npub struct DockerProvider {\n    client: docker::Docker,\n    config: DockerConfig,\n    workers: Arc\u003cRwLock\u003cHashMap\u003cWorkerId, DockerWorker\u003e\u003e\u003e,\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n}\n\nimpl DockerProvider {\n    /// Create a new Docker Provider\n    pub async fn new(\n        config: DockerConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let client = docker::Docker::connect_with_unix_socket(\u0026config.host.unwrap_or_else(|| \"/var/run/docker.sock\".to_string()), 115, docker::Docker::default_timeout())\n            .map_err(|e| ProviderError::infrastructure(\n                format!(\"Failed to create Docker client: {}\", e),\n                \"docker\".to_string()\n            ))?;\n\n        Ok(Self {\n            client,\n            config,\n            workers: Arc::new(RwLock::new(HashMap::new())),\n            credential_provider,\n        })\n    }\n\n    /// Get Docker configuration from ProviderConfig\n    pub fn from_provider_config(\n        provider_config: \u0026ProviderConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let mut config = DockerConfig::default();\n        \n        if let serde_json::Value::Object(obj) = \u0026provider_config.specific_config {\n            if let Some(host) = obj.get(\"host\").and_then(|v| v.as_str()) {\n                config.host = Some(host.to_string());\n            }\n            if let Some(network_mode) = obj.get(\"network_mode\").and_then(|v| v.as_str()) {\n                config.network_mode = network_mode.to_string();\n            }\n            if let Some(registry_auth) = obj.get(\"registry_auth\").and_then(|v| v.as_str()) {\n                config.registry_auth = Some(registry_auth.to_string());\n            }\n        }\n\n        Self::new(config, credential_provider)\n    }\n\n    /// Create Docker network for isolation\n    async fn create_network(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cString, ProviderError\u003e {\n        let network_name = format!(\"worker-network-{}\", worker_id);\n        \n        let network_config = docker::models::NetworkCreateRequest::builder()\n            .name(\u0026network_name)\n            .driver(\"bridge\")\n            .internal(false)\n            .ipam(docker::models::IPAMConfig::builder().build())\n            .options({\n                let mut options = HashMap::new();\n                options.insert(\"com.docker.network.bridge.enable_icc\".to_string(), \"true\".to_string());\n                options.insert(\"com.docker.network.bridge.enable_ip_masquerade\".to_string(), \"true\".to_string());\n                options\n            })\n            .build();\n\n        match self.client.create_network(network_config).await {\n            Ok(_) =\u003e Ok(network_name),\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to create Docker network: {}\", e),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    /// Get container logs\n    async fn get_container_logs(\u0026self, container_id: \u0026str) -\u003e Result\u003cString, ProviderError\u003e {\n        let mut logs = String::new();\n        \n        match self.client.logs(container_id, \u0026docker::Docker::default_timeout()).await {\n            Ok(log_stream) =\u003e {\n                for result in log_stream {\n                    match result {\n                        Ok(log) =\u003e {\n                            match log {\n                                docker::LogOutput::StdOut { message } =\u003e {\n                                    logs.push_str(\u0026String::from_utf8_lossy(\u0026message));\n                                }\n                                docker::LogOutput::StdErr { message } =\u003e {\n                                    logs.push_str(\u0026format!(\"STDERR: {}\", String::from_utf8_lossy(\u0026message)));\n                                }\n                                docker::LogOutput::StdIn { message } =\u003e {\n                                    logs.push_str(\u0026format!(\"STDIN: {}\", String::from_utf8_lossy(\u0026message)));\n                                }\n                                docker::LogOutput::Unknown { data } =\u003e {\n                                    logs.push_str(\u0026format!(\"UNKNOWN: {}\", String::from_utf8_lossy(\u0026data)));\n                                }\n                            }\n                        }\n                        Err(e) =\u003e return Err(ProviderError::infrastructure(\n                            format!(\"Failed to read log: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n                Ok(logs)\n            }\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to get container logs: {}\", e),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    /// Create volume mount configuration\n    async fn create_volume_mounts(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n    ) -\u003e Result\u003cVec\u003cdocker::models::Mount\u003e, ProviderError\u003e {\n        let mut mounts = Vec::new();\n\n        for volume_mount in \u0026spec.volumes {\n            let mount = match \u0026volume_mount.source {\n                VolumeSource::EmptyDir =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::TMPFS)\n                        .source(None)\n                        .target(\u0026volume_mount.mount_path)\n                        .tmpfs_options(Some(docker::models::TmpfsOptions {\n                            size_bytes: None,\n                            mode: Some(0o755),\n                        }))\n                        .build()\n                }\n                VolumeSource::HostPath(path) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::BIND)\n                        .source(Some(path.clone()))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n                VolumeSource::Secret(secret_name) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::VOLUME)\n                        .source(Some(format!(\"secret-{}\", secret_name)))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n                VolumeSource::ConfigMap(config_name) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::VOLUME)\n                        .source(Some(format!(\"config-{}\", config_name)))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n                VolumeSource::PersistentVolume(pv_name) =\u003e {\n                    docker::models::Mount::builder()\n                        .type_(docker::models::MountTypeEnum::VOLUME)\n                        .source(Some(pv_name.clone()))\n                        .target(\u0026volume_mount.mount_path)\n                        .read_only(Some(volume_mount.read_only))\n                        .build()\n                }\n            };\n            mounts.push(mount);\n        }\n\n        Ok(mounts)\n    }\n\n    /// Create health check configuration\n    async fn create_health_check(\u0026self, spec: \u0026RuntimeSpec) -\u003e Option\u003cdocker::models::HealthConfig\u003e {\n        Some(docker::models::HealthConfig {\n            test: Some(vec![\"CMD\".to_string(), \"curl\".to_string(), \"-f\".to_string(), \"http://localhost:8080/health\".to_string()]),\n            interval: Some(30_000_000_000), // 30 seconds in nanoseconds\n            timeout: Some(5_000_000_000),  // 5 seconds\n            retries: Some(3),\n            start_period: Some(0),\n        })\n    }\n}\n\n#[async_trait]\nimpl WorkerManagerProvider for DockerProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"docker\"\n    }\n\n    async fn create_worker(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        config: \u0026ProviderConfig,\n    ) -\u003e Result\u003cWorker, ProviderError\u003e {\n        let worker_id = WorkerId::new();\n        \n        // Create network for isolation\n        let network_name = self.create_network(\u0026worker_id).await?;\n        \n        // Create volume mounts\n        let mounts = self.create_volume_mounts(spec).await?;\n\n        // Prepare container creation options\n        let mut create_options = docker::models::CreateContainerOptions {\n            name: Some(format!(\"worker-{}\", worker_id)),\n            ..Default::default()\n        };\n\n        // Prepare host configuration\n        let mut host_config = docker::models::HostConfig::builder()\n            .network_mode(Some(network_name.clone()))\n            .memory(self.config.memory_limit)\n            .cpu_shares(self.config.cpu_shares)\n            .restart_policy(Some(docker::models::RestartPolicy {\n                name: Some(docker::models::RestartPolicyNameEnum::ALWAYS),\n                maximum_retry_count: None,\n            }))\n            .build();\n\n        // Add volume mounts\n        host_config.mounts = Some(mounts);\n\n        // Add log configuration\n        if let Some(log_driver) = \u0026self.config.log_driver {\n            let mut log_config = docker::models::LogConfig {\n                type_: Some(log_driver.clone()),\n                config: self.config.log_options.clone(),\n            };\n            host_config.log_config = Some(log_config);\n        }\n\n        // Add environment variables\n        let mut env_vars = Vec::new();\n        for (key, value) in \u0026spec.env {\n            env_vars.push(format!(\"{}={}\", key, value));\n        }\n\n        // Add secrets as environment variables if not using volume mounts\n        for secret_ref in \u0026spec.secret_refs {\n            match self.credential_provider.get_secret(secret_ref, None).await {\n                Ok(secret) =\u003e {\n                    for (key, value) in \u0026secret.values {\n                        env_vars.push(format!(\"{}_{}={}\", secret_ref.to_uppercase(), key, value));\n                    }\n                }\n                Err(e) =\u003e {\n                    return Err(ProviderError::credentials(\n                        format!(\"Failed to get secret {}: {}\", secret_ref, e),\n                        \"docker\".to_string()\n                    ));\n                }\n            }\n        }\n\n        // Create container\n        let container_config = docker::models::ContainerCreateBody {\n            image: spec.image.clone(),\n            cmd: spec.command.clone(),\n            env: Some(env_vars),\n            host_config: Some(host_config),\n            healthcheck: self.create_health_check(spec).await,\n            ..Default::default()\n        };\n\n        match self.client.create_container(\u0026create_options, \u0026container_config).await {\n            Ok(container_info) =\u003e {\n                // Start the container\n                match self.client.start_container(\u0026container_info.id).await {\n                    Ok(_) =\u003e {\n                        let worker = Worker {\n                            id: worker_id.clone(),\n                            state: WorkerState::Running,\n                            runtime_spec: spec.clone(),\n                            provider_config: config.clone(),\n                            metadata: HashMap::new(),\n                            created_at: Utc::now(),\n                            last_update: Utc::now(),\n                        };\n\n                        // Track the worker\n                        {\n                            let mut workers = self.workers.write().await;\n                            workers.insert(worker_id.clone(), DockerWorker {\n                                id: worker_id,\n                                container_id: container_info.id.clone(),\n                                network_name: Some(network_name),\n                                volume_mounts: Vec::new(),\n                                created_at: Utc::now(),\n                                last_health_check: Utc::now(),\n                            });\n                        }\n\n                        Ok(worker)\n                    }\n                    Err(e) =\u003e {\n                        // Clean up container if start failed\n                        let _ = self.client.remove_container(\u0026container_info.id, \u0026docker::Docker::default_timeout()).await;\n                        Err(ProviderError::infrastructure(\n                            format!(\"Failed to start Docker container: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n            }\n            Err(e) =\u003e {\n                // Clean up network if container creation failed\n                let _ = self.client.remove_network(\u0026network_name).await;\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to create Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn terminate_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            let network_name = worker.network_name.clone();\n            drop(workers);\n\n            // Stop and remove container\n            let remove_options = docker::models::RemoveContainerOptions {\n                force: true,\n                v: false,\n                link: false,\n            };\n\n            match self.client.stop_container(\u0026container_id, \u0026docker::Docker::default_timeout()).await {\n                Ok(_) =\u003e {\n                    match self.client.remove_container(\u0026container_id, \u0026remove_options).await {\n                        Ok(_) =\u003e {\n                            // Remove network if it exists\n                            if let Some(network) = network_name {\n                                let _ = self.client.remove_network(\u0026network).await;\n                            }\n\n                            // Remove from tracking\n                            {\n                                let mut workers = self.workers.write().await;\n                                workers.remove(worker_id);\n                            }\n\n                            Ok(())\n                        }\n                        Err(e) =\u003e Err(ProviderError::infrastructure(\n                            format!(\"Failed to remove Docker container: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to stop Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerState, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            match self.client.inspect_container(\u0026container_id).await {\n                Ok(inspect_info) =\u003e {\n                    // Update last health check\n                    {\n                        let mut workers = self.workers.write().await;\n                        if let Some(w) = workers.get_mut(worker_id) {\n                            w.last_health_check = Utc::now();\n                        }\n                    }\n\n                    match inspect_info.state {\n                        Some(state) =\u003e {\n                            match state.status {\n                                Some(docker::models::ContainerStateStatusEnum::RUNNING) =\u003e {\n                                    // Check if container has a health check and if it's healthy\n                                    if let Some(health) = \u0026inspect_info.health {\n                                        match health.status {\n                                            Some(docker::models::HealthStatus::HEALTHY) =\u003e Ok(WorkerState::Running),\n                                            Some(docker::models::HealthStatus::UNHEALTHY) =\u003e Ok(WorkerState::Failed {\n                                                reason: \"Health check failed\".to_string()\n                                            }),\n                                            _ =\u003e Ok(WorkerState::Running),\n                                        }\n                                    } else {\n                                        Ok(WorkerState::Running)\n                                    }\n                                }\n                                Some(docker::models::ContainerStateStatusEnum::EXITED) =\u003e {\n                                    Ok(WorkerState::Terminated)\n                                }\n                                Some(docker::models::ContainerStateStatusEnum::PAUSED) =\u003e {\n                                    Ok(WorkerState::Paused)\n                                }\n                                Some(docker::models::ContainerStateStatusEnum::DEAD) =\u003e {\n                                    Ok(WorkerState::Failed {\n                                        reason: \"Container is dead\".to_string()\n                                    })\n                                }\n                                _ =\u003e Ok(WorkerState::Unknown),\n                            }\n                        }\n                        None =\u003e Ok(WorkerState::Unknown),\n                    }\n                }\n                Err(e) =\u003e {\n                    if e.is_not_found() {\n                        Ok(WorkerState::Terminated)\n                    } else {\n                        Err(ProviderError::infrastructure(\n                            format!(\"Failed to inspect Docker container: {}\", e),\n                            \"docker\".to_string()\n                        ))\n                    }\n                }\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn get_logs(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cLogStreamRef, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Get logs through Docker API\n            let logs = self.get_container_logs(\u0026container_id).await?;\n\n            Ok(LogStreamRef {\n                stream_id: Uuid::new_v4(),\n                worker_id: worker_id.clone(),\n                format: LogFormat::Json,\n                endpoints: vec![\n                    format!(\"docker://logs/{}\", container_id),\n                ],\n            })\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn port_forward(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        local_port: u16,\n        remote_port: u16,\n    ) -\u003e Result\u003cString, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            let network_name = worker.network_name.clone().unwrap_or_default();\n            drop(workers);\n\n            // Docker port forwarding through exec or attach\n            let endpoint = format!(\"localhost:{} (container:{})\", local_port, container_id);\n            \n            // In practice, you would use Docker's port forwarding mechanisms\n            // This is a simplified implementation\n            Ok(endpoint)\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn get_capacity(\u0026self) -\u003e Result\u003cCapacityInfo, ProviderError\u003e {\n        match self.client.info().await {\n            Ok(info) =\u003e {\n                let active_workers = {\n                    let workers = self.workers.read().await;\n                    workers.len() as u32\n                };\n\n                Ok(CapacityInfo {\n                    total_resources: ResourceQuota {\n                        cpu_m: (info.cpu_count.unwrap_or(1) * 1000) as u64,\n                        memory_mb: (info.mem_total.unwrap_or(1024 * 1024 * 1024) / 1024 / 1024) as u64,\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    used_resources: ResourceQuota {\n                        cpu_m: active_workers as u64 * 500, // Estimate\n                        memory_mb: active_workers as u64 * 256, // Estimate\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    available_resources: ResourceQuota {\n                        cpu_m: ((info.cpu_count.unwrap_or(1) * 1000) as u64).saturating_sub(active_workers as u64 * 500),\n                        memory_mb: ((info.mem_total.unwrap_or(1024 * 1024 * 1024) / 1024 / 1024) as u64).saturating_sub(active_workers as u64 * 256),\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    active_workers,\n                    last_updated: Utc::now(),\n                })\n            }\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to get Docker capacity: {}\", e),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn execute_command(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        command: Vec\u003cString\u003e,\n        timeout: Option\u003cstd::time::Duration\u003e,\n    ) -\u003e Result\u003cExecutionResult, ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            let start_time = std::time::Instant::now();\n            \n            // Execute command in container\n            let exec_config = docker::models::ExecCreateBody {\n                cmd: Some(command),\n                attach_stdout: Some(true),\n                attach_stderr: Some(true),\n                attach_stdin: Some(false),\n                working_dir: None,\n                user: None,\n                env: None,\n            };\n\n            match self.client.exec_create(\u0026container_id, \u0026exec_config).await {\n                Ok(exec_info) =\u003e {\n                    match self.client.exec_start(\u0026exec_info.id, \u0026docker::Docker::default_timeout()).await {\n                        Ok(output) =\u003e {\n                            let duration = start_time.elapsed();\n                            let finished_at = Utc::now();\n                            let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n\n                            let mut stdout = String::new();\n                            let mut stderr = String::new();\n\n                            for result in output {\n                                match result {\n                                    Ok(output) =\u003e {\n                                        match output {\n                                            docker::LogOutput::StdOut { message } =\u003e {\n                                                stdout.push_str(\u0026String::from_utf8_lossy(\u0026message));\n                                            }\n                                            docker::LogOutput::StdErr { message } =\u003e {\n                                                stderr.push_str(\u0026String::from_utf8_lossy(\u0026message));\n                                            }\n                                            _ =\u003e {}\n                                        }\n                                    }\n                                    Err(e) =\u003e {\n                                        stderr.push_str(\u0026format!(\"Error reading output: {}\", e));\n                                    }\n                                }\n                            }\n\n                            Ok(ExecutionResult {\n                                exit_code: 0,\n                                stdout,\n                                stderr,\n                                duration,\n                                started_at,\n                                finished_at,\n                            })\n                        }\n                        Err(e) =\u003e {\n                            let duration = start_time.elapsed();\n                            let finished_at = Utc::now();\n                            let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n\n                            Ok(ExecutionResult {\n                                exit_code: -1,\n                                stdout: String::new(),\n                                stderr: format!(\"Command execution failed: {}\", e),\n                                duration,\n                                started_at,\n                                finished_at,\n                            })\n                        }\n                    }\n                }\n                Err(e) =\u003e {\n                    let duration = start_time.elapsed();\n                    let finished_at = Utc::now();\n                    let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n\n                    Ok(ExecutionResult {\n                        exit_code: -1,\n                        stdout: String::new(),\n                        stderr: format!(\"Failed to create exec: {}\", e),\n                        duration,\n                        started_at,\n                        finished_at,\n                    })\n                }\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn restart_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Restart container\n            match self.client.restart_container(\u0026container_id, \u0026docker::Docker::default_timeout()).await {\n                Ok(_) =\u003e Ok(()),\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to restart Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn pause_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Pause container\n            match self.client.pause_container(\u0026container_id).await {\n                Ok(_) =\u003e Ok(()),\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to pause Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    async fn resume_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let workers = self.workers.read().await;\n        if let Some(worker) = workers.get(worker_id) {\n            let container_id = worker.container_id.clone();\n            drop(workers);\n\n            // Unpause container\n            match self.client.unpause_container(\u0026container_id).await {\n                Ok(_) =\u003e Ok(()),\n                Err(e) =\u003e Err(ProviderError::infrastructure(\n                    format!(\"Failed to resume Docker container: {}\", e),\n                    \"docker\".to_string()\n                ))\n            }\n        } else {\n            Err(ProviderError::not_found(\n                \"Worker not found\".to_string(),\n                \"worker\".to_string(),\n                worker_id.to_string(),\n                \"docker\".to_string()\n            ))\n        }\n    }\n\n    fn stream_worker_events(\u0026self) -\u003e tokio_stream::wrappers::IntervalStream {\n        let interval = tokio::time::interval(std::time::Duration::from_secs(30));\n        tokio_stream::wrappers::IntervalStream::new(interval)\n    }\n}\n\nimpl Drop for DockerProvider {\n    fn drop(\u0026mut self) {\n        // Clean up any remaining networks on drop\n        // In practice, you might want to implement a proper cleanup mechanism\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","rubentxu","Proyectos","rust","hodei-jobs","worker-manager","providers","kubernetes","mod.rs"],"content":"//! Kubernetes Provider Implementation\n//! \n//! This module provides a complete implementation of the WorkerManagerProvider trait\n//! using Kubernetes as the underlying infrastructure provider.\n\npub mod client;\npub mod crds;\npub mod security;\n\nuse crate::traits::*;\nuse crate::credentials::CredentialProvider;\nuse async_trait::async_trait;\nuse chrono::Utc;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\nuse uuid::Uuid;\n\n/// Kubernetes-specific configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KubernetesConfig {\n    pub namespace: String,\n    pub service_account: Option\u003cString\u003e,\n    pub node_selector: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    pub tolerations: Option\u003cVec\u003cToleration\u003e\u003e,\n    pub node_affinity: Option\u003cString\u003e,\n    pub pod_disruption_budget: Option\u003cu32\u003e,\n    pub termination_grace_period_seconds: Option\u003ci64\u003e,\n    pub image_pull_secrets: Option\u003cVec\u003cString\u003e\u003e,\n    pub priority_class: Option\u003cString\u003e,\n    pub security_context: Option\u003cContainerSecurity\u003e,\n}\n\nimpl Default for KubernetesConfig {\n    fn default() -\u003e Self {\n        Self {\n            namespace: \"default\".to_string(),\n            service_account: None,\n            node_selector: None,\n            tolerations: None,\n            node_affinity: None,\n            pod_disruption_budget: None,\n            termination_grace_period_seconds: Some(30),\n            image_pull_secrets: None,\n            priority_class: None,\n            security_context: None,\n        }\n    }\n}\n\n/// Worker tracking in Kubernetes\n#[derive(Debug, Clone)]\nstruct KubernetesWorker {\n    id: WorkerId,\n    pod_name: String,\n    node_name: Option\u003cString\u003e,\n    created_at: chrono::DateTime\u003cchrono::Utc\u003e,\n    last_heartbeat: chrono::DateTime\u003cchrono::Utc\u003e,\n}\n\n/// Kubernetes Provider Implementation\npub struct KubernetesProvider {\n    client: kube::Client,\n    config: KubernetesConfig,\n    workers: Arc\u003cRwLock\u003cHashMap\u003cWorkerId, KubernetesWorker\u003e\u003e\u003e,\n    credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n}\n\nimpl KubernetesProvider {\n    /// Create a new Kubernetes Provider\n    pub async fn new(\n        config: KubernetesConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let client = kube::Client::try_default()\n            .await\n            .map_err(|e| ProviderError::infrastructure(\n                format!(\"Failed to create Kubernetes client: {}\", e),\n                \"kubernetes\".to_string()\n            ))?;\n\n        Ok(Self {\n            client,\n            config,\n            workers: Arc::new(RwLock::new(HashMap::new())),\n            credential_provider,\n        })\n    }\n\n    /// Get Kubernetes configuration from ProviderConfig\n    pub fn from_provider_config(\n        provider_config: \u0026ProviderConfig,\n        credential_provider: Arc\u003cdyn CredentialProvider + Send + Sync\u003e,\n    ) -\u003e Result\u003cSelf, ProviderError\u003e {\n        let mut config = KubernetesConfig::default();\n        \n        if let serde_json::Value::Object(obj) = \u0026provider_config.specific_config {\n            if let Some(namespace) = obj.get(\"namespace\").and_then(|v| v.as_str()) {\n                config.namespace = namespace.to_string();\n            }\n            if let Some(service_account) = obj.get(\"service_account\").and_then(|v| v.as_str()) {\n                config.service_account = Some(service_account.to_string());\n            }\n            // Additional Kubernetes-specific configurations...\n        }\n\n        Self::new(config, credential_provider)\n    }\n}\n\n#[async_trait]\nimpl WorkerManagerProvider for KubernetesProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"kubernetes\"\n    }\n\n    async fn create_worker(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        config: \u0026ProviderConfig,\n    ) -\u003e Result\u003cWorker, ProviderError\u003e {\n        let worker_id = WorkerId::new();\n        let mut labels = spec.labels.clone();\n        labels.insert(\"worker-id\".to_string(), worker_id.to_string());\n        labels.insert(\"provider\".to_string(), \"kubernetes\".to_string());\n        labels.insert(\"created-at\".to_string(), Utc::now().to_rfc3339());\n\n        // Create pod specification\n        let mut pod_spec = k8s_openapi::api::core::v1::PodSpec::default();\n        pod_spec.containers = vec![self.create_container_spec(spec, \u0026labels).await?];\n        pod_spec.service_account_name = self.config.service_account.clone().unwrap_or_default();\n        pod_spec.node_selector = self.config.node_selector.clone();\n        pod_spec.tolerations = self.config.tolerations.clone();\n        pod_spec.termination_grace_period_seconds = self.config.termination_grace_period_seconds;\n\n        if let Some(pdb) = self.config.pod_disruption_budget {\n            pod_spec.min_ready_seconds = Some(5);\n        }\n\n        // Configure security context\n        if let Some(security_ctx) = \u0026self.config.security_context {\n            pod_spec.security_context = Some(k8s_openapi::api::core::v1::PodSecurityContext {\n                run_as_user: security_ctx.user,\n                se_linux_options: security_ctx.se_linux_options.clone().map(|opts| k8s_openapi::api::core::v1::SELinuxOptions {\n                    user: opts.get(\"user\").cloned(),\n                    role: opts.get(\"role\").cloned(),\n                    type_: opts.get(\"type\").cloned(),\n                    level: opts.get(\"level\").cloned(),\n                }),\n                ..Default::default()\n            });\n        }\n\n        let mut pod = k8s_openapi::api::core::v1::Pod {\n            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {\n                name: Some(format!(\"worker-{}\", worker_id)),\n                namespace: Some(self.config.namespace.clone()),\n                labels: Some(labels),\n                annotations: Some(HashMap::from([\n                    (\"created-by\".to_string(), \"worker-manager\".to_string()),\n                    (\"runtime-spec-hash\".to_string(), format!(\"{:?}\", std::collections::hash_map::DefaultHasher::new().finish())),\n                ])),\n                ..Default::default()\n            },\n            spec: Some(pod_spec),\n            status: None,\n        };\n\n        // Add volumes for secrets and configmaps\n        let volumes = self.create_volumes(spec).await?;\n        if !volumes.is_empty() {\n            pod.spec.as_mut().unwrap().volumes = Some(volumes);\n        }\n\n        // Add resource limits\n        self.apply_resource_limits(\u0026mut pod, \u0026spec.resources)?;\n\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        // Create the pod\n        match api.create(\u0026kube::PostParams::default(), \u0026pod).await {\n            Ok(created_pod) =\u003e {\n                let worker = Worker {\n                    id: worker_id.clone(),\n                    state: WorkerState::Creating,\n                    runtime_spec: spec.clone(),\n                    provider_config: config.clone(),\n                    metadata: HashMap::new(),\n                    created_at: Utc::now(),\n                    last_update: Utc::now(),\n                };\n\n                // Track the worker\n                {\n                    let mut workers = self.workers.write().await;\n                    workers.insert(worker_id.clone(), KubernetesWorker {\n                        id: worker_id,\n                        pod_name: format!(\"worker-{}\", worker_id),\n                        node_name: None,\n                        created_at: Utc::now(),\n                        last_heartbeat: Utc::now(),\n                    });\n                }\n\n                Ok(worker)\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to create Kubernetes pod: {}\", e),\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn terminate_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.delete(\u0026pod_name, \u0026kube::DeleteParams::default()).await {\n            Ok(_) =\u003e {\n                // Remove from tracking\n                {\n                    let mut workers = self.workers.write().await;\n                    workers.remove(worker_id);\n                }\n                Ok(())\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to terminate Kubernetes pod {}: {}\", pod_name, e),\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn get_worker_status(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cWorkerState, ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.get(\u0026pod_name).await {\n            Ok(pod) =\u003e {\n                // Update heartbeat\n                {\n                    let mut workers = self.workers.write().await;\n                    if let Some(worker) = workers.get_mut(worker_id) {\n                        worker.last_heartbeat = Utc::now();\n                    }\n                }\n\n                match pod.status {\n                    Some(status) =\u003e {\n                        match status.phase {\n                            Some(k8s_openapi::api::core::v1::PodPhase::Running) =\u003e Ok(WorkerState::Running),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Pending) =\u003e Ok(WorkerState::Creating),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Failed) =\u003e Ok(WorkerState::Failed {\n                                reason: status.message.clone().unwrap_or_default()\n                            }),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Succeeded) =\u003e Ok(WorkerState::Terminated),\n                            Some(k8s_openapi::api::core::v1::PodPhase::Unknown) =\u003e Ok(WorkerState::Unknown),\n                            _ =\u003e Ok(WorkerState::Unknown),\n                        }\n                    }\n                    None =\u003e Ok(WorkerState::Creating),\n                }\n            }\n            Err(e) =\u003e {\n                if e.is_not_found() {\n                    Ok(WorkerState::Terminated)\n                } else {\n                    Err(ProviderError::infrastructure(\n                        format!(\"Failed to get Kubernetes pod status: {}\", e),\n                        \"kubernetes\".to_string()\n                    ))\n                }\n            }\n        }\n    }\n\n    async fn get_logs(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003cLogStreamRef, ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.get(\u0026pod_name).await {\n            Ok(pod) =\u003e {\n                // Note: This is a simplified log reference - in practice, you would\n                // typically use a log aggregation system or the Kubernetes log API\n                let log_ref = LogStreamRef {\n                    stream_id: Uuid::new_v4(),\n                    worker_id: worker_id.clone(),\n                    format: LogFormat::Json,\n                    endpoints: vec![\n                        format!(\"https://{}/api/v1/namespaces/{}/pods/{}/log?container={}\", \n                               \"kubernetes-api\", self.config.namespace, pod_name),\n                    ],\n                };\n                Ok(log_ref)\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::not_found(\n                    format!(\"Pod not found: {}\", e),\n                    \"pod\".to_string(),\n                    pod_name,\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn port_forward(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        local_port: u16,\n        remote_port: u16,\n    ) -\u003e Result\u003cString, ProviderError\u003e {\n        // Kubernetes port forwarding implementation\n        // This would typically involve using kubectl port-forward or a similar mechanism\n        // For now, return a placeholder endpoint\n        let endpoint = format!(\"localhost:{}\", local_port);\n        Ok(endpoint)\n    }\n\n    async fn get_capacity(\u0026self) -\u003e Result\u003cCapacityInfo, ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Node\u003e = kube::Api::all(self.client.clone());\n        \n        match api.list(\u0026kube::ListParams::default()).await {\n            Ok(nodes) =\u003e {\n                let mut total_cpu = 0u64;\n                let mut total_memory = 0u64;\n                let mut used_cpu = 0u64;\n                let mut used_memory = 0u64;\n                let mut active_workers = 0u32;\n\n                for node in nodes.items {\n                    if let Some(status) = \u0026node.status {\n                        if let Some(capacity) = \u0026status.capacity {\n                            if let Some(cpu) = capacity.get(\"cpu\").and_then(|c| c.as_str()) {\n                                total_cpu += cpu.parse::\u003cu64\u003e().unwrap_or(0) * 1000; // Convert to milli-cores\n                            }\n                            if let Some(memory) = capacity.get(\"memory\").and_then(|m| m.as_str()) {\n                                total_memory += memory.parse::\u003cu64\u003e().unwrap_or(0) / 1024 / 1024; // Convert to MB\n                            }\n                        }\n                        \n                        if let Some(allocatable) = \u0026status.allocatable {\n                            if let Some(cpu) = allocatable.get(\"cpu\").and_then(|c| c.as_str()) {\n                                used_cpu += cpu.parse::\u003cu64\u003e().unwrap_or(0) * 1000;\n                            }\n                            if let Some(memory) = allocatable.get(\"memory\").and_then(|m| m.as_str()) {\n                                used_memory += memory.parse::\u003cu64\u003e().unwrap_or(0) / 1024 / 1024;\n                            }\n                        }\n                    }\n                }\n\n                // Count active workers\n                {\n                    let workers = self.workers.read().await;\n                    active_workers = workers.len() as u32;\n                }\n\n                Ok(CapacityInfo {\n                    total_resources: ResourceQuota {\n                        cpu_m: total_cpu,\n                        memory_mb: total_memory,\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    used_resources: ResourceQuota {\n                        cpu_m: used_cpu,\n                        memory_mb: used_memory,\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    available_resources: ResourceQuota {\n                        cpu_m: total_cpu.saturating_sub(used_cpu),\n                        memory_mb: total_memory.saturating_sub(used_memory),\n                        gpu: None,\n                        storage_mb: None,\n                    },\n                    active_workers,\n                    last_updated: Utc::now(),\n                })\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::infrastructure(\n                    format!(\"Failed to get Kubernetes capacity: {}\", e),\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn execute_command(\n        \u0026self,\n        worker_id: \u0026WorkerId,\n        command: Vec\u003cString\u003e,\n        timeout: Option\u003cstd::time::Duration\u003e,\n    ) -\u003e Result\u003cExecutionResult, ProviderError\u003e {\n        // Kubernetes command execution via exec API\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        let timeout_duration = timeout.unwrap_or(std::time::Duration::from_secs(30));\n        \n        let start_time = std::time::Instant::now();\n        \n        match api.exec(\u0026pod_name, \u0026command, \u0026kube::ExecParams::default()).await {\n            Ok(output) =\u003e {\n                let duration = start_time.elapsed();\n                let finished_at = Utc::now();\n                let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n                \n                Ok(ExecutionResult {\n                    exit_code: 0,\n                    stdout: output.stdout,\n                    stderr: output.stderr,\n                    duration,\n                    started_at,\n                    finished_at,\n                })\n            }\n            Err(e) =\u003e {\n                let duration = start_time.elapsed();\n                let finished_at = Utc::now();\n                let started_at = finished_at - chrono::Duration::from_std(duration).unwrap_or(chrono::Duration::seconds(0));\n                \n                Ok(ExecutionResult {\n                    exit_code: -1,\n                    stdout: String::new(),\n                    stderr: format!(\"Command execution failed: {}\", e),\n                    duration,\n                    started_at,\n                    finished_at,\n                })\n            }\n        }\n    }\n\n    async fn restart_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        match api.get(\u0026pod_name).await {\n            Ok(mut pod) =\u003e {\n                // Update the pod's generation to force a restart\n                if let Some(metadata) = \u0026mut pod.metadata {\n                    if let Some(annotations) = \u0026mut metadata.annotations {\n                        annotations.insert(\n                            \"restart-timestamp\".to_string(),\n                            Utc::now().to_rfc3339(),\n                        );\n                    }\n                }\n                \n                match api.patch(\u0026pod_name, \u0026kube::PatchParams::default(), \n                             \u0026kube::Patch::\u003ckube::json::Json\u003e::Merge(pod)).await {\n                    Ok(_) =\u003e Ok(()),\n                    Err(e) =\u003e Err(ProviderError::infrastructure(\n                        format!(\"Failed to restart Kubernetes pod: {}\", e),\n                        \"kubernetes\".to_string()\n                    ))\n                }\n            }\n            Err(e) =\u003e {\n                Err(ProviderError::not_found(\n                    format!(\"Pod not found for restart: {}\", e),\n                    \"pod\".to_string(),\n                    pod_name,\n                    \"kubernetes\".to_string()\n                ))\n            }\n        }\n    }\n\n    async fn pause_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        // Implement pause by updating pod annotations\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        let patch = serde_json::json!({\n            \"metadata\": {\n                \"annotations\": {\n                    \"worker-state\": \"paused\",\n                    \"paused-at\": Utc::now().to_rfc3339()\n                }\n            }\n        });\n        \n        match api.patch(\u0026pod_name, \u0026kube::PatchParams::default(), \n                      \u0026kube::Patch::\u003ckube::json::Json\u003e::Merge(patch)).await {\n            Ok(_) =\u003e Ok(()),\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to pause Kubernetes pod: {}\", e),\n                \"kubernetes\".to_string()\n            ))\n        }\n    }\n\n    async fn resume_worker(\u0026self, worker_id: \u0026WorkerId) -\u003e Result\u003c(), ProviderError\u003e {\n        // Remove pause annotations\n        let api: kube::Api\u003ck8s_openapi::api::core::v1::Pod\u003e = kube::Api::namespaced(self.client.clone(), \u0026self.config.namespace);\n        \n        let pod_name = format!(\"worker-{}\", worker_id);\n        \n        let patch = serde_json::json!({\n            \"metadata\": {\n                \"annotations\": {\n                    \"worker-state\": \"running\"\n                }\n            }\n        });\n        \n        match api.patch(\u0026pod_name, \u0026kube::PatchParams::default(), \n                      \u0026kube::Patch::\u003ckube::json::Json\u003e::Merge(patch)).await {\n            Ok(_) =\u003e Ok(()),\n            Err(e) =\u003e Err(ProviderError::infrastructure(\n                format!(\"Failed to resume Kubernetes pod: {}\", e),\n                \"kubernetes\".to_string()\n            ))\n        }\n    }\n\n    fn stream_worker_events(\u0026self) -\u003e tokio_stream::wrappers::IntervalStream {\n        let interval = tokio::time::interval(std::time::Duration::from_secs(30));\n        tokio_stream::wrappers::IntervalStream::new(interval)\n    }\n}\n\nimpl KubernetesProvider {\n    /// Create container specification from runtime spec\n    async fn create_container_spec(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n        labels: \u0026std::collections::HashMap\u003cString, String\u003e,\n    ) -\u003e Result\u003ck8s_openapi::api::core::v1::Container, ProviderError\u003e {\n        let mut container = k8s_openapi::api::core::v1::Container {\n            name: \"worker\".to_string(),\n            image: Some(spec.image.clone()),\n            command: spec.command.clone(),\n            args: None,\n            working_dir: None,\n            ports: Some(spec.ports.iter().map(|port| k8s_openapi::api::core::v1::ContainerPort {\n                container_port: *port as i32,\n                protocol: Some(k8s_openapi::api::core::v1::Protocol::TCP),\n                ..Default::default()\n            }).collect()),\n            env_from: None,\n            env: Some(spec.env.iter().map(|(k, v)| k8s_openapi::api::core::v1::EnvVar {\n                name: k.clone(),\n                value: Some(v.clone()),\n                value_from: None,\n            }).collect()),\n            resources: Some(k8s_openapi::api::core::v1::ResourceRequirements {\n                requests: Some(std::collections::HashMap::from([\n                    (\"cpu\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity(spec.resources.cpu_m.to_string() + \"m\")),\n                    (\"memory\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity((spec.resources.memory_mb * 1024 * 1024).to_string())),\n                ])),\n                limits: Some(std::collections::HashMap::from([\n                    (\"cpu\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity(spec.resources.cpu_m.to_string() + \"m\")),\n                    (\"memory\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity((spec.resources.memory_mb * 1024 * 1024).to_string())),\n                ])),\n                ..Default::default()\n            }),\n            volume_mounts: Some(Vec::new()), // Will be filled by create_volumes\n            liveness_probe: None,\n            readiness_probe: None,\n            startup_probe: None,\n            ..Default::default()\n        };\n\n        // Add health checks\n        let health_check = k8s_openapi::api::core::v1::Probe {\n            http_get: Some(k8s_openapi::api::core::v1::HTTPGetAction {\n                path: Some(\"/health\".to_string()),\n                port: 8080.try_into().unwrap(),\n                host: None,\n                scheme: Some(k8s_openapi::api::core::v1::URIScheme::HTTP),\n            }),\n            tcp_socket: None,\n            exec: None,\n            period_seconds: Some(30),\n            timeout_seconds: Some(5),\n            success_threshold: Some(1),\n            failure_threshold: Some(3),\n            termination_grace_period_seconds: None,\n        };\n\n        container.liveness_probe = Some(health_check.clone());\n        container.readiness_probe = Some(health_check);\n\n        Ok(container)\n    }\n\n    /// Create volumes for secrets and configmaps\n    async fn create_volumes(\n        \u0026self,\n        spec: \u0026RuntimeSpec,\n    ) -\u003e Result\u003cVec\u003ck8s_openapi::api::core::v1::Volume\u003e, ProviderError\u003e {\n        let mut volumes = Vec::new();\n\n        for volume_mount in \u0026spec.volumes {\n            let volume = match \u0026volume_mount.source {\n                VolumeSource::Secret(secret_name) =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: format!(\"secret-{}\", secret_name),\n                        config_map: Some(k8s_openapi::api::core::v1::ConfigMapVolumeSource {\n                            default_mode: Some(0o444),\n                            items: None,\n                            name: Some(secret_name.clone()),\n                            optional: None,\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::ConfigMap(config_name) =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: format!(\"config-{}\", config_name),\n                        config_map: Some(k8s_openapi::api::core::v1::ConfigMapVolumeSource {\n                            default_mode: Some(0o444),\n                            items: None,\n                            name: Some(config_name.clone()),\n                            optional: None,\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::EmptyDir =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: \"empty-dir\".to_string(),\n                        empty_dir: Some(k8s_openapi::api::core::v1::EmptyDirVolumeSource {\n                            medium: Some(k8s_openapi::api::core::v1::StorageMedium::Memory),\n                            size_limit: None,\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::HostPath(path) =\u003e {\n                    k8s_openapi::api::core::v1::Volume {\n                        name: format!(\"host-path-{}\", path.replace(\"/\", \"-\")),\n                        host_path: Some(k8s_openapi::api::core::v1::HostPathVolumeSource {\n                            path: path.clone(),\n                            type_: Some(k8s_openapi::api::core::v1::HostPathType::DirectoryOrCreate),\n                        }),\n                        ..Default::default()\n                    }\n                }\n                VolumeSource::PersistentVolume(_) =\u003e {\n                    // For persistent volumes, you would typically reference a PVC\n                    k8s_openapi::api::core::v1::Volume {\n                        name: \"persistent-volume\".to_string(),\n                        persistent_volume_claim: Some(k8s_openapi::api::core::v1::PersistentVolumeClaimVolumeSource {\n                            claim_name: \"placeholder-pvc\".to_string(),\n                            read_only: Some(volume_mount.read_only),\n                        }),\n                        ..Default::default()\n                    }\n                }\n            };\n\n            volumes.push(volume);\n        }\n\n        Ok(volumes)\n    }\n\n    /// Apply resource limits to pod\n    fn apply_resource_limits(\n        \u0026self,\n        pod: \u0026mut k8s_openapi::api::core::v1::Pod,\n        resources: \u0026ResourceQuota,\n    ) -\u003e Result\u003c(), ProviderError\u003e {\n        if let Some(spec) = pod.spec.as_mut() {\n            if let Some(container) = spec.containers.first_mut() {\n                if let Some(resources) = \u0026mut container.resources {\n                    resources.limits = Some(std::collections::HashMap::from([\n                        (\"cpu\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity(resources.cpu_m.to_string() + \"m\")),\n                        (\"memory\".to_string(), k8s_openapi::apimachinery::pkg::api::resource::Quantity((resources.memory_mb * 1024 * 1024).to_string())),\n                    ]));\n                    \n                    if let Some(storage_mb) = resources.storage_mb {\n                        resources.limits.as_mut().unwrap().insert(\n                            \"ephemeral-storage\".to_string(),\n                            k8s_openapi::apimachinery::pkg::api::resource::Quantity((storage_mb * 1024 * 1024).to_string()),\n                        );\n                    }\n\n                    if let Some(gpu) = resources.gpu {\n                        resources.limits.as_mut().unwrap().insert(\n                            \"nvidia.com/gpu\".to_string(),\n                            k8s_openapi::apimachinery::pkg::api::resource::Quantity(gpu.to_string()),\n                        );\n                    }\n                }\n            }\n        }\n\n        Ok(())\n    }\n}\n","traces":[],"covered":0,"coverable":0}]};
    </script>
    <script crossorigin>/** @license React v16.13.1
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
'use strict';(function(d,r){"object"===typeof exports&&"undefined"!==typeof module?r(exports):"function"===typeof define&&define.amd?define(["exports"],r):(d=d||self,r(d.React={}))})(this,function(d){function r(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function w(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function da(){}function L(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function ea(a,b,c){var g,e={},fa=null,d=null;if(null!=b)for(g in void 0!==b.ref&&(d=b.ref),void 0!==b.key&&(fa=""+b.key),b)ha.call(b,g)&&!ia.hasOwnProperty(g)&&(e[g]=b[g]);var h=arguments.length-2;if(1===h)e.children=c;else if(1<h){for(var k=Array(h),f=0;f<h;f++)k[f]=arguments[f+2];e.children=k}if(a&&a.defaultProps)for(g in h=a.defaultProps,
h)void 0===e[g]&&(e[g]=h[g]);return{$$typeof:x,type:a,key:fa,ref:d,props:e,_owner:M.current}}function va(a,b){return{$$typeof:x,type:a.type,key:b,ref:a.ref,props:a.props,_owner:a._owner}}function N(a){return"object"===typeof a&&null!==a&&a.$$typeof===x}function wa(a){var b={"=":"=0",":":"=2"};return"$"+(""+a).replace(/[=:]/g,function(a){return b[a]})}function ja(a,b,c,g){if(C.length){var e=C.pop();e.result=a;e.keyPrefix=b;e.func=c;e.context=g;e.count=0;return e}return{result:a,keyPrefix:b,func:c,
context:g,count:0}}function ka(a){a.result=null;a.keyPrefix=null;a.func=null;a.context=null;a.count=0;10>C.length&&C.push(a)}function O(a,b,c,g){var e=typeof a;if("undefined"===e||"boolean"===e)a=null;var d=!1;if(null===a)d=!0;else switch(e){case "string":case "number":d=!0;break;case "object":switch(a.$$typeof){case x:case xa:d=!0}}if(d)return c(g,a,""===b?"."+P(a,0):b),1;d=0;b=""===b?".":b+":";if(Array.isArray(a))for(var f=0;f<a.length;f++){e=a[f];var h=b+P(e,f);d+=O(e,h,c,g)}else if(null===a||
"object"!==typeof a?h=null:(h=la&&a[la]||a["@@iterator"],h="function"===typeof h?h:null),"function"===typeof h)for(a=h.call(a),f=0;!(e=a.next()).done;)e=e.value,h=b+P(e,f++),d+=O(e,h,c,g);else if("object"===e)throw c=""+a,Error(r(31,"[object Object]"===c?"object with keys {"+Object.keys(a).join(", ")+"}":c,""));return d}function Q(a,b,c){return null==a?0:O(a,"",b,c)}function P(a,b){return"object"===typeof a&&null!==a&&null!=a.key?wa(a.key):b.toString(36)}function ya(a,b,c){a.func.call(a.context,b,
a.count++)}function za(a,b,c){var g=a.result,e=a.keyPrefix;a=a.func.call(a.context,b,a.count++);Array.isArray(a)?R(a,g,c,function(a){return a}):null!=a&&(N(a)&&(a=va(a,e+(!a.key||b&&b.key===a.key?"":(""+a.key).replace(ma,"$&/")+"/")+c)),g.push(a))}function R(a,b,c,g,e){var d="";null!=c&&(d=(""+c).replace(ma,"$&/")+"/");b=ja(b,d,g,e);Q(a,za,b);ka(b)}function t(){var a=na.current;if(null===a)throw Error(r(321));return a}function S(a,b){var c=a.length;a.push(b);a:for(;;){var g=c-1>>>1,e=a[g];if(void 0!==
e&&0<D(e,b))a[g]=b,a[c]=e,c=g;else break a}}function n(a){a=a[0];return void 0===a?null:a}function E(a){var b=a[0];if(void 0!==b){var c=a.pop();if(c!==b){a[0]=c;a:for(var g=0,e=a.length;g<e;){var d=2*(g+1)-1,f=a[d],h=d+1,k=a[h];if(void 0!==f&&0>D(f,c))void 0!==k&&0>D(k,f)?(a[g]=k,a[h]=c,g=h):(a[g]=f,a[d]=c,g=d);else if(void 0!==k&&0>D(k,c))a[g]=k,a[h]=c,g=h;else break a}}return b}return null}function D(a,b){var c=a.sortIndex-b.sortIndex;return 0!==c?c:a.id-b.id}function F(a){for(var b=n(u);null!==
b;){if(null===b.callback)E(u);else if(b.startTime<=a)E(u),b.sortIndex=b.expirationTime,S(p,b);else break;b=n(u)}}function T(a){y=!1;F(a);if(!v)if(null!==n(p))v=!0,z(U);else{var b=n(u);null!==b&&G(T,b.startTime-a)}}function U(a,b){v=!1;y&&(y=!1,V());H=!0;var c=m;try{F(b);for(l=n(p);null!==l&&(!(l.expirationTime>b)||a&&!W());){var g=l.callback;if(null!==g){l.callback=null;m=l.priorityLevel;var e=g(l.expirationTime<=b);b=q();"function"===typeof e?l.callback=e:l===n(p)&&E(p);F(b)}else E(p);l=n(p)}if(null!==
l)var d=!0;else{var f=n(u);null!==f&&G(T,f.startTime-b);d=!1}return d}finally{l=null,m=c,H=!1}}function oa(a){switch(a){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1E4;default:return 5E3}}var f="function"===typeof Symbol&&Symbol.for,x=f?Symbol.for("react.element"):60103,xa=f?Symbol.for("react.portal"):60106,Aa=f?Symbol.for("react.fragment"):60107,Ba=f?Symbol.for("react.strict_mode"):60108,Ca=f?Symbol.for("react.profiler"):60114,Da=f?Symbol.for("react.provider"):60109,
Ea=f?Symbol.for("react.context"):60110,Fa=f?Symbol.for("react.forward_ref"):60112,Ga=f?Symbol.for("react.suspense"):60113,Ha=f?Symbol.for("react.memo"):60115,Ia=f?Symbol.for("react.lazy"):60116,la="function"===typeof Symbol&&Symbol.iterator,pa=Object.getOwnPropertySymbols,Ja=Object.prototype.hasOwnProperty,Ka=Object.prototype.propertyIsEnumerable,I=function(){try{if(!Object.assign)return!1;var a=new String("abc");a[5]="de";if("5"===Object.getOwnPropertyNames(a)[0])return!1;var b={};for(a=0;10>a;a++)b["_"+
String.fromCharCode(a)]=a;if("0123456789"!==Object.getOwnPropertyNames(b).map(function(a){return b[a]}).join(""))return!1;var c={};"abcdefghijklmnopqrst".split("").forEach(function(a){c[a]=a});return"abcdefghijklmnopqrst"!==Object.keys(Object.assign({},c)).join("")?!1:!0}catch(g){return!1}}()?Object.assign:function(a,b){if(null===a||void 0===a)throw new TypeError("Object.assign cannot be called with null or undefined");var c=Object(a);for(var g,e=1;e<arguments.length;e++){var d=Object(arguments[e]);
for(var f in d)Ja.call(d,f)&&(c[f]=d[f]);if(pa){g=pa(d);for(var h=0;h<g.length;h++)Ka.call(d,g[h])&&(c[g[h]]=d[g[h]])}}return c},ca={isMounted:function(a){return!1},enqueueForceUpdate:function(a,b,c){},enqueueReplaceState:function(a,b,c,d){},enqueueSetState:function(a,b,c,d){}},ba={};w.prototype.isReactComponent={};w.prototype.setState=function(a,b){if("object"!==typeof a&&"function"!==typeof a&&null!=a)throw Error(r(85));this.updater.enqueueSetState(this,a,b,"setState")};w.prototype.forceUpdate=
function(a){this.updater.enqueueForceUpdate(this,a,"forceUpdate")};da.prototype=w.prototype;f=L.prototype=new da;f.constructor=L;I(f,w.prototype);f.isPureReactComponent=!0;var M={current:null},ha=Object.prototype.hasOwnProperty,ia={key:!0,ref:!0,__self:!0,__source:!0},ma=/\/+/g,C=[],na={current:null},X;if("undefined"===typeof window||"function"!==typeof MessageChannel){var A=null,qa=null,ra=function(){if(null!==A)try{var a=q();A(!0,a);A=null}catch(b){throw setTimeout(ra,0),b;}},La=Date.now();var q=
function(){return Date.now()-La};var z=function(a){null!==A?setTimeout(z,0,a):(A=a,setTimeout(ra,0))};var G=function(a,b){qa=setTimeout(a,b)};var V=function(){clearTimeout(qa)};var W=function(){return!1};f=X=function(){}}else{var Y=window.performance,sa=window.Date,Ma=window.setTimeout,Na=window.clearTimeout;"undefined"!==typeof console&&(f=window.cancelAnimationFrame,"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),
"function"!==typeof f&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"));if("object"===typeof Y&&"function"===typeof Y.now)q=function(){return Y.now()};else{var Oa=sa.now();q=function(){return sa.now()-Oa}}var J=!1,K=null,Z=-1,ta=5,ua=0;W=function(){return q()>=ua};f=function(){};X=function(a){0>a||125<a?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):
ta=0<a?Math.floor(1E3/a):5};var B=new MessageChannel,aa=B.port2;B.port1.onmessage=function(){if(null!==K){var a=q();ua=a+ta;try{K(!0,a)?aa.postMessage(null):(J=!1,K=null)}catch(b){throw aa.postMessage(null),b;}}else J=!1};z=function(a){K=a;J||(J=!0,aa.postMessage(null))};G=function(a,b){Z=Ma(function(){a(q())},b)};V=function(){Na(Z);Z=-1}}var p=[],u=[],Pa=1,l=null,m=3,H=!1,v=!1,y=!1,Qa=0;B={ReactCurrentDispatcher:na,ReactCurrentOwner:M,IsSomeRendererActing:{current:!1},assign:I};I(B,{Scheduler:{__proto__:null,
unstable_ImmediatePriority:1,unstable_UserBlockingPriority:2,unstable_NormalPriority:3,unstable_IdlePriority:5,unstable_LowPriority:4,unstable_runWithPriority:function(a,b){switch(a){case 1:case 2:case 3:case 4:case 5:break;default:a=3}var c=m;m=a;try{return b()}finally{m=c}},unstable_next:function(a){switch(m){case 1:case 2:case 3:var b=3;break;default:b=m}var c=m;m=b;try{return a()}finally{m=c}},unstable_scheduleCallback:function(a,b,c){var d=q();if("object"===typeof c&&null!==c){var e=c.delay;
e="number"===typeof e&&0<e?d+e:d;c="number"===typeof c.timeout?c.timeout:oa(a)}else c=oa(a),e=d;c=e+c;a={id:Pa++,callback:b,priorityLevel:a,startTime:e,expirationTime:c,sortIndex:-1};e>d?(a.sortIndex=e,S(u,a),null===n(p)&&a===n(u)&&(y?V():y=!0,G(T,e-d))):(a.sortIndex=c,S(p,a),v||H||(v=!0,z(U)));return a},unstable_cancelCallback:function(a){a.callback=null},unstable_wrapCallback:function(a){var b=m;return function(){var c=m;m=b;try{return a.apply(this,arguments)}finally{m=c}}},unstable_getCurrentPriorityLevel:function(){return m},
unstable_shouldYield:function(){var a=q();F(a);var b=n(p);return b!==l&&null!==l&&null!==b&&null!==b.callback&&b.startTime<=a&&b.expirationTime<l.expirationTime||W()},unstable_requestPaint:f,unstable_continueExecution:function(){v||H||(v=!0,z(U))},unstable_pauseExecution:function(){},unstable_getFirstCallbackNode:function(){return n(p)},get unstable_now(){return q},get unstable_forceFrameRate(){return X},unstable_Profiling:null},SchedulerTracing:{__proto__:null,__interactionsRef:null,__subscriberRef:null,
unstable_clear:function(a){return a()},unstable_getCurrent:function(){return null},unstable_getThreadID:function(){return++Qa},unstable_trace:function(a,b,c){return c()},unstable_wrap:function(a){return a},unstable_subscribe:function(a){},unstable_unsubscribe:function(a){}}});d.Children={map:function(a,b,c){if(null==a)return a;var d=[];R(a,d,null,b,c);return d},forEach:function(a,b,c){if(null==a)return a;b=ja(null,null,b,c);Q(a,ya,b);ka(b)},count:function(a){return Q(a,function(){return null},null)},
toArray:function(a){var b=[];R(a,b,null,function(a){return a});return b},only:function(a){if(!N(a))throw Error(r(143));return a}};d.Component=w;d.Fragment=Aa;d.Profiler=Ca;d.PureComponent=L;d.StrictMode=Ba;d.Suspense=Ga;d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=B;d.cloneElement=function(a,b,c){if(null===a||void 0===a)throw Error(r(267,a));var d=I({},a.props),e=a.key,f=a.ref,m=a._owner;if(null!=b){void 0!==b.ref&&(f=b.ref,m=M.current);void 0!==b.key&&(e=""+b.key);if(a.type&&a.type.defaultProps)var h=
a.type.defaultProps;for(k in b)ha.call(b,k)&&!ia.hasOwnProperty(k)&&(d[k]=void 0===b[k]&&void 0!==h?h[k]:b[k])}var k=arguments.length-2;if(1===k)d.children=c;else if(1<k){h=Array(k);for(var l=0;l<k;l++)h[l]=arguments[l+2];d.children=h}return{$$typeof:x,type:a.type,key:e,ref:f,props:d,_owner:m}};d.createContext=function(a,b){void 0===b&&(b=null);a={$$typeof:Ea,_calculateChangedBits:b,_currentValue:a,_currentValue2:a,_threadCount:0,Provider:null,Consumer:null};a.Provider={$$typeof:Da,_context:a};return a.Consumer=
a};d.createElement=ea;d.createFactory=function(a){var b=ea.bind(null,a);b.type=a;return b};d.createRef=function(){return{current:null}};d.forwardRef=function(a){return{$$typeof:Fa,render:a}};d.isValidElement=N;d.lazy=function(a){return{$$typeof:Ia,_ctor:a,_status:-1,_result:null}};d.memo=function(a,b){return{$$typeof:Ha,type:a,compare:void 0===b?null:b}};d.useCallback=function(a,b){return t().useCallback(a,b)};d.useContext=function(a,b){return t().useContext(a,b)};d.useDebugValue=function(a,b){};
d.useEffect=function(a,b){return t().useEffect(a,b)};d.useImperativeHandle=function(a,b,c){return t().useImperativeHandle(a,b,c)};d.useLayoutEffect=function(a,b){return t().useLayoutEffect(a,b)};d.useMemo=function(a,b){return t().useMemo(a,b)};d.useReducer=function(a,b,c){return t().useReducer(a,b,c)};d.useRef=function(a){return t().useRef(a)};d.useState=function(a){return t().useState(a)};d.version="16.13.1"});
</script>
    <script crossorigin>/** @license React v16.13.1
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
/*
 Modernizr 3.0.0pre (Custom Build) | MIT
*/
'use strict';(function(I,ea){"object"===typeof exports&&"undefined"!==typeof module?ea(exports,require("react")):"function"===typeof define&&define.amd?define(["exports","react"],ea):(I=I||self,ea(I.ReactDOM={},I.React))})(this,function(I,ea){function k(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function ji(a,b,c,d,e,f,g,h,m){yb=!1;gc=null;ki.apply(li,arguments)}function mi(a,b,c,d,e,f,g,h,m){ji.apply(this,arguments);if(yb){if(yb){var n=gc;yb=!1;gc=null}else throw Error(k(198));hc||(hc=!0,pd=n)}}function lf(a,b,c){var d=a.type||"unknown-event";a.currentTarget=mf(c);mi(d,b,void 0,a);a.currentTarget=null}function nf(){if(ic)for(var a in cb){var b=cb[a],c=ic.indexOf(a);if(!(-1<c))throw Error(k(96,a));if(!jc[c]){if(!b.extractEvents)throw Error(k(97,a));jc[c]=b;c=b.eventTypes;for(var d in c){var e=
void 0;var f=c[d],g=b,h=d;if(qd.hasOwnProperty(h))throw Error(k(99,h));qd[h]=f;var m=f.phasedRegistrationNames;if(m){for(e in m)m.hasOwnProperty(e)&&of(m[e],g,h);e=!0}else f.registrationName?(of(f.registrationName,g,h),e=!0):e=!1;if(!e)throw Error(k(98,d,a));}}}}function of(a,b,c){if(db[a])throw Error(k(100,a));db[a]=b;rd[a]=b.eventTypes[c].dependencies}function pf(a){var b=!1,c;for(c in a)if(a.hasOwnProperty(c)){var d=a[c];if(!cb.hasOwnProperty(c)||cb[c]!==d){if(cb[c])throw Error(k(102,c));cb[c]=
d;b=!0}}b&&nf()}function qf(a){if(a=rf(a)){if("function"!==typeof sd)throw Error(k(280));var b=a.stateNode;b&&(b=td(b),sd(a.stateNode,a.type,b))}}function sf(a){eb?fb?fb.push(a):fb=[a]:eb=a}function tf(){if(eb){var a=eb,b=fb;fb=eb=null;qf(a);if(b)for(a=0;a<b.length;a++)qf(b[a])}}function ud(){if(null!==eb||null!==fb)vd(),tf()}function uf(a,b,c){if(wd)return a(b,c);wd=!0;try{return vf(a,b,c)}finally{wd=!1,ud()}}function ni(a){if(wf.call(xf,a))return!0;if(wf.call(yf,a))return!1;if(oi.test(a))return xf[a]=
!0;yf[a]=!0;return!1}function pi(a,b,c,d){if(null!==c&&0===c.type)return!1;switch(typeof b){case "function":case "symbol":return!0;case "boolean":if(d)return!1;if(null!==c)return!c.acceptsBooleans;a=a.toLowerCase().slice(0,5);return"data-"!==a&&"aria-"!==a;default:return!1}}function qi(a,b,c,d){if(null===b||"undefined"===typeof b||pi(a,b,c,d))return!0;if(d)return!1;if(null!==c)switch(c.type){case 3:return!b;case 4:return!1===b;case 5:return isNaN(b);case 6:return isNaN(b)||1>b}return!1}function L(a,
b,c,d,e,f){this.acceptsBooleans=2===b||3===b||4===b;this.attributeName=d;this.attributeNamespace=e;this.mustUseProperty=c;this.propertyName=a;this.type=b;this.sanitizeURL=f}function xd(a,b,c,d){var e=E.hasOwnProperty(b)?E[b]:null;var f=null!==e?0===e.type:d?!1:!(2<b.length)||"o"!==b[0]&&"O"!==b[0]||"n"!==b[1]&&"N"!==b[1]?!1:!0;f||(qi(b,c,e,d)&&(c=null),d||null===e?ni(b)&&(null===c?a.removeAttribute(b):a.setAttribute(b,""+c)):e.mustUseProperty?a[e.propertyName]=null===c?3===e.type?!1:"":c:(b=e.attributeName,
d=e.attributeNamespace,null===c?a.removeAttribute(b):(e=e.type,c=3===e||4===e&&!0===c?"":""+c,d?a.setAttributeNS(d,b,c):a.setAttribute(b,c))))}function zb(a){if(null===a||"object"!==typeof a)return null;a=zf&&a[zf]||a["@@iterator"];return"function"===typeof a?a:null}function ri(a){if(-1===a._status){a._status=0;var b=a._ctor;b=b();a._result=b;b.then(function(b){0===a._status&&(b=b.default,a._status=1,a._result=b)},function(b){0===a._status&&(a._status=2,a._result=b)})}}function na(a){if(null==a)return null;
if("function"===typeof a)return a.displayName||a.name||null;if("string"===typeof a)return a;switch(a){case Ma:return"Fragment";case gb:return"Portal";case kc:return"Profiler";case Af:return"StrictMode";case lc:return"Suspense";case yd:return"SuspenseList"}if("object"===typeof a)switch(a.$$typeof){case Bf:return"Context.Consumer";case Cf:return"Context.Provider";case zd:var b=a.render;b=b.displayName||b.name||"";return a.displayName||(""!==b?"ForwardRef("+b+")":"ForwardRef");case Ad:return na(a.type);
case Df:return na(a.render);case Ef:if(a=1===a._status?a._result:null)return na(a)}return null}function Bd(a){var b="";do{a:switch(a.tag){case 3:case 4:case 6:case 7:case 10:case 9:var c="";break a;default:var d=a._debugOwner,e=a._debugSource,f=na(a.type);c=null;d&&(c=na(d.type));d=f;f="";e?f=" (at "+e.fileName.replace(si,"")+":"+e.lineNumber+")":c&&(f=" (created by "+c+")");c="\n    in "+(d||"Unknown")+f}b+=c;a=a.return}while(a);return b}function va(a){switch(typeof a){case "boolean":case "number":case "object":case "string":case "undefined":return a;
default:return""}}function Ff(a){var b=a.type;return(a=a.nodeName)&&"input"===a.toLowerCase()&&("checkbox"===b||"radio"===b)}function ti(a){var b=Ff(a)?"checked":"value",c=Object.getOwnPropertyDescriptor(a.constructor.prototype,b),d=""+a[b];if(!a.hasOwnProperty(b)&&"undefined"!==typeof c&&"function"===typeof c.get&&"function"===typeof c.set){var e=c.get,f=c.set;Object.defineProperty(a,b,{configurable:!0,get:function(){return e.call(this)},set:function(a){d=""+a;f.call(this,a)}});Object.defineProperty(a,
b,{enumerable:c.enumerable});return{getValue:function(){return d},setValue:function(a){d=""+a},stopTracking:function(){a._valueTracker=null;delete a[b]}}}}function mc(a){a._valueTracker||(a._valueTracker=ti(a))}function Gf(a){if(!a)return!1;var b=a._valueTracker;if(!b)return!0;var c=b.getValue();var d="";a&&(d=Ff(a)?a.checked?"true":"false":a.value);a=d;return a!==c?(b.setValue(a),!0):!1}function Cd(a,b){var c=b.checked;return M({},b,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=
c?c:a._wrapperState.initialChecked})}function Hf(a,b){var c=null==b.defaultValue?"":b.defaultValue,d=null!=b.checked?b.checked:b.defaultChecked;c=va(null!=b.value?b.value:c);a._wrapperState={initialChecked:d,initialValue:c,controlled:"checkbox"===b.type||"radio"===b.type?null!=b.checked:null!=b.value}}function If(a,b){b=b.checked;null!=b&&xd(a,"checked",b,!1)}function Dd(a,b){If(a,b);var c=va(b.value),d=b.type;if(null!=c)if("number"===d){if(0===c&&""===a.value||a.value!=c)a.value=""+c}else a.value!==
""+c&&(a.value=""+c);else if("submit"===d||"reset"===d){a.removeAttribute("value");return}b.hasOwnProperty("value")?Ed(a,b.type,c):b.hasOwnProperty("defaultValue")&&Ed(a,b.type,va(b.defaultValue));null==b.checked&&null!=b.defaultChecked&&(a.defaultChecked=!!b.defaultChecked)}function Jf(a,b,c){if(b.hasOwnProperty("value")||b.hasOwnProperty("defaultValue")){var d=b.type;if(!("submit"!==d&&"reset"!==d||void 0!==b.value&&null!==b.value))return;b=""+a._wrapperState.initialValue;c||b===a.value||(a.value=
b);a.defaultValue=b}c=a.name;""!==c&&(a.name="");a.defaultChecked=!!a._wrapperState.initialChecked;""!==c&&(a.name=c)}function Ed(a,b,c){if("number"!==b||a.ownerDocument.activeElement!==a)null==c?a.defaultValue=""+a._wrapperState.initialValue:a.defaultValue!==""+c&&(a.defaultValue=""+c)}function ui(a){var b="";ea.Children.forEach(a,function(a){null!=a&&(b+=a)});return b}function Fd(a,b){a=M({children:void 0},b);if(b=ui(b.children))a.children=b;return a}function hb(a,b,c,d){a=a.options;if(b){b={};
for(var e=0;e<c.length;e++)b["$"+c[e]]=!0;for(c=0;c<a.length;c++)e=b.hasOwnProperty("$"+a[c].value),a[c].selected!==e&&(a[c].selected=e),e&&d&&(a[c].defaultSelected=!0)}else{c=""+va(c);b=null;for(e=0;e<a.length;e++){if(a[e].value===c){a[e].selected=!0;d&&(a[e].defaultSelected=!0);return}null!==b||a[e].disabled||(b=a[e])}null!==b&&(b.selected=!0)}}function Gd(a,b){if(null!=b.dangerouslySetInnerHTML)throw Error(k(91));return M({},b,{value:void 0,defaultValue:void 0,children:""+a._wrapperState.initialValue})}
function Kf(a,b){var c=b.value;if(null==c){c=b.children;b=b.defaultValue;if(null!=c){if(null!=b)throw Error(k(92));if(Array.isArray(c)){if(!(1>=c.length))throw Error(k(93));c=c[0]}b=c}null==b&&(b="");c=b}a._wrapperState={initialValue:va(c)}}function Lf(a,b){var c=va(b.value),d=va(b.defaultValue);null!=c&&(c=""+c,c!==a.value&&(a.value=c),null==b.defaultValue&&a.defaultValue!==c&&(a.defaultValue=c));null!=d&&(a.defaultValue=""+d)}function Mf(a,b){b=a.textContent;b===a._wrapperState.initialValue&&""!==
b&&null!==b&&(a.value=b)}function Nf(a){switch(a){case "svg":return"http://www.w3.org/2000/svg";case "math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Hd(a,b){return null==a||"http://www.w3.org/1999/xhtml"===a?Nf(b):"http://www.w3.org/2000/svg"===a&&"foreignObject"===b?"http://www.w3.org/1999/xhtml":a}function nc(a,b){var c={};c[a.toLowerCase()]=b.toLowerCase();c["Webkit"+a]="webkit"+b;c["Moz"+a]="moz"+b;return c}function oc(a){if(Id[a])return Id[a];
if(!ib[a])return a;var b=ib[a],c;for(c in b)if(b.hasOwnProperty(c)&&c in Of)return Id[a]=b[c];return a}function Jd(a){var b=Pf.get(a);void 0===b&&(b=new Map,Pf.set(a,b));return b}function Na(a){var b=a,c=a;if(a.alternate)for(;b.return;)b=b.return;else{a=b;do b=a,0!==(b.effectTag&1026)&&(c=b.return),a=b.return;while(a)}return 3===b.tag?c:null}function Qf(a){if(13===a.tag){var b=a.memoizedState;null===b&&(a=a.alternate,null!==a&&(b=a.memoizedState));if(null!==b)return b.dehydrated}return null}function Rf(a){if(Na(a)!==
a)throw Error(k(188));}function vi(a){var b=a.alternate;if(!b){b=Na(a);if(null===b)throw Error(k(188));return b!==a?null:a}for(var c=a,d=b;;){var e=c.return;if(null===e)break;var f=e.alternate;if(null===f){d=e.return;if(null!==d){c=d;continue}break}if(e.child===f.child){for(f=e.child;f;){if(f===c)return Rf(e),a;if(f===d)return Rf(e),b;f=f.sibling}throw Error(k(188));}if(c.return!==d.return)c=e,d=f;else{for(var g=!1,h=e.child;h;){if(h===c){g=!0;c=e;d=f;break}if(h===d){g=!0;d=e;c=f;break}h=h.sibling}if(!g){for(h=
f.child;h;){if(h===c){g=!0;c=f;d=e;break}if(h===d){g=!0;d=f;c=e;break}h=h.sibling}if(!g)throw Error(k(189));}}if(c.alternate!==d)throw Error(k(190));}if(3!==c.tag)throw Error(k(188));return c.stateNode.current===c?a:b}function Sf(a){a=vi(a);if(!a)return null;for(var b=a;;){if(5===b.tag||6===b.tag)return b;if(b.child)b.child.return=b,b=b.child;else{if(b===a)break;for(;!b.sibling;){if(!b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}}return null}function jb(a,b){if(null==
b)throw Error(k(30));if(null==a)return b;if(Array.isArray(a)){if(Array.isArray(b))return a.push.apply(a,b),a;a.push(b);return a}return Array.isArray(b)?[a].concat(b):[a,b]}function Kd(a,b,c){Array.isArray(a)?a.forEach(b,c):a&&b.call(c,a)}function pc(a){null!==a&&(Ab=jb(Ab,a));a=Ab;Ab=null;if(a){Kd(a,wi);if(Ab)throw Error(k(95));if(hc)throw a=pd,hc=!1,pd=null,a;}}function Ld(a){a=a.target||a.srcElement||window;a.correspondingUseElement&&(a=a.correspondingUseElement);return 3===a.nodeType?a.parentNode:
a}function Tf(a){if(!wa)return!1;a="on"+a;var b=a in document;b||(b=document.createElement("div"),b.setAttribute(a,"return;"),b="function"===typeof b[a]);return b}function Uf(a){a.topLevelType=null;a.nativeEvent=null;a.targetInst=null;a.ancestors.length=0;10>qc.length&&qc.push(a)}function Vf(a,b,c,d){if(qc.length){var e=qc.pop();e.topLevelType=a;e.eventSystemFlags=d;e.nativeEvent=b;e.targetInst=c;return e}return{topLevelType:a,eventSystemFlags:d,nativeEvent:b,targetInst:c,ancestors:[]}}function Wf(a){var b=
a.targetInst,c=b;do{if(!c){a.ancestors.push(c);break}var d=c;if(3===d.tag)d=d.stateNode.containerInfo;else{for(;d.return;)d=d.return;d=3!==d.tag?null:d.stateNode.containerInfo}if(!d)break;b=c.tag;5!==b&&6!==b||a.ancestors.push(c);c=Bb(d)}while(c);for(c=0;c<a.ancestors.length;c++){b=a.ancestors[c];var e=Ld(a.nativeEvent);d=a.topLevelType;var f=a.nativeEvent,g=a.eventSystemFlags;0===c&&(g|=64);for(var h=null,m=0;m<jc.length;m++){var n=jc[m];n&&(n=n.extractEvents(d,b,f,e,g))&&(h=jb(h,n))}pc(h)}}function Md(a,
b,c){if(!c.has(a)){switch(a){case "scroll":Cb(b,"scroll",!0);break;case "focus":case "blur":Cb(b,"focus",!0);Cb(b,"blur",!0);c.set("blur",null);c.set("focus",null);break;case "cancel":case "close":Tf(a)&&Cb(b,a,!0);break;case "invalid":case "submit":case "reset":break;default:-1===Db.indexOf(a)&&w(a,b)}c.set(a,null)}}function xi(a,b){var c=Jd(b);Nd.forEach(function(a){Md(a,b,c)});yi.forEach(function(a){Md(a,b,c)})}function Od(a,b,c,d,e){return{blockedOn:a,topLevelType:b,eventSystemFlags:c|32,nativeEvent:e,
container:d}}function Xf(a,b){switch(a){case "focus":case "blur":xa=null;break;case "dragenter":case "dragleave":ya=null;break;case "mouseover":case "mouseout":za=null;break;case "pointerover":case "pointerout":Eb.delete(b.pointerId);break;case "gotpointercapture":case "lostpointercapture":Fb.delete(b.pointerId)}}function Gb(a,b,c,d,e,f){if(null===a||a.nativeEvent!==f)return a=Od(b,c,d,e,f),null!==b&&(b=Hb(b),null!==b&&Yf(b)),a;a.eventSystemFlags|=d;return a}function zi(a,b,c,d,e){switch(b){case "focus":return xa=
Gb(xa,a,b,c,d,e),!0;case "dragenter":return ya=Gb(ya,a,b,c,d,e),!0;case "mouseover":return za=Gb(za,a,b,c,d,e),!0;case "pointerover":var f=e.pointerId;Eb.set(f,Gb(Eb.get(f)||null,a,b,c,d,e));return!0;case "gotpointercapture":return f=e.pointerId,Fb.set(f,Gb(Fb.get(f)||null,a,b,c,d,e)),!0}return!1}function Ai(a){var b=Bb(a.target);if(null!==b){var c=Na(b);if(null!==c)if(b=c.tag,13===b){if(b=Qf(c),null!==b){a.blockedOn=b;Pd(a.priority,function(){Bi(c)});return}}else if(3===b&&c.stateNode.hydrate){a.blockedOn=
3===c.tag?c.stateNode.containerInfo:null;return}}a.blockedOn=null}function rc(a){if(null!==a.blockedOn)return!1;var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);if(null!==b){var c=Hb(b);null!==c&&Yf(c);a.blockedOn=b;return!1}return!0}function Zf(a,b,c){rc(a)&&c.delete(b)}function Ci(){for(Rd=!1;0<fa.length;){var a=fa[0];if(null!==a.blockedOn){a=Hb(a.blockedOn);null!==a&&Di(a);break}var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);null!==b?a.blockedOn=b:fa.shift()}null!==
xa&&rc(xa)&&(xa=null);null!==ya&&rc(ya)&&(ya=null);null!==za&&rc(za)&&(za=null);Eb.forEach(Zf);Fb.forEach(Zf)}function Ib(a,b){a.blockedOn===b&&(a.blockedOn=null,Rd||(Rd=!0,$f(ag,Ci)))}function bg(a){if(0<fa.length){Ib(fa[0],a);for(var b=1;b<fa.length;b++){var c=fa[b];c.blockedOn===a&&(c.blockedOn=null)}}null!==xa&&Ib(xa,a);null!==ya&&Ib(ya,a);null!==za&&Ib(za,a);b=function(b){return Ib(b,a)};Eb.forEach(b);Fb.forEach(b);for(b=0;b<Jb.length;b++)c=Jb[b],c.blockedOn===a&&(c.blockedOn=null);for(;0<Jb.length&&
(b=Jb[0],null===b.blockedOn);)Ai(b),null===b.blockedOn&&Jb.shift()}function Sd(a,b){for(var c=0;c<a.length;c+=2){var d=a[c],e=a[c+1],f="on"+(e[0].toUpperCase()+e.slice(1));f={phasedRegistrationNames:{bubbled:f,captured:f+"Capture"},dependencies:[d],eventPriority:b};Td.set(d,b);cg.set(d,f);dg[e]=f}}function w(a,b){Cb(b,a,!1)}function Cb(a,b,c){var d=Td.get(b);switch(void 0===d?2:d){case 0:d=Ei.bind(null,b,1,a);break;case 1:d=Fi.bind(null,b,1,a);break;default:d=sc.bind(null,b,1,a)}c?a.addEventListener(b,
d,!0):a.addEventListener(b,d,!1)}function Ei(a,b,c,d){Oa||vd();var e=sc,f=Oa;Oa=!0;try{eg(e,a,b,c,d)}finally{(Oa=f)||ud()}}function Fi(a,b,c,d){Gi(Hi,sc.bind(null,a,b,c,d))}function sc(a,b,c,d){if(tc)if(0<fa.length&&-1<Nd.indexOf(a))a=Od(null,a,b,c,d),fa.push(a);else{var e=Qd(a,b,c,d);if(null===e)Xf(a,d);else if(-1<Nd.indexOf(a))a=Od(e,a,b,c,d),fa.push(a);else if(!zi(e,a,b,c,d)){Xf(a,d);a=Vf(a,d,null,b);try{uf(Wf,a)}finally{Uf(a)}}}}function Qd(a,b,c,d){c=Ld(d);c=Bb(c);if(null!==c){var e=Na(c);if(null===
e)c=null;else{var f=e.tag;if(13===f){c=Qf(e);if(null!==c)return c;c=null}else if(3===f){if(e.stateNode.hydrate)return 3===e.tag?e.stateNode.containerInfo:null;c=null}else e!==c&&(c=null)}}a=Vf(a,d,c,b);try{uf(Wf,a)}finally{Uf(a)}return null}function fg(a,b,c){return null==b||"boolean"===typeof b||""===b?"":c||"number"!==typeof b||0===b||Kb.hasOwnProperty(a)&&Kb[a]?(""+b).trim():b+"px"}function gg(a,b){a=a.style;for(var c in b)if(b.hasOwnProperty(c)){var d=0===c.indexOf("--"),e=fg(c,b[c],d);"float"===
c&&(c="cssFloat");d?a.setProperty(c,e):a[c]=e}}function Ud(a,b){if(b){if(Ii[a]&&(null!=b.children||null!=b.dangerouslySetInnerHTML))throw Error(k(137,a,""));if(null!=b.dangerouslySetInnerHTML){if(null!=b.children)throw Error(k(60));if(!("object"===typeof b.dangerouslySetInnerHTML&&"__html"in b.dangerouslySetInnerHTML))throw Error(k(61));}if(null!=b.style&&"object"!==typeof b.style)throw Error(k(62,""));}}function Vd(a,b){if(-1===a.indexOf("-"))return"string"===typeof b.is;switch(a){case "annotation-xml":case "color-profile":case "font-face":case "font-face-src":case "font-face-uri":case "font-face-format":case "font-face-name":case "missing-glyph":return!1;
default:return!0}}function oa(a,b){a=9===a.nodeType||11===a.nodeType?a:a.ownerDocument;var c=Jd(a);b=rd[b];for(var d=0;d<b.length;d++)Md(b[d],a,c)}function uc(){}function Wd(a){a=a||("undefined"!==typeof document?document:void 0);if("undefined"===typeof a)return null;try{return a.activeElement||a.body}catch(b){return a.body}}function hg(a){for(;a&&a.firstChild;)a=a.firstChild;return a}function ig(a,b){var c=hg(a);a=0;for(var d;c;){if(3===c.nodeType){d=a+c.textContent.length;if(a<=b&&d>=b)return{node:c,
offset:b-a};a=d}a:{for(;c;){if(c.nextSibling){c=c.nextSibling;break a}c=c.parentNode}c=void 0}c=hg(c)}}function jg(a,b){return a&&b?a===b?!0:a&&3===a.nodeType?!1:b&&3===b.nodeType?jg(a,b.parentNode):"contains"in a?a.contains(b):a.compareDocumentPosition?!!(a.compareDocumentPosition(b)&16):!1:!1}function kg(){for(var a=window,b=Wd();b instanceof a.HTMLIFrameElement;){try{var c="string"===typeof b.contentWindow.location.href}catch(d){c=!1}if(c)a=b.contentWindow;else break;b=Wd(a.document)}return b}
function Xd(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return b&&("input"===b&&("text"===a.type||"search"===a.type||"tel"===a.type||"url"===a.type||"password"===a.type)||"textarea"===b||"true"===a.contentEditable)}function lg(a,b){switch(a){case "button":case "input":case "select":case "textarea":return!!b.autoFocus}return!1}function Yd(a,b){return"textarea"===a||"option"===a||"noscript"===a||"string"===typeof b.children||"number"===typeof b.children||"object"===typeof b.dangerouslySetInnerHTML&&
null!==b.dangerouslySetInnerHTML&&null!=b.dangerouslySetInnerHTML.__html}function kb(a){for(;null!=a;a=a.nextSibling){var b=a.nodeType;if(1===b||3===b)break}return a}function mg(a){a=a.previousSibling;for(var b=0;a;){if(8===a.nodeType){var c=a.data;if(c===ng||c===Zd||c===$d){if(0===b)return a;b--}else c===og&&b++}a=a.previousSibling}return null}function Bb(a){var b=a[Aa];if(b)return b;for(var c=a.parentNode;c;){if(b=c[Lb]||c[Aa]){c=b.alternate;if(null!==b.child||null!==c&&null!==c.child)for(a=mg(a);null!==
a;){if(c=a[Aa])return c;a=mg(a)}return b}a=c;c=a.parentNode}return null}function Hb(a){a=a[Aa]||a[Lb];return!a||5!==a.tag&&6!==a.tag&&13!==a.tag&&3!==a.tag?null:a}function Pa(a){if(5===a.tag||6===a.tag)return a.stateNode;throw Error(k(33));}function ae(a){return a[vc]||null}function pa(a){do a=a.return;while(a&&5!==a.tag);return a?a:null}function pg(a,b){var c=a.stateNode;if(!c)return null;var d=td(c);if(!d)return null;c=d[b];a:switch(b){case "onClick":case "onClickCapture":case "onDoubleClick":case "onDoubleClickCapture":case "onMouseDown":case "onMouseDownCapture":case "onMouseMove":case "onMouseMoveCapture":case "onMouseUp":case "onMouseUpCapture":case "onMouseEnter":(d=
!d.disabled)||(a=a.type,d=!("button"===a||"input"===a||"select"===a||"textarea"===a));a=!d;break a;default:a=!1}if(a)return null;if(c&&"function"!==typeof c)throw Error(k(231,b,typeof c));return c}function qg(a,b,c){if(b=pg(a,c.dispatchConfig.phasedRegistrationNames[b]))c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a)}function Ji(a){if(a&&a.dispatchConfig.phasedRegistrationNames){for(var b=a._targetInst,c=[];b;)c.push(b),b=pa(b);for(b=c.length;0<b--;)qg(c[b],
"captured",a);for(b=0;b<c.length;b++)qg(c[b],"bubbled",a)}}function be(a,b,c){a&&c&&c.dispatchConfig.registrationName&&(b=pg(a,c.dispatchConfig.registrationName))&&(c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a))}function Ki(a){a&&a.dispatchConfig.registrationName&&be(a._targetInst,null,a)}function lb(a){Kd(a,Ji)}function rg(){if(wc)return wc;var a,b=ce,c=b.length,d,e="value"in Ba?Ba.value:Ba.textContent,f=e.length;for(a=0;a<c&&b[a]===e[a];a++);var g=
c-a;for(d=1;d<=g&&b[c-d]===e[f-d];d++);return wc=e.slice(a,1<d?1-d:void 0)}function xc(){return!0}function yc(){return!1}function R(a,b,c,d){this.dispatchConfig=a;this._targetInst=b;this.nativeEvent=c;a=this.constructor.Interface;for(var e in a)a.hasOwnProperty(e)&&((b=a[e])?this[e]=b(c):"target"===e?this.target=d:this[e]=c[e]);this.isDefaultPrevented=(null!=c.defaultPrevented?c.defaultPrevented:!1===c.returnValue)?xc:yc;this.isPropagationStopped=yc;return this}function Li(a,b,c,d){if(this.eventPool.length){var e=
this.eventPool.pop();this.call(e,a,b,c,d);return e}return new this(a,b,c,d)}function Mi(a){if(!(a instanceof this))throw Error(k(279));a.destructor();10>this.eventPool.length&&this.eventPool.push(a)}function sg(a){a.eventPool=[];a.getPooled=Li;a.release=Mi}function tg(a,b){switch(a){case "keyup":return-1!==Ni.indexOf(b.keyCode);case "keydown":return 229!==b.keyCode;case "keypress":case "mousedown":case "blur":return!0;default:return!1}}function ug(a){a=a.detail;return"object"===typeof a&&"data"in
a?a.data:null}function Oi(a,b){switch(a){case "compositionend":return ug(b);case "keypress":if(32!==b.which)return null;vg=!0;return wg;case "textInput":return a=b.data,a===wg&&vg?null:a;default:return null}}function Pi(a,b){if(mb)return"compositionend"===a||!de&&tg(a,b)?(a=rg(),wc=ce=Ba=null,mb=!1,a):null;switch(a){case "paste":return null;case "keypress":if(!(b.ctrlKey||b.altKey||b.metaKey)||b.ctrlKey&&b.altKey){if(b.char&&1<b.char.length)return b.char;if(b.which)return String.fromCharCode(b.which)}return null;
case "compositionend":return xg&&"ko"!==b.locale?null:b.data;default:return null}}function yg(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return"input"===b?!!Qi[a.type]:"textarea"===b?!0:!1}function zg(a,b,c){a=R.getPooled(Ag.change,a,b,c);a.type="change";sf(c);lb(a);return a}function Ri(a){pc(a)}function zc(a){var b=Pa(a);if(Gf(b))return a}function Si(a,b){if("change"===a)return b}function Bg(){Mb&&(Mb.detachEvent("onpropertychange",Cg),Nb=Mb=null)}function Cg(a){if("value"===a.propertyName&&
zc(Nb))if(a=zg(Nb,a,Ld(a)),Oa)pc(a);else{Oa=!0;try{ee(Ri,a)}finally{Oa=!1,ud()}}}function Ti(a,b,c){"focus"===a?(Bg(),Mb=b,Nb=c,Mb.attachEvent("onpropertychange",Cg)):"blur"===a&&Bg()}function Ui(a,b){if("selectionchange"===a||"keyup"===a||"keydown"===a)return zc(Nb)}function Vi(a,b){if("click"===a)return zc(b)}function Wi(a,b){if("input"===a||"change"===a)return zc(b)}function Xi(a){var b=this.nativeEvent;return b.getModifierState?b.getModifierState(a):(a=Yi[a])?!!b[a]:!1}function fe(a){return Xi}
function Zi(a,b){return a===b&&(0!==a||1/a===1/b)||a!==a&&b!==b}function Ob(a,b){if(Qa(a,b))return!0;if("object"!==typeof a||null===a||"object"!==typeof b||null===b)return!1;var c=Object.keys(a),d=Object.keys(b);if(c.length!==d.length)return!1;for(d=0;d<c.length;d++)if(!$i.call(b,c[d])||!Qa(a[c[d]],b[c[d]]))return!1;return!0}function Dg(a,b){var c=b.window===b?b.document:9===b.nodeType?b:b.ownerDocument;if(ge||null==nb||nb!==Wd(c))return null;c=nb;"selectionStart"in c&&Xd(c)?c={start:c.selectionStart,
end:c.selectionEnd}:(c=(c.ownerDocument&&c.ownerDocument.defaultView||window).getSelection(),c={anchorNode:c.anchorNode,anchorOffset:c.anchorOffset,focusNode:c.focusNode,focusOffset:c.focusOffset});return Pb&&Ob(Pb,c)?null:(Pb=c,a=R.getPooled(Eg.select,he,a,b),a.type="select",a.target=nb,lb(a),a)}function Ac(a){var b=a.keyCode;"charCode"in a?(a=a.charCode,0===a&&13===b&&(a=13)):a=b;10===a&&(a=13);return 32<=a||13===a?a:0}function q(a,b){0>ob||(a.current=ie[ob],ie[ob]=null,ob--)}function y(a,b,c){ob++;
ie[ob]=a.current;a.current=b}function pb(a,b){var c=a.type.contextTypes;if(!c)return Ca;var d=a.stateNode;if(d&&d.__reactInternalMemoizedUnmaskedChildContext===b)return d.__reactInternalMemoizedMaskedChildContext;var e={},f;for(f in c)e[f]=b[f];d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=b,a.__reactInternalMemoizedMaskedChildContext=e);return e}function N(a){a=a.childContextTypes;return null!==a&&void 0!==a}function Fg(a,b,c){if(B.current!==Ca)throw Error(k(168));y(B,b);y(G,c)}
function Gg(a,b,c){var d=a.stateNode;a=b.childContextTypes;if("function"!==typeof d.getChildContext)return c;d=d.getChildContext();for(var e in d)if(!(e in a))throw Error(k(108,na(b)||"Unknown",e));return M({},c,{},d)}function Bc(a){a=(a=a.stateNode)&&a.__reactInternalMemoizedMergedChildContext||Ca;Ra=B.current;y(B,a);y(G,G.current);return!0}function Hg(a,b,c){var d=a.stateNode;if(!d)throw Error(k(169));c?(a=Gg(a,b,Ra),d.__reactInternalMemoizedMergedChildContext=a,q(G),q(B),y(B,a)):q(G);y(G,c)}function Cc(){switch(aj()){case Dc:return 99;
case Ig:return 98;case Jg:return 97;case Kg:return 96;case Lg:return 95;default:throw Error(k(332));}}function Mg(a){switch(a){case 99:return Dc;case 98:return Ig;case 97:return Jg;case 96:return Kg;case 95:return Lg;default:throw Error(k(332));}}function Da(a,b){a=Mg(a);return bj(a,b)}function Ng(a,b,c){a=Mg(a);return je(a,b,c)}function Og(a){null===qa?(qa=[a],Ec=je(Dc,Pg)):qa.push(a);return Qg}function ha(){if(null!==Ec){var a=Ec;Ec=null;Rg(a)}Pg()}function Pg(){if(!ke&&null!==qa){ke=!0;var a=0;
try{var b=qa;Da(99,function(){for(;a<b.length;a++){var c=b[a];do c=c(!0);while(null!==c)}});qa=null}catch(c){throw null!==qa&&(qa=qa.slice(a+1)),je(Dc,ha),c;}finally{ke=!1}}}function Fc(a,b,c){c/=10;return 1073741821-(((1073741821-a+b/10)/c|0)+1)*c}function aa(a,b){if(a&&a.defaultProps){b=M({},b);a=a.defaultProps;for(var c in a)void 0===b[c]&&(b[c]=a[c])}return b}function le(){Gc=qb=Hc=null}function me(a){var b=Ic.current;q(Ic);a.type._context._currentValue=b}function Sg(a,b){for(;null!==a;){var c=
a.alternate;if(a.childExpirationTime<b)a.childExpirationTime=b,null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);else if(null!==c&&c.childExpirationTime<b)c.childExpirationTime=b;else break;a=a.return}}function rb(a,b){Hc=a;Gc=qb=null;a=a.dependencies;null!==a&&null!==a.firstContext&&(a.expirationTime>=b&&(ia=!0),a.firstContext=null)}function W(a,b){if(Gc!==a&&!1!==b&&0!==b){if("number"!==typeof b||1073741823===b)Gc=a,b=1073741823;b={context:a,observedBits:b,next:null};if(null===qb){if(null===
Hc)throw Error(k(308));qb=b;Hc.dependencies={expirationTime:0,firstContext:b,responders:null}}else qb=qb.next=b}return a._currentValue}function ne(a){a.updateQueue={baseState:a.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function oe(a,b){a=a.updateQueue;b.updateQueue===a&&(b.updateQueue={baseState:a.baseState,baseQueue:a.baseQueue,shared:a.shared,effects:a.effects})}function Ea(a,b){a={expirationTime:a,suspenseConfig:b,tag:Tg,payload:null,callback:null,next:null};return a.next=
a}function Fa(a,b){a=a.updateQueue;if(null!==a){a=a.shared;var c=a.pending;null===c?b.next=b:(b.next=c.next,c.next=b);a.pending=b}}function Ug(a,b){var c=a.alternate;null!==c&&oe(c,a);a=a.updateQueue;c=a.baseQueue;null===c?(a.baseQueue=b.next=b,b.next=b):(b.next=c.next,c.next=b)}function Qb(a,b,c,d){var e=a.updateQueue;Ga=!1;var f=e.baseQueue,g=e.shared.pending;if(null!==g){if(null!==f){var h=f.next;f.next=g.next;g.next=h}f=g;e.shared.pending=null;h=a.alternate;null!==h&&(h=h.updateQueue,null!==h&&
(h.baseQueue=g))}if(null!==f){h=f.next;var m=e.baseState,n=0,k=null,ba=null,l=null;if(null!==h){var p=h;do{g=p.expirationTime;if(g<d){var t={expirationTime:p.expirationTime,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null};null===l?(ba=l=t,k=m):l=l.next=t;g>n&&(n=g)}else{null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null});Vg(g,p.suspenseConfig);a:{var q=a,r=p;g=b;t=c;switch(r.tag){case 1:q=
r.payload;if("function"===typeof q){m=q.call(t,m,g);break a}m=q;break a;case 3:q.effectTag=q.effectTag&-4097|64;case Tg:q=r.payload;g="function"===typeof q?q.call(t,m,g):q;if(null===g||void 0===g)break a;m=M({},m,g);break a;case Jc:Ga=!0}}null!==p.callback&&(a.effectTag|=32,g=e.effects,null===g?e.effects=[p]:g.push(p))}p=p.next;if(null===p||p===h)if(g=e.shared.pending,null===g)break;else p=f.next=g.next,g.next=h,e.baseQueue=f=g,e.shared.pending=null}while(1)}null===l?k=m:l.next=ba;e.baseState=k;e.baseQueue=
l;Kc(n);a.expirationTime=n;a.memoizedState=m}}function Wg(a,b,c){a=b.effects;b.effects=null;if(null!==a)for(b=0;b<a.length;b++){var d=a[b],e=d.callback;if(null!==e){d.callback=null;d=e;e=c;if("function"!==typeof d)throw Error(k(191,d));d.call(e)}}}function Lc(a,b,c,d){b=a.memoizedState;c=c(d,b);c=null===c||void 0===c?b:M({},b,c);a.memoizedState=c;0===a.expirationTime&&(a.updateQueue.baseState=c)}function Xg(a,b,c,d,e,f,g){a=a.stateNode;return"function"===typeof a.shouldComponentUpdate?a.shouldComponentUpdate(d,
f,g):b.prototype&&b.prototype.isPureReactComponent?!Ob(c,d)||!Ob(e,f):!0}function Yg(a,b,c){var d=!1,e=Ca;var f=b.contextType;"object"===typeof f&&null!==f?f=W(f):(e=N(b)?Ra:B.current,d=b.contextTypes,f=(d=null!==d&&void 0!==d)?pb(a,e):Ca);b=new b(c,f);a.memoizedState=null!==b.state&&void 0!==b.state?b.state:null;b.updater=Mc;a.stateNode=b;b._reactInternalFiber=a;d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=e,a.__reactInternalMemoizedMaskedChildContext=f);return b}function Zg(a,
b,c,d){a=b.state;"function"===typeof b.componentWillReceiveProps&&b.componentWillReceiveProps(c,d);"function"===typeof b.UNSAFE_componentWillReceiveProps&&b.UNSAFE_componentWillReceiveProps(c,d);b.state!==a&&Mc.enqueueReplaceState(b,b.state,null)}function pe(a,b,c,d){var e=a.stateNode;e.props=c;e.state=a.memoizedState;e.refs=$g;ne(a);var f=b.contextType;"object"===typeof f&&null!==f?e.context=W(f):(f=N(b)?Ra:B.current,e.context=pb(a,f));Qb(a,c,e,d);e.state=a.memoizedState;f=b.getDerivedStateFromProps;
"function"===typeof f&&(Lc(a,b,f,c),e.state=a.memoizedState);"function"===typeof b.getDerivedStateFromProps||"function"===typeof e.getSnapshotBeforeUpdate||"function"!==typeof e.UNSAFE_componentWillMount&&"function"!==typeof e.componentWillMount||(b=e.state,"function"===typeof e.componentWillMount&&e.componentWillMount(),"function"===typeof e.UNSAFE_componentWillMount&&e.UNSAFE_componentWillMount(),b!==e.state&&Mc.enqueueReplaceState(e,e.state,null),Qb(a,c,e,d),e.state=a.memoizedState);"function"===
typeof e.componentDidMount&&(a.effectTag|=4)}function Rb(a,b,c){a=c.ref;if(null!==a&&"function"!==typeof a&&"object"!==typeof a){if(c._owner){c=c._owner;if(c){if(1!==c.tag)throw Error(k(309));var d=c.stateNode}if(!d)throw Error(k(147,a));var e=""+a;if(null!==b&&null!==b.ref&&"function"===typeof b.ref&&b.ref._stringRef===e)return b.ref;b=function(a){var b=d.refs;b===$g&&(b=d.refs={});null===a?delete b[e]:b[e]=a};b._stringRef=e;return b}if("string"!==typeof a)throw Error(k(284));if(!c._owner)throw Error(k(290,
a));}return a}function Nc(a,b){if("textarea"!==a.type)throw Error(k(31,"[object Object]"===Object.prototype.toString.call(b)?"object with keys {"+Object.keys(b).join(", ")+"}":b,""));}function ah(a){function b(b,c){if(a){var d=b.lastEffect;null!==d?(d.nextEffect=c,b.lastEffect=c):b.firstEffect=b.lastEffect=c;c.nextEffect=null;c.effectTag=8}}function c(c,d){if(!a)return null;for(;null!==d;)b(c,d),d=d.sibling;return null}function d(a,b){for(a=new Map;null!==b;)null!==b.key?a.set(b.key,b):a.set(b.index,
b),b=b.sibling;return a}function e(a,b){a=Sa(a,b);a.index=0;a.sibling=null;return a}function f(b,c,d){b.index=d;if(!a)return c;d=b.alternate;if(null!==d)return d=d.index,d<c?(b.effectTag=2,c):d;b.effectTag=2;return c}function g(b){a&&null===b.alternate&&(b.effectTag=2);return b}function h(a,b,c,d){if(null===b||6!==b.tag)return b=qe(c,a.mode,d),b.return=a,b;b=e(b,c);b.return=a;return b}function m(a,b,c,d){if(null!==b&&b.elementType===c.type)return d=e(b,c.props),d.ref=Rb(a,b,c),d.return=a,d;d=Oc(c.type,
c.key,c.props,null,a.mode,d);d.ref=Rb(a,b,c);d.return=a;return d}function n(a,b,c,d){if(null===b||4!==b.tag||b.stateNode.containerInfo!==c.containerInfo||b.stateNode.implementation!==c.implementation)return b=re(c,a.mode,d),b.return=a,b;b=e(b,c.children||[]);b.return=a;return b}function l(a,b,c,d,f){if(null===b||7!==b.tag)return b=Ha(c,a.mode,d,f),b.return=a,b;b=e(b,c);b.return=a;return b}function ba(a,b,c){if("string"===typeof b||"number"===typeof b)return b=qe(""+b,a.mode,c),b.return=a,b;if("object"===
typeof b&&null!==b){switch(b.$$typeof){case Pc:return c=Oc(b.type,b.key,b.props,null,a.mode,c),c.ref=Rb(a,null,b),c.return=a,c;case gb:return b=re(b,a.mode,c),b.return=a,b}if(Qc(b)||zb(b))return b=Ha(b,a.mode,c,null),b.return=a,b;Nc(a,b)}return null}function p(a,b,c,d){var e=null!==b?b.key:null;if("string"===typeof c||"number"===typeof c)return null!==e?null:h(a,b,""+c,d);if("object"===typeof c&&null!==c){switch(c.$$typeof){case Pc:return c.key===e?c.type===Ma?l(a,b,c.props.children,d,e):m(a,b,c,
d):null;case gb:return c.key===e?n(a,b,c,d):null}if(Qc(c)||zb(c))return null!==e?null:l(a,b,c,d,null);Nc(a,c)}return null}function t(a,b,c,d,e){if("string"===typeof d||"number"===typeof d)return a=a.get(c)||null,h(b,a,""+d,e);if("object"===typeof d&&null!==d){switch(d.$$typeof){case Pc:return a=a.get(null===d.key?c:d.key)||null,d.type===Ma?l(b,a,d.props.children,e,d.key):m(b,a,d,e);case gb:return a=a.get(null===d.key?c:d.key)||null,n(b,a,d,e)}if(Qc(d)||zb(d))return a=a.get(c)||null,l(b,a,d,e,null);
Nc(b,d)}return null}function q(e,g,h,m){for(var n=null,k=null,l=g,r=g=0,C=null;null!==l&&r<h.length;r++){l.index>r?(C=l,l=null):C=l.sibling;var O=p(e,l,h[r],m);if(null===O){null===l&&(l=C);break}a&&l&&null===O.alternate&&b(e,l);g=f(O,g,r);null===k?n=O:k.sibling=O;k=O;l=C}if(r===h.length)return c(e,l),n;if(null===l){for(;r<h.length;r++)l=ba(e,h[r],m),null!==l&&(g=f(l,g,r),null===k?n=l:k.sibling=l,k=l);return n}for(l=d(e,l);r<h.length;r++)C=t(l,e,r,h[r],m),null!==C&&(a&&null!==C.alternate&&l.delete(null===
C.key?r:C.key),g=f(C,g,r),null===k?n=C:k.sibling=C,k=C);a&&l.forEach(function(a){return b(e,a)});return n}function w(e,g,h,n){var m=zb(h);if("function"!==typeof m)throw Error(k(150));h=m.call(h);if(null==h)throw Error(k(151));for(var l=m=null,r=g,C=g=0,O=null,v=h.next();null!==r&&!v.done;C++,v=h.next()){r.index>C?(O=r,r=null):O=r.sibling;var q=p(e,r,v.value,n);if(null===q){null===r&&(r=O);break}a&&r&&null===q.alternate&&b(e,r);g=f(q,g,C);null===l?m=q:l.sibling=q;l=q;r=O}if(v.done)return c(e,r),m;
if(null===r){for(;!v.done;C++,v=h.next())v=ba(e,v.value,n),null!==v&&(g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);return m}for(r=d(e,r);!v.done;C++,v=h.next())v=t(r,e,C,v.value,n),null!==v&&(a&&null!==v.alternate&&r.delete(null===v.key?C:v.key),g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);a&&r.forEach(function(a){return b(e,a)});return m}return function(a,d,f,h){var m="object"===typeof f&&null!==f&&f.type===Ma&&null===f.key;m&&(f=f.props.children);var n="object"===typeof f&&null!==f;if(n)switch(f.$$typeof){case Pc:a:{n=
f.key;for(m=d;null!==m;){if(m.key===n){switch(m.tag){case 7:if(f.type===Ma){c(a,m.sibling);d=e(m,f.props.children);d.return=a;a=d;break a}break;default:if(m.elementType===f.type){c(a,m.sibling);d=e(m,f.props);d.ref=Rb(a,m,f);d.return=a;a=d;break a}}c(a,m);break}else b(a,m);m=m.sibling}f.type===Ma?(d=Ha(f.props.children,a.mode,h,f.key),d.return=a,a=d):(h=Oc(f.type,f.key,f.props,null,a.mode,h),h.ref=Rb(a,d,f),h.return=a,a=h)}return g(a);case gb:a:{for(m=f.key;null!==d;){if(d.key===m)if(4===d.tag&&d.stateNode.containerInfo===
f.containerInfo&&d.stateNode.implementation===f.implementation){c(a,d.sibling);d=e(d,f.children||[]);d.return=a;a=d;break a}else{c(a,d);break}else b(a,d);d=d.sibling}d=re(f,a.mode,h);d.return=a;a=d}return g(a)}if("string"===typeof f||"number"===typeof f)return f=""+f,null!==d&&6===d.tag?(c(a,d.sibling),d=e(d,f),d.return=a,a=d):(c(a,d),d=qe(f,a.mode,h),d.return=a,a=d),g(a);if(Qc(f))return q(a,d,f,h);if(zb(f))return w(a,d,f,h);n&&Nc(a,f);if("undefined"===typeof f&&!m)switch(a.tag){case 1:case 0:throw a=
a.type,Error(k(152,a.displayName||a.name||"Component"));}return c(a,d)}}function Ta(a){if(a===Sb)throw Error(k(174));return a}function se(a,b){y(Tb,b);y(Ub,a);y(ja,Sb);a=b.nodeType;switch(a){case 9:case 11:b=(b=b.documentElement)?b.namespaceURI:Hd(null,"");break;default:a=8===a?b.parentNode:b,b=a.namespaceURI||null,a=a.tagName,b=Hd(b,a)}q(ja);y(ja,b)}function tb(a){q(ja);q(Ub);q(Tb)}function bh(a){Ta(Tb.current);var b=Ta(ja.current);var c=Hd(b,a.type);b!==c&&(y(Ub,a),y(ja,c))}function te(a){Ub.current===
a&&(q(ja),q(Ub))}function Rc(a){for(var b=a;null!==b;){if(13===b.tag){var c=b.memoizedState;if(null!==c&&(c=c.dehydrated,null===c||c.data===$d||c.data===Zd))return b}else if(19===b.tag&&void 0!==b.memoizedProps.revealOrder){if(0!==(b.effectTag&64))return b}else if(null!==b.child){b.child.return=b;b=b.child;continue}if(b===a)break;for(;null===b.sibling;){if(null===b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}return null}function ue(a,b){return{responder:a,props:b}}
function S(){throw Error(k(321));}function ve(a,b){if(null===b)return!1;for(var c=0;c<b.length&&c<a.length;c++)if(!Qa(a[c],b[c]))return!1;return!0}function we(a,b,c,d,e,f){Ia=f;z=b;b.memoizedState=null;b.updateQueue=null;b.expirationTime=0;Sc.current=null===a||null===a.memoizedState?dj:ej;a=c(d,e);if(b.expirationTime===Ia){f=0;do{b.expirationTime=0;if(!(25>f))throw Error(k(301));f+=1;J=K=null;b.updateQueue=null;Sc.current=fj;a=c(d,e)}while(b.expirationTime===Ia)}Sc.current=Tc;b=null!==K&&null!==K.next;
Ia=0;J=K=z=null;Uc=!1;if(b)throw Error(k(300));return a}function ub(){var a={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};null===J?z.memoizedState=J=a:J=J.next=a;return J}function vb(){if(null===K){var a=z.alternate;a=null!==a?a.memoizedState:null}else a=K.next;var b=null===J?z.memoizedState:J.next;if(null!==b)J=b,K=a;else{if(null===a)throw Error(k(310));K=a;a={memoizedState:K.memoizedState,baseState:K.baseState,baseQueue:K.baseQueue,queue:K.queue,next:null};null===J?z.memoizedState=
J=a:J=J.next=a}return J}function Ua(a,b){return"function"===typeof b?b(a):b}function Vc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=K,e=d.baseQueue,f=c.pending;if(null!==f){if(null!==e){var g=e.next;e.next=f.next;f.next=g}d.baseQueue=e=f;c.pending=null}if(null!==e){e=e.next;d=d.baseState;var h=g=f=null,m=e;do{var n=m.expirationTime;if(n<Ia){var l={expirationTime:m.expirationTime,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,
next:null};null===h?(g=h=l,f=d):h=h.next=l;n>z.expirationTime&&(z.expirationTime=n,Kc(n))}else null!==h&&(h=h.next={expirationTime:1073741823,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,next:null}),Vg(n,m.suspenseConfig),d=m.eagerReducer===a?m.eagerState:a(d,m.action);m=m.next}while(null!==m&&m!==e);null===h?f=d:h.next=g;Qa(d,b.memoizedState)||(ia=!0);b.memoizedState=d;b.baseState=f;b.baseQueue=h;c.lastRenderedState=d}return[b.memoizedState,
c.dispatch]}function Wc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=c.dispatch,e=c.pending,f=b.memoizedState;if(null!==e){c.pending=null;var g=e=e.next;do f=a(f,g.action),g=g.next;while(g!==e);Qa(f,b.memoizedState)||(ia=!0);b.memoizedState=f;null===b.baseQueue&&(b.baseState=f);c.lastRenderedState=f}return[f,d]}function xe(a){var b=ub();"function"===typeof a&&(a=a());b.memoizedState=b.baseState=a;a=b.queue={pending:null,dispatch:null,lastRenderedReducer:Ua,
lastRenderedState:a};a=a.dispatch=ch.bind(null,z,a);return[b.memoizedState,a]}function ye(a,b,c,d){a={tag:a,create:b,destroy:c,deps:d,next:null};b=z.updateQueue;null===b?(b={lastEffect:null},z.updateQueue=b,b.lastEffect=a.next=a):(c=b.lastEffect,null===c?b.lastEffect=a.next=a:(d=c.next,c.next=a,a.next=d,b.lastEffect=a));return a}function dh(a){return vb().memoizedState}function ze(a,b,c,d){var e=ub();z.effectTag|=a;e.memoizedState=ye(1|b,c,void 0,void 0===d?null:d)}function Ae(a,b,c,d){var e=vb();
d=void 0===d?null:d;var f=void 0;if(null!==K){var g=K.memoizedState;f=g.destroy;if(null!==d&&ve(d,g.deps)){ye(b,c,f,d);return}}z.effectTag|=a;e.memoizedState=ye(1|b,c,f,d)}function eh(a,b){return ze(516,4,a,b)}function Xc(a,b){return Ae(516,4,a,b)}function fh(a,b){return Ae(4,2,a,b)}function gh(a,b){if("function"===typeof b)return a=a(),b(a),function(){b(null)};if(null!==b&&void 0!==b)return a=a(),b.current=a,function(){b.current=null}}function hh(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;
return Ae(4,2,gh.bind(null,b,a),c)}function Be(a,b){}function ih(a,b){ub().memoizedState=[a,void 0===b?null:b];return a}function Yc(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];c.memoizedState=[a,b];return a}function jh(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];a=a();c.memoizedState=[a,b];return a}function Ce(a,b,c){var d=Cc();Da(98>d?98:d,function(){a(!0)});Da(97<d?97:d,function(){var d=
X.suspense;X.suspense=void 0===b?null:b;try{a(!1),c()}finally{X.suspense=d}})}function ch(a,b,c){var d=ka(),e=Vb.suspense;d=Va(d,a,e);e={expirationTime:d,suspenseConfig:e,action:c,eagerReducer:null,eagerState:null,next:null};var f=b.pending;null===f?e.next=e:(e.next=f.next,f.next=e);b.pending=e;f=a.alternate;if(a===z||null!==f&&f===z)Uc=!0,e.expirationTime=Ia,z.expirationTime=Ia;else{if(0===a.expirationTime&&(null===f||0===f.expirationTime)&&(f=b.lastRenderedReducer,null!==f))try{var g=b.lastRenderedState,
h=f(g,c);e.eagerReducer=f;e.eagerState=h;if(Qa(h,g))return}catch(m){}finally{}Ja(a,d)}}function kh(a,b){var c=la(5,null,null,0);c.elementType="DELETED";c.type="DELETED";c.stateNode=b;c.return=a;c.effectTag=8;null!==a.lastEffect?(a.lastEffect.nextEffect=c,a.lastEffect=c):a.firstEffect=a.lastEffect=c}function lh(a,b){switch(a.tag){case 5:var c=a.type;b=1!==b.nodeType||c.toLowerCase()!==b.nodeName.toLowerCase()?null:b;return null!==b?(a.stateNode=b,!0):!1;case 6:return b=""===a.pendingProps||3!==b.nodeType?
null:b,null!==b?(a.stateNode=b,!0):!1;case 13:return!1;default:return!1}}function De(a){if(Wa){var b=Ka;if(b){var c=b;if(!lh(a,b)){b=kb(c.nextSibling);if(!b||!lh(a,b)){a.effectTag=a.effectTag&-1025|2;Wa=!1;ra=a;return}kh(ra,c)}ra=a;Ka=kb(b.firstChild)}else a.effectTag=a.effectTag&-1025|2,Wa=!1,ra=a}}function mh(a){for(a=a.return;null!==a&&5!==a.tag&&3!==a.tag&&13!==a.tag;)a=a.return;ra=a}function Zc(a){if(a!==ra)return!1;if(!Wa)return mh(a),Wa=!0,!1;var b=a.type;if(5!==a.tag||"head"!==b&&"body"!==
b&&!Yd(b,a.memoizedProps))for(b=Ka;b;)kh(a,b),b=kb(b.nextSibling);mh(a);if(13===a.tag){a=a.memoizedState;a=null!==a?a.dehydrated:null;if(!a)throw Error(k(317));a:{a=a.nextSibling;for(b=0;a;){if(8===a.nodeType){var c=a.data;if(c===og){if(0===b){Ka=kb(a.nextSibling);break a}b--}else c!==ng&&c!==Zd&&c!==$d||b++}a=a.nextSibling}Ka=null}}else Ka=ra?kb(a.stateNode.nextSibling):null;return!0}function Ee(){Ka=ra=null;Wa=!1}function T(a,b,c,d){b.child=null===a?Fe(b,null,c,d):wb(b,a.child,c,d)}function nh(a,
b,c,d,e){c=c.render;var f=b.ref;rb(b,e);d=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,d,e);return b.child}function oh(a,b,c,d,e,f){if(null===a){var g=c.type;if("function"===typeof g&&!Ge(g)&&void 0===g.defaultProps&&null===c.compare&&void 0===c.defaultProps)return b.tag=15,b.type=g,ph(a,b,g,d,e,f);a=Oc(c.type,null,d,null,b.mode,f);a.ref=b.ref;a.return=b;return b.child=a}g=a.child;if(e<
f&&(e=g.memoizedProps,c=c.compare,c=null!==c?c:Ob,c(e,d)&&a.ref===b.ref))return sa(a,b,f);b.effectTag|=1;a=Sa(g,d);a.ref=b.ref;a.return=b;return b.child=a}function ph(a,b,c,d,e,f){return null!==a&&Ob(a.memoizedProps,d)&&a.ref===b.ref&&(ia=!1,e<f)?(b.expirationTime=a.expirationTime,sa(a,b,f)):He(a,b,c,d,f)}function qh(a,b){var c=b.ref;if(null===a&&null!==c||null!==a&&a.ref!==c)b.effectTag|=128}function He(a,b,c,d,e){var f=N(c)?Ra:B.current;f=pb(b,f);rb(b,e);c=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=
a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,c,e);return b.child}function rh(a,b,c,d,e){if(N(c)){var f=!0;Bc(b)}else f=!1;rb(b,e);if(null===b.stateNode)null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),Yg(b,c,d),pe(b,c,d,e),d=!0;else if(null===a){var g=b.stateNode,h=b.memoizedProps;g.props=h;var m=g.context,n=c.contextType;"object"===typeof n&&null!==n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n));var l=c.getDerivedStateFromProps,k="function"===
typeof l||"function"===typeof g.getSnapshotBeforeUpdate;k||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n);Ga=!1;var p=b.memoizedState;g.state=p;Qb(b,d,g,e);m=b.memoizedState;h!==d||p!==m||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),m=b.memoizedState),(h=Ga||Xg(b,c,h,d,p,m,n))?(k||"function"!==typeof g.UNSAFE_componentWillMount&&"function"!==typeof g.componentWillMount||("function"===typeof g.componentWillMount&&
g.componentWillMount(),"function"===typeof g.UNSAFE_componentWillMount&&g.UNSAFE_componentWillMount()),"function"===typeof g.componentDidMount&&(b.effectTag|=4)):("function"===typeof g.componentDidMount&&(b.effectTag|=4),b.memoizedProps=d,b.memoizedState=m),g.props=d,g.state=m,g.context=n,d=h):("function"===typeof g.componentDidMount&&(b.effectTag|=4),d=!1)}else g=b.stateNode,oe(a,b),h=b.memoizedProps,g.props=b.type===b.elementType?h:aa(b.type,h),m=g.context,n=c.contextType,"object"===typeof n&&null!==
n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n)),l=c.getDerivedStateFromProps,(k="function"===typeof l||"function"===typeof g.getSnapshotBeforeUpdate)||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n),Ga=!1,m=b.memoizedState,g.state=m,Qb(b,d,g,e),p=b.memoizedState,h!==d||m!==p||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),p=b.memoizedState),(l=Ga||Xg(b,c,h,d,m,p,n))?(k||"function"!==typeof g.UNSAFE_componentWillUpdate&&
"function"!==typeof g.componentWillUpdate||("function"===typeof g.componentWillUpdate&&g.componentWillUpdate(d,p,n),"function"===typeof g.UNSAFE_componentWillUpdate&&g.UNSAFE_componentWillUpdate(d,p,n)),"function"===typeof g.componentDidUpdate&&(b.effectTag|=4),"function"===typeof g.getSnapshotBeforeUpdate&&(b.effectTag|=256)):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===
a.memoizedState||(b.effectTag|=256),b.memoizedProps=d,b.memoizedState=p),g.props=d,g.state=p,g.context=n,d=l):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=256),d=!1);return Ie(a,b,c,d,f,e)}function Ie(a,b,c,d,e,f){qh(a,b);var g=0!==(b.effectTag&64);if(!d&&!g)return e&&Hg(b,c,!1),sa(a,b,f);d=b.stateNode;gj.current=b;var h=g&&"function"!==typeof c.getDerivedStateFromError?
null:d.render();b.effectTag|=1;null!==a&&g?(b.child=wb(b,a.child,null,f),b.child=wb(b,null,h,f)):T(a,b,h,f);b.memoizedState=d.state;e&&Hg(b,c,!0);return b.child}function sh(a){var b=a.stateNode;b.pendingContext?Fg(a,b.pendingContext,b.pendingContext!==b.context):b.context&&Fg(a,b.context,!1);se(a,b.containerInfo)}function th(a,b,c){var d=b.mode,e=b.pendingProps,f=D.current,g=!1,h;(h=0!==(b.effectTag&64))||(h=0!==(f&2)&&(null===a||null!==a.memoizedState));h?(g=!0,b.effectTag&=-65):null!==a&&null===
a.memoizedState||void 0===e.fallback||!0===e.unstable_avoidThisFallback||(f|=1);y(D,f&1);if(null===a){void 0!==e.fallback&&De(b);if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;b.memoizedState=Je;b.child=e;return c}d=e.children;b.memoizedState=null;return b.child=Fe(b,null,d,c)}if(null!==a.memoizedState){a=a.child;d=a.sibling;if(g){e=e.fallback;
c=Sa(a,a.pendingProps);c.return=b;if(0===(b.mode&2)&&(g=null!==b.memoizedState?b.child.child:b.child,g!==a.child))for(c.child=g;null!==g;)g.return=c,g=g.sibling;d=Sa(d,e);d.return=b;c.sibling=d;c.childExpirationTime=0;b.memoizedState=Je;b.child=c;return d}c=wb(b,a.child,e.children,c);b.memoizedState=null;return b.child=c}a=a.child;if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;e.child=a;null!==a&&(a.return=e);if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==
a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;c.effectTag|=2;e.childExpirationTime=0;b.memoizedState=Je;b.child=e;return c}b.memoizedState=null;return b.child=wb(b,a,e.children,c)}function uh(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);Sg(a.return,b)}function Ke(a,b,c,d,e,f){var g=a.memoizedState;null===g?a.memoizedState={isBackwards:b,rendering:null,renderingStartTime:0,last:d,tail:c,tailExpiration:0,tailMode:e,
lastEffect:f}:(g.isBackwards=b,g.rendering=null,g.renderingStartTime=0,g.last=d,g.tail=c,g.tailExpiration=0,g.tailMode=e,g.lastEffect=f)}function vh(a,b,c){var d=b.pendingProps,e=d.revealOrder,f=d.tail;T(a,b,d.children,c);d=D.current;if(0!==(d&2))d=d&1|2,b.effectTag|=64;else{if(null!==a&&0!==(a.effectTag&64))a:for(a=b.child;null!==a;){if(13===a.tag)null!==a.memoizedState&&uh(a,c);else if(19===a.tag)uh(a,c);else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===b)break a;for(;null===a.sibling;){if(null===
a.return||a.return===b)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}d&=1}y(D,d);if(0===(b.mode&2))b.memoizedState=null;else switch(e){case "forwards":c=b.child;for(e=null;null!==c;)a=c.alternate,null!==a&&null===Rc(a)&&(e=c),c=c.sibling;c=e;null===c?(e=b.child,b.child=null):(e=c.sibling,c.sibling=null);Ke(b,!1,e,c,f,b.lastEffect);break;case "backwards":c=null;e=b.child;for(b.child=null;null!==e;){a=e.alternate;if(null!==a&&null===Rc(a)){b.child=e;break}a=e.sibling;e.sibling=c;c=e;e=a}Ke(b,
!0,c,null,f,b.lastEffect);break;case "together":Ke(b,!1,null,null,void 0,b.lastEffect);break;default:b.memoizedState=null}return b.child}function sa(a,b,c){null!==a&&(b.dependencies=a.dependencies);var d=b.expirationTime;0!==d&&Kc(d);if(b.childExpirationTime<c)return null;if(null!==a&&b.child!==a.child)throw Error(k(153));if(null!==b.child){a=b.child;c=Sa(a,a.pendingProps);b.child=c;for(c.return=b;null!==a.sibling;)a=a.sibling,c=c.sibling=Sa(a,a.pendingProps),c.return=b;c.sibling=null}return b.child}
function $c(a,b){switch(a.tailMode){case "hidden":b=a.tail;for(var c=null;null!==b;)null!==b.alternate&&(c=b),b=b.sibling;null===c?a.tail=null:c.sibling=null;break;case "collapsed":c=a.tail;for(var d=null;null!==c;)null!==c.alternate&&(d=c),c=c.sibling;null===d?b||null===a.tail?a.tail=null:a.tail.sibling=null:d.sibling=null}}function hj(a,b,c){var d=b.pendingProps;switch(b.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return N(b.type)&&(q(G),q(B)),
null;case 3:return tb(),q(G),q(B),c=b.stateNode,c.pendingContext&&(c.context=c.pendingContext,c.pendingContext=null),null!==a&&null!==a.child||!Zc(b)||(b.effectTag|=4),wh(b),null;case 5:te(b);c=Ta(Tb.current);var e=b.type;if(null!==a&&null!=b.stateNode)ij(a,b,e,d,c),a.ref!==b.ref&&(b.effectTag|=128);else{if(!d){if(null===b.stateNode)throw Error(k(166));return null}a=Ta(ja.current);if(Zc(b)){d=b.stateNode;e=b.type;var f=b.memoizedProps;d[Aa]=b;d[vc]=f;switch(e){case "iframe":case "object":case "embed":w("load",
d);break;case "video":case "audio":for(a=0;a<Db.length;a++)w(Db[a],d);break;case "source":w("error",d);break;case "img":case "image":case "link":w("error",d);w("load",d);break;case "form":w("reset",d);w("submit",d);break;case "details":w("toggle",d);break;case "input":Hf(d,f);w("invalid",d);oa(c,"onChange");break;case "select":d._wrapperState={wasMultiple:!!f.multiple};w("invalid",d);oa(c,"onChange");break;case "textarea":Kf(d,f),w("invalid",d),oa(c,"onChange")}Ud(e,f);a=null;for(var g in f)if(f.hasOwnProperty(g)){var h=
f[g];"children"===g?"string"===typeof h?d.textContent!==h&&(a=["children",h]):"number"===typeof h&&d.textContent!==""+h&&(a=["children",""+h]):db.hasOwnProperty(g)&&null!=h&&oa(c,g)}switch(e){case "input":mc(d);Jf(d,f,!0);break;case "textarea":mc(d);Mf(d);break;case "select":case "option":break;default:"function"===typeof f.onClick&&(d.onclick=uc)}c=a;b.updateQueue=c;null!==c&&(b.effectTag|=4)}else{g=9===c.nodeType?c:c.ownerDocument;"http://www.w3.org/1999/xhtml"===a&&(a=Nf(e));"http://www.w3.org/1999/xhtml"===
a?"script"===e?(a=g.createElement("div"),a.innerHTML="<script>\x3c/script>",a=a.removeChild(a.firstChild)):"string"===typeof d.is?a=g.createElement(e,{is:d.is}):(a=g.createElement(e),"select"===e&&(g=a,d.multiple?g.multiple=!0:d.size&&(g.size=d.size))):a=g.createElementNS(a,e);a[Aa]=b;a[vc]=d;jj(a,b,!1,!1);b.stateNode=a;g=Vd(e,d);switch(e){case "iframe":case "object":case "embed":w("load",a);h=d;break;case "video":case "audio":for(h=0;h<Db.length;h++)w(Db[h],a);h=d;break;case "source":w("error",a);
h=d;break;case "img":case "image":case "link":w("error",a);w("load",a);h=d;break;case "form":w("reset",a);w("submit",a);h=d;break;case "details":w("toggle",a);h=d;break;case "input":Hf(a,d);h=Cd(a,d);w("invalid",a);oa(c,"onChange");break;case "option":h=Fd(a,d);break;case "select":a._wrapperState={wasMultiple:!!d.multiple};h=M({},d,{value:void 0});w("invalid",a);oa(c,"onChange");break;case "textarea":Kf(a,d);h=Gd(a,d);w("invalid",a);oa(c,"onChange");break;default:h=d}Ud(e,h);var m=h;for(f in m)if(m.hasOwnProperty(f)){var n=
m[f];"style"===f?gg(a,n):"dangerouslySetInnerHTML"===f?(n=n?n.__html:void 0,null!=n&&xh(a,n)):"children"===f?"string"===typeof n?("textarea"!==e||""!==n)&&Wb(a,n):"number"===typeof n&&Wb(a,""+n):"suppressContentEditableWarning"!==f&&"suppressHydrationWarning"!==f&&"autoFocus"!==f&&(db.hasOwnProperty(f)?null!=n&&oa(c,f):null!=n&&xd(a,f,n,g))}switch(e){case "input":mc(a);Jf(a,d,!1);break;case "textarea":mc(a);Mf(a);break;case "option":null!=d.value&&a.setAttribute("value",""+va(d.value));break;case "select":a.multiple=
!!d.multiple;c=d.value;null!=c?hb(a,!!d.multiple,c,!1):null!=d.defaultValue&&hb(a,!!d.multiple,d.defaultValue,!0);break;default:"function"===typeof h.onClick&&(a.onclick=uc)}lg(e,d)&&(b.effectTag|=4)}null!==b.ref&&(b.effectTag|=128)}return null;case 6:if(a&&null!=b.stateNode)kj(a,b,a.memoizedProps,d);else{if("string"!==typeof d&&null===b.stateNode)throw Error(k(166));c=Ta(Tb.current);Ta(ja.current);Zc(b)?(c=b.stateNode,d=b.memoizedProps,c[Aa]=b,c.nodeValue!==d&&(b.effectTag|=4)):(c=(9===c.nodeType?
c:c.ownerDocument).createTextNode(d),c[Aa]=b,b.stateNode=c)}return null;case 13:q(D);d=b.memoizedState;if(0!==(b.effectTag&64))return b.expirationTime=c,b;c=null!==d;d=!1;null===a?void 0!==b.memoizedProps.fallback&&Zc(b):(e=a.memoizedState,d=null!==e,c||null===e||(e=a.child.sibling,null!==e&&(f=b.firstEffect,null!==f?(b.firstEffect=e,e.nextEffect=f):(b.firstEffect=b.lastEffect=e,e.nextEffect=null),e.effectTag=8)));if(c&&!d&&0!==(b.mode&2))if(null===a&&!0!==b.memoizedProps.unstable_avoidThisFallback||
0!==(D.current&1))F===Xa&&(F=ad);else{if(F===Xa||F===ad)F=bd;0!==Xb&&null!==U&&(Ya(U,P),yh(U,Xb))}if(c||d)b.effectTag|=4;return null;case 4:return tb(),wh(b),null;case 10:return me(b),null;case 17:return N(b.type)&&(q(G),q(B)),null;case 19:q(D);d=b.memoizedState;if(null===d)return null;e=0!==(b.effectTag&64);f=d.rendering;if(null===f)if(e)$c(d,!1);else{if(F!==Xa||null!==a&&0!==(a.effectTag&64))for(f=b.child;null!==f;){a=Rc(f);if(null!==a){b.effectTag|=64;$c(d,!1);e=a.updateQueue;null!==e&&(b.updateQueue=
e,b.effectTag|=4);null===d.lastEffect&&(b.firstEffect=null);b.lastEffect=d.lastEffect;for(d=b.child;null!==d;)e=d,f=c,e.effectTag&=2,e.nextEffect=null,e.firstEffect=null,e.lastEffect=null,a=e.alternate,null===a?(e.childExpirationTime=0,e.expirationTime=f,e.child=null,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null):(e.childExpirationTime=a.childExpirationTime,e.expirationTime=a.expirationTime,e.child=a.child,e.memoizedProps=a.memoizedProps,e.memoizedState=a.memoizedState,
e.updateQueue=a.updateQueue,f=a.dependencies,e.dependencies=null===f?null:{expirationTime:f.expirationTime,firstContext:f.firstContext,responders:f.responders}),d=d.sibling;y(D,D.current&1|2);return b.child}f=f.sibling}}else{if(!e)if(a=Rc(f),null!==a){if(b.effectTag|=64,e=!0,c=a.updateQueue,null!==c&&(b.updateQueue=c,b.effectTag|=4),$c(d,!0),null===d.tail&&"hidden"===d.tailMode&&!f.alternate)return b=b.lastEffect=d.lastEffect,null!==b&&(b.nextEffect=null),null}else 2*Y()-d.renderingStartTime>d.tailExpiration&&
1<c&&(b.effectTag|=64,e=!0,$c(d,!1),b.expirationTime=b.childExpirationTime=c-1);d.isBackwards?(f.sibling=b.child,b.child=f):(c=d.last,null!==c?c.sibling=f:b.child=f,d.last=f)}return null!==d.tail?(0===d.tailExpiration&&(d.tailExpiration=Y()+500),c=d.tail,d.rendering=c,d.tail=c.sibling,d.lastEffect=b.lastEffect,d.renderingStartTime=Y(),c.sibling=null,b=D.current,y(D,e?b&1|2:b&1),c):null}throw Error(k(156,b.tag));}function lj(a,b){switch(a.tag){case 1:return N(a.type)&&(q(G),q(B)),b=a.effectTag,b&4096?
(a.effectTag=b&-4097|64,a):null;case 3:tb();q(G);q(B);b=a.effectTag;if(0!==(b&64))throw Error(k(285));a.effectTag=b&-4097|64;return a;case 5:return te(a),null;case 13:return q(D),b=a.effectTag,b&4096?(a.effectTag=b&-4097|64,a):null;case 19:return q(D),null;case 4:return tb(),null;case 10:return me(a),null;default:return null}}function Le(a,b){return{value:a,source:b,stack:Bd(b)}}function Me(a,b){var c=b.source,d=b.stack;null===d&&null!==c&&(d=Bd(c));null!==c&&na(c.type);b=b.value;null!==a&&1===a.tag&&
na(a.type);try{console.error(b)}catch(e){setTimeout(function(){throw e;})}}function mj(a,b){try{b.props=a.memoizedProps,b.state=a.memoizedState,b.componentWillUnmount()}catch(c){Za(a,c)}}function zh(a){var b=a.ref;if(null!==b)if("function"===typeof b)try{b(null)}catch(c){Za(a,c)}else b.current=null}function nj(a,b){switch(b.tag){case 0:case 11:case 15:case 22:return;case 1:if(b.effectTag&256&&null!==a){var c=a.memoizedProps,d=a.memoizedState;a=b.stateNode;b=a.getSnapshotBeforeUpdate(b.elementType===
b.type?c:aa(b.type,c),d);a.__reactInternalSnapshotBeforeUpdate=b}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(k(163));}function Ah(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.destroy;c.destroy=void 0;void 0!==d&&d()}c=c.next}while(c!==b)}}function Bh(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.create;c.destroy=d()}c=c.next}while(c!==b)}}function oj(a,b,c,d){switch(c.tag){case 0:case 11:case 15:case 22:Bh(3,
c);return;case 1:a=c.stateNode;c.effectTag&4&&(null===b?a.componentDidMount():(d=c.elementType===c.type?b.memoizedProps:aa(c.type,b.memoizedProps),a.componentDidUpdate(d,b.memoizedState,a.__reactInternalSnapshotBeforeUpdate)));b=c.updateQueue;null!==b&&Wg(c,b,a);return;case 3:b=c.updateQueue;if(null!==b){a=null;if(null!==c.child)switch(c.child.tag){case 5:a=c.child.stateNode;break;case 1:a=c.child.stateNode}Wg(c,b,a)}return;case 5:a=c.stateNode;null===b&&c.effectTag&4&&lg(c.type,c.memoizedProps)&&
a.focus();return;case 6:return;case 4:return;case 12:return;case 13:null===c.memoizedState&&(c=c.alternate,null!==c&&(c=c.memoizedState,null!==c&&(c=c.dehydrated,null!==c&&bg(c))));return;case 19:case 17:case 20:case 21:return}throw Error(k(163));}function Ch(a,b,c){"function"===typeof Ne&&Ne(b);switch(b.tag){case 0:case 11:case 14:case 15:case 22:a=b.updateQueue;if(null!==a&&(a=a.lastEffect,null!==a)){var d=a.next;Da(97<c?97:c,function(){var a=d;do{var c=a.destroy;if(void 0!==c){var g=b;try{c()}catch(h){Za(g,
h)}}a=a.next}while(a!==d)})}break;case 1:zh(b);c=b.stateNode;"function"===typeof c.componentWillUnmount&&mj(b,c);break;case 5:zh(b);break;case 4:Dh(a,b,c)}}function Eh(a){var b=a.alternate;a.return=null;a.child=null;a.memoizedState=null;a.updateQueue=null;a.dependencies=null;a.alternate=null;a.firstEffect=null;a.lastEffect=null;a.pendingProps=null;a.memoizedProps=null;a.stateNode=null;null!==b&&Eh(b)}function Fh(a){return 5===a.tag||3===a.tag||4===a.tag}function Gh(a){a:{for(var b=a.return;null!==
b;){if(Fh(b)){var c=b;break a}b=b.return}throw Error(k(160));}b=c.stateNode;switch(c.tag){case 5:var d=!1;break;case 3:b=b.containerInfo;d=!0;break;case 4:b=b.containerInfo;d=!0;break;default:throw Error(k(161));}c.effectTag&16&&(Wb(b,""),c.effectTag&=-17);a:b:for(c=a;;){for(;null===c.sibling;){if(null===c.return||Fh(c.return)){c=null;break a}c=c.return}c.sibling.return=c.return;for(c=c.sibling;5!==c.tag&&6!==c.tag&&18!==c.tag;){if(c.effectTag&2)continue b;if(null===c.child||4===c.tag)continue b;
else c.child.return=c,c=c.child}if(!(c.effectTag&2)){c=c.stateNode;break a}}d?Oe(a,c,b):Pe(a,c,b)}function Oe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?8===c.nodeType?c.parentNode.insertBefore(a,b):c.insertBefore(a,b):(8===c.nodeType?(b=c.parentNode,b.insertBefore(a,c)):(b=c,b.appendChild(a)),c=c._reactRootContainer,null!==c&&void 0!==c||null!==b.onclick||(b.onclick=uc));else if(4!==d&&(a=a.child,null!==a))for(Oe(a,b,c),a=a.sibling;null!==a;)Oe(a,b,c),a=a.sibling}
function Pe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?c.insertBefore(a,b):c.appendChild(a);else if(4!==d&&(a=a.child,null!==a))for(Pe(a,b,c),a=a.sibling;null!==a;)Pe(a,b,c),a=a.sibling}function Dh(a,b,c){for(var d=b,e=!1,f,g;;){if(!e){e=d.return;a:for(;;){if(null===e)throw Error(k(160));f=e.stateNode;switch(e.tag){case 5:g=!1;break a;case 3:f=f.containerInfo;g=!0;break a;case 4:f=f.containerInfo;g=!0;break a}e=e.return}e=!0}if(5===d.tag||6===d.tag){a:for(var h=
a,m=d,n=c,l=m;;)if(Ch(h,l,n),null!==l.child&&4!==l.tag)l.child.return=l,l=l.child;else{if(l===m)break a;for(;null===l.sibling;){if(null===l.return||l.return===m)break a;l=l.return}l.sibling.return=l.return;l=l.sibling}g?(h=f,m=d.stateNode,8===h.nodeType?h.parentNode.removeChild(m):h.removeChild(m)):f.removeChild(d.stateNode)}else if(4===d.tag){if(null!==d.child){f=d.stateNode.containerInfo;g=!0;d.child.return=d;d=d.child;continue}}else if(Ch(a,d,c),null!==d.child){d.child.return=d;d=d.child;continue}if(d===
b)break;for(;null===d.sibling;){if(null===d.return||d.return===b)return;d=d.return;4===d.tag&&(e=!1)}d.sibling.return=d.return;d=d.sibling}}function Qe(a,b){switch(b.tag){case 0:case 11:case 14:case 15:case 22:Ah(3,b);return;case 1:return;case 5:var c=b.stateNode;if(null!=c){var d=b.memoizedProps,e=null!==a?a.memoizedProps:d;a=b.type;var f=b.updateQueue;b.updateQueue=null;if(null!==f){c[vc]=d;"input"===a&&"radio"===d.type&&null!=d.name&&If(c,d);Vd(a,e);b=Vd(a,d);for(e=0;e<f.length;e+=2){var g=f[e],
h=f[e+1];"style"===g?gg(c,h):"dangerouslySetInnerHTML"===g?xh(c,h):"children"===g?Wb(c,h):xd(c,g,h,b)}switch(a){case "input":Dd(c,d);break;case "textarea":Lf(c,d);break;case "select":b=c._wrapperState.wasMultiple,c._wrapperState.wasMultiple=!!d.multiple,a=d.value,null!=a?hb(c,!!d.multiple,a,!1):b!==!!d.multiple&&(null!=d.defaultValue?hb(c,!!d.multiple,d.defaultValue,!0):hb(c,!!d.multiple,d.multiple?[]:"",!1))}}}return;case 6:if(null===b.stateNode)throw Error(k(162));b.stateNode.nodeValue=b.memoizedProps;
return;case 3:b=b.stateNode;b.hydrate&&(b.hydrate=!1,bg(b.containerInfo));return;case 12:return;case 13:c=b;null===b.memoizedState?d=!1:(d=!0,c=b.child,Re=Y());if(null!==c)a:for(a=c;;){if(5===a.tag)f=a.stateNode,d?(f=f.style,"function"===typeof f.setProperty?f.setProperty("display","none","important"):f.display="none"):(f=a.stateNode,e=a.memoizedProps.style,e=void 0!==e&&null!==e&&e.hasOwnProperty("display")?e.display:null,f.style.display=fg("display",e));else if(6===a.tag)a.stateNode.nodeValue=d?
"":a.memoizedProps;else if(13===a.tag&&null!==a.memoizedState&&null===a.memoizedState.dehydrated){f=a.child.sibling;f.return=a;a=f;continue}else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===c)break;for(;null===a.sibling;){if(null===a.return||a.return===c)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}Hh(b);return;case 19:Hh(b);return;case 17:return}throw Error(k(163));}function Hh(a){var b=a.updateQueue;if(null!==b){a.updateQueue=null;var c=a.stateNode;null===c&&(c=a.stateNode=
new pj);b.forEach(function(b){var d=qj.bind(null,a,b);c.has(b)||(c.add(b),b.then(d,d))})}}function Ih(a,b,c){c=Ea(c,null);c.tag=3;c.payload={element:null};var d=b.value;c.callback=function(){cd||(cd=!0,Se=d);Me(a,b)};return c}function Jh(a,b,c){c=Ea(c,null);c.tag=3;var d=a.type.getDerivedStateFromError;if("function"===typeof d){var e=b.value;c.payload=function(){Me(a,b);return d(e)}}var f=a.stateNode;null!==f&&"function"===typeof f.componentDidCatch&&(c.callback=function(){"function"!==typeof d&&
(null===La?La=new Set([this]):La.add(this),Me(a,b));var c=b.stack;this.componentDidCatch(b.value,{componentStack:null!==c?c:""})});return c}function ka(){return(p&(ca|ma))!==H?1073741821-(Y()/10|0):0!==dd?dd:dd=1073741821-(Y()/10|0)}function Va(a,b,c){b=b.mode;if(0===(b&2))return 1073741823;var d=Cc();if(0===(b&4))return 99===d?1073741823:1073741822;if((p&ca)!==H)return P;if(null!==c)a=Fc(a,c.timeoutMs|0||5E3,250);else switch(d){case 99:a=1073741823;break;case 98:a=Fc(a,150,100);break;case 97:case 96:a=
Fc(a,5E3,250);break;case 95:a=2;break;default:throw Error(k(326));}null!==U&&a===P&&--a;return a}function ed(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);var d=a.return,e=null;if(null===d&&3===a.tag)e=a.stateNode;else for(;null!==d;){c=d.alternate;d.childExpirationTime<b&&(d.childExpirationTime=b);null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);if(null===d.return&&3===d.tag){e=d.stateNode;break}d=d.return}null!==e&&
(U===e&&(Kc(b),F===bd&&Ya(e,P)),yh(e,b));return e}function fd(a){var b=a.lastExpiredTime;if(0!==b)return b;b=a.firstPendingTime;if(!Kh(a,b))return b;var c=a.lastPingedTime;a=a.nextKnownPendingLevel;a=c>a?c:a;return 2>=a&&b!==a?0:a}function V(a){if(0!==a.lastExpiredTime)a.callbackExpirationTime=1073741823,a.callbackPriority=99,a.callbackNode=Og(Te.bind(null,a));else{var b=fd(a),c=a.callbackNode;if(0===b)null!==c&&(a.callbackNode=null,a.callbackExpirationTime=0,a.callbackPriority=90);else{var d=ka();
1073741823===b?d=99:1===b||2===b?d=95:(d=10*(1073741821-b)-10*(1073741821-d),d=0>=d?99:250>=d?98:5250>=d?97:95);if(null!==c){var e=a.callbackPriority;if(a.callbackExpirationTime===b&&e>=d)return;c!==Qg&&Rg(c)}a.callbackExpirationTime=b;a.callbackPriority=d;b=1073741823===b?Og(Te.bind(null,a)):Ng(d,Lh.bind(null,a),{timeout:10*(1073741821-b)-Y()});a.callbackNode=b}}}function Lh(a,b){dd=0;if(b)return b=ka(),Ue(a,b),V(a),null;var c=fd(a);if(0!==c){b=a.callbackNode;if((p&(ca|ma))!==H)throw Error(k(327));
xb();a===U&&c===P||$a(a,c);if(null!==t){var d=p;p|=ca;var e=Mh();do try{rj();break}catch(h){Nh(a,h)}while(1);le();p=d;gd.current=e;if(F===hd)throw b=id,$a(a,c),Ya(a,c),V(a),b;if(null===t)switch(e=a.finishedWork=a.current.alternate,a.finishedExpirationTime=c,d=F,U=null,d){case Xa:case hd:throw Error(k(345));case Oh:Ue(a,2<c?2:c);break;case ad:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(1073741823===ta&&(e=Re+Ph-Y(),10<e)){if(jd){var f=a.lastPingedTime;if(0===f||f>=c){a.lastPingedTime=
c;$a(a,c);break}}f=fd(a);if(0!==f&&f!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}a.timeoutHandle=We(ab.bind(null,a),e);break}ab(a);break;case bd:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(jd&&(e=a.lastPingedTime,0===e||e>=c)){a.lastPingedTime=c;$a(a,c);break}e=fd(a);if(0!==e&&e!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}1073741823!==Yb?d=10*(1073741821-Yb)-Y():1073741823===ta?d=0:(d=10*(1073741821-ta)-5E3,e=Y(),c=10*(1073741821-c)-e,d=e-d,0>d&&(d=0),d=
(120>d?120:480>d?480:1080>d?1080:1920>d?1920:3E3>d?3E3:4320>d?4320:1960*sj(d/1960))-d,c<d&&(d=c));if(10<d){a.timeoutHandle=We(ab.bind(null,a),d);break}ab(a);break;case Xe:if(1073741823!==ta&&null!==kd){f=ta;var g=kd;d=g.busyMinDurationMs|0;0>=d?d=0:(e=g.busyDelayMs|0,f=Y()-(10*(1073741821-f)-(g.timeoutMs|0||5E3)),d=f<=e?0:e+d-f);if(10<d){Ya(a,c);a.timeoutHandle=We(ab.bind(null,a),d);break}}ab(a);break;default:throw Error(k(329));}V(a);if(a.callbackNode===b)return Lh.bind(null,a)}}return null}function Te(a){var b=
a.lastExpiredTime;b=0!==b?b:1073741823;if((p&(ca|ma))!==H)throw Error(k(327));xb();a===U&&b===P||$a(a,b);if(null!==t){var c=p;p|=ca;var d=Mh();do try{tj();break}catch(e){Nh(a,e)}while(1);le();p=c;gd.current=d;if(F===hd)throw c=id,$a(a,b),Ya(a,b),V(a),c;if(null!==t)throw Error(k(261));a.finishedWork=a.current.alternate;a.finishedExpirationTime=b;U=null;ab(a);V(a)}return null}function uj(){if(null!==bb){var a=bb;bb=null;a.forEach(function(a,c){Ue(c,a);V(c)});ha()}}function Qh(a,b){var c=p;p|=1;try{return a(b)}finally{p=
c,p===H&&ha()}}function Rh(a,b){var c=p;p&=-2;p|=Ye;try{return a(b)}finally{p=c,p===H&&ha()}}function $a(a,b){a.finishedWork=null;a.finishedExpirationTime=0;var c=a.timeoutHandle;-1!==c&&(a.timeoutHandle=-1,vj(c));if(null!==t)for(c=t.return;null!==c;){var d=c;switch(d.tag){case 1:d=d.type.childContextTypes;null!==d&&void 0!==d&&(q(G),q(B));break;case 3:tb();q(G);q(B);break;case 5:te(d);break;case 4:tb();break;case 13:q(D);break;case 19:q(D);break;case 10:me(d)}c=c.return}U=a;t=Sa(a.current,null);
P=b;F=Xa;id=null;Yb=ta=1073741823;kd=null;Xb=0;jd=!1}function Nh(a,b){do{try{le();Sc.current=Tc;if(Uc)for(var c=z.memoizedState;null!==c;){var d=c.queue;null!==d&&(d.pending=null);c=c.next}Ia=0;J=K=z=null;Uc=!1;if(null===t||null===t.return)return F=hd,id=b,t=null;a:{var e=a,f=t.return,g=t,h=b;b=P;g.effectTag|=2048;g.firstEffect=g.lastEffect=null;if(null!==h&&"object"===typeof h&&"function"===typeof h.then){var m=h;if(0===(g.mode&2)){var n=g.alternate;n?(g.updateQueue=n.updateQueue,g.memoizedState=
n.memoizedState,g.expirationTime=n.expirationTime):(g.updateQueue=null,g.memoizedState=null)}var l=0!==(D.current&1),k=f;do{var p;if(p=13===k.tag){var q=k.memoizedState;if(null!==q)p=null!==q.dehydrated?!0:!1;else{var w=k.memoizedProps;p=void 0===w.fallback?!1:!0!==w.unstable_avoidThisFallback?!0:l?!1:!0}}if(p){var y=k.updateQueue;if(null===y){var r=new Set;r.add(m);k.updateQueue=r}else y.add(m);if(0===(k.mode&2)){k.effectTag|=64;g.effectTag&=-2981;if(1===g.tag)if(null===g.alternate)g.tag=17;else{var O=
Ea(1073741823,null);O.tag=Jc;Fa(g,O)}g.expirationTime=1073741823;break a}h=void 0;g=b;var v=e.pingCache;null===v?(v=e.pingCache=new wj,h=new Set,v.set(m,h)):(h=v.get(m),void 0===h&&(h=new Set,v.set(m,h)));if(!h.has(g)){h.add(g);var x=xj.bind(null,e,m,g);m.then(x,x)}k.effectTag|=4096;k.expirationTime=b;break a}k=k.return}while(null!==k);h=Error((na(g.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+
Bd(g))}F!==Xe&&(F=Oh);h=Le(h,g);k=f;do{switch(k.tag){case 3:m=h;k.effectTag|=4096;k.expirationTime=b;var A=Ih(k,m,b);Ug(k,A);break a;case 1:m=h;var u=k.type,B=k.stateNode;if(0===(k.effectTag&64)&&("function"===typeof u.getDerivedStateFromError||null!==B&&"function"===typeof B.componentDidCatch&&(null===La||!La.has(B)))){k.effectTag|=4096;k.expirationTime=b;var H=Jh(k,m,b);Ug(k,H);break a}}k=k.return}while(null!==k)}t=Sh(t)}catch(cj){b=cj;continue}break}while(1)}function Mh(a){a=gd.current;gd.current=
Tc;return null===a?Tc:a}function Vg(a,b){a<ta&&2<a&&(ta=a);null!==b&&a<Yb&&2<a&&(Yb=a,kd=b)}function Kc(a){a>Xb&&(Xb=a)}function tj(){for(;null!==t;)t=Th(t)}function rj(){for(;null!==t&&!yj();)t=Th(t)}function Th(a){var b=zj(a.alternate,a,P);a.memoizedProps=a.pendingProps;null===b&&(b=Sh(a));Uh.current=null;return b}function Sh(a){t=a;do{var b=t.alternate;a=t.return;if(0===(t.effectTag&2048)){b=hj(b,t,P);if(1===P||1!==t.childExpirationTime){for(var c=0,d=t.child;null!==d;){var e=d.expirationTime,
f=d.childExpirationTime;e>c&&(c=e);f>c&&(c=f);d=d.sibling}t.childExpirationTime=c}if(null!==b)return b;null!==a&&0===(a.effectTag&2048)&&(null===a.firstEffect&&(a.firstEffect=t.firstEffect),null!==t.lastEffect&&(null!==a.lastEffect&&(a.lastEffect.nextEffect=t.firstEffect),a.lastEffect=t.lastEffect),1<t.effectTag&&(null!==a.lastEffect?a.lastEffect.nextEffect=t:a.firstEffect=t,a.lastEffect=t))}else{b=lj(t);if(null!==b)return b.effectTag&=2047,b;null!==a&&(a.firstEffect=a.lastEffect=null,a.effectTag|=
2048)}b=t.sibling;if(null!==b)return b;t=a}while(null!==t);F===Xa&&(F=Xe);return null}function Ve(a){var b=a.expirationTime;a=a.childExpirationTime;return b>a?b:a}function ab(a){var b=Cc();Da(99,Aj.bind(null,a,b));return null}function Aj(a,b){do xb();while(null!==Zb);if((p&(ca|ma))!==H)throw Error(k(327));var c=a.finishedWork,d=a.finishedExpirationTime;if(null===c)return null;a.finishedWork=null;a.finishedExpirationTime=0;if(c===a.current)throw Error(k(177));a.callbackNode=null;a.callbackExpirationTime=
0;a.callbackPriority=90;a.nextKnownPendingLevel=0;var e=Ve(c);a.firstPendingTime=e;d<=a.lastSuspendedTime?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:d<=a.firstSuspendedTime&&(a.firstSuspendedTime=d-1);d<=a.lastPingedTime&&(a.lastPingedTime=0);d<=a.lastExpiredTime&&(a.lastExpiredTime=0);a===U&&(t=U=null,P=0);1<c.effectTag?null!==c.lastEffect?(c.lastEffect.nextEffect=c,e=c.firstEffect):e=c:e=c.firstEffect;if(null!==e){var f=p;p|=ma;Uh.current=null;Ze=tc;var g=kg();if(Xd(g)){if("selectionStart"in
g)var h={start:g.selectionStart,end:g.selectionEnd};else a:{h=(h=g.ownerDocument)&&h.defaultView||window;var m=h.getSelection&&h.getSelection();if(m&&0!==m.rangeCount){h=m.anchorNode;var n=m.anchorOffset,q=m.focusNode;m=m.focusOffset;try{h.nodeType,q.nodeType}catch(sb){h=null;break a}var ba=0,w=-1,y=-1,B=0,D=0,r=g,z=null;b:for(;;){for(var v;;){r!==h||0!==n&&3!==r.nodeType||(w=ba+n);r!==q||0!==m&&3!==r.nodeType||(y=ba+m);3===r.nodeType&&(ba+=r.nodeValue.length);if(null===(v=r.firstChild))break;z=r;
r=v}for(;;){if(r===g)break b;z===h&&++B===n&&(w=ba);z===q&&++D===m&&(y=ba);if(null!==(v=r.nextSibling))break;r=z;z=r.parentNode}r=v}h=-1===w||-1===y?null:{start:w,end:y}}else h=null}h=h||{start:0,end:0}}else h=null;$e={activeElementDetached:null,focusedElem:g,selectionRange:h};tc=!1;l=e;do try{Bj()}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=e;do try{for(g=a,h=b;null!==l;){var x=l.effectTag;x&16&&Wb(l.stateNode,"");if(x&128){var A=l.alternate;if(null!==A){var u=
A.ref;null!==u&&("function"===typeof u?u(null):u.current=null)}}switch(x&1038){case 2:Gh(l);l.effectTag&=-3;break;case 6:Gh(l);l.effectTag&=-3;Qe(l.alternate,l);break;case 1024:l.effectTag&=-1025;break;case 1028:l.effectTag&=-1025;Qe(l.alternate,l);break;case 4:Qe(l.alternate,l);break;case 8:n=l,Dh(g,n,h),Eh(n)}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);u=$e;A=kg();x=u.focusedElem;h=u.selectionRange;if(A!==x&&x&&x.ownerDocument&&jg(x.ownerDocument.documentElement,
x)){null!==h&&Xd(x)&&(A=h.start,u=h.end,void 0===u&&(u=A),"selectionStart"in x?(x.selectionStart=A,x.selectionEnd=Math.min(u,x.value.length)):(u=(A=x.ownerDocument||document)&&A.defaultView||window,u.getSelection&&(u=u.getSelection(),n=x.textContent.length,g=Math.min(h.start,n),h=void 0===h.end?g:Math.min(h.end,n),!u.extend&&g>h&&(n=h,h=g,g=n),n=ig(x,g),q=ig(x,h),n&&q&&(1!==u.rangeCount||u.anchorNode!==n.node||u.anchorOffset!==n.offset||u.focusNode!==q.node||u.focusOffset!==q.offset)&&(A=A.createRange(),
A.setStart(n.node,n.offset),u.removeAllRanges(),g>h?(u.addRange(A),u.extend(q.node,q.offset)):(A.setEnd(q.node,q.offset),u.addRange(A))))));A=[];for(u=x;u=u.parentNode;)1===u.nodeType&&A.push({element:u,left:u.scrollLeft,top:u.scrollTop});"function"===typeof x.focus&&x.focus();for(x=0;x<A.length;x++)u=A[x],u.element.scrollLeft=u.left,u.element.scrollTop=u.top}tc=!!Ze;$e=Ze=null;a.current=c;l=e;do try{for(x=a;null!==l;){var F=l.effectTag;F&36&&oj(x,l.alternate,l);if(F&128){A=void 0;var E=l.ref;if(null!==
E){var G=l.stateNode;switch(l.tag){case 5:A=G;break;default:A=G}"function"===typeof E?E(A):E.current=A}}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=null;Cj();p=f}else a.current=c;if(ld)ld=!1,Zb=a,$b=b;else for(l=e;null!==l;)b=l.nextEffect,l.nextEffect=null,l=b;b=a.firstPendingTime;0===b&&(La=null);1073741823===b?a===af?ac++:(ac=0,af=a):ac=0;"function"===typeof bf&&bf(c.stateNode,d);V(a);if(cd)throw cd=!1,a=Se,Se=null,a;if((p&Ye)!==H)return null;
ha();return null}function Bj(){for(;null!==l;){var a=l.effectTag;0!==(a&256)&&nj(l.alternate,l);0===(a&512)||ld||(ld=!0,Ng(97,function(){xb();return null}));l=l.nextEffect}}function xb(){if(90!==$b){var a=97<$b?97:$b;$b=90;return Da(a,Dj)}}function Dj(){if(null===Zb)return!1;var a=Zb;Zb=null;if((p&(ca|ma))!==H)throw Error(k(331));var b=p;p|=ma;for(a=a.current.firstEffect;null!==a;){try{var c=a;if(0!==(c.effectTag&512))switch(c.tag){case 0:case 11:case 15:case 22:Ah(5,c),Bh(5,c)}}catch(d){if(null===
a)throw Error(k(330));Za(a,d)}c=a.nextEffect;a.nextEffect=null;a=c}p=b;ha();return!0}function Vh(a,b,c){b=Le(c,b);b=Ih(a,b,1073741823);Fa(a,b);a=ed(a,1073741823);null!==a&&V(a)}function Za(a,b){if(3===a.tag)Vh(a,a,b);else for(var c=a.return;null!==c;){if(3===c.tag){Vh(c,a,b);break}else if(1===c.tag){var d=c.stateNode;if("function"===typeof c.type.getDerivedStateFromError||"function"===typeof d.componentDidCatch&&(null===La||!La.has(d))){a=Le(b,a);a=Jh(c,a,1073741823);Fa(c,a);c=ed(c,1073741823);null!==
c&&V(c);break}}c=c.return}}function xj(a,b,c){var d=a.pingCache;null!==d&&d.delete(b);U===a&&P===c?F===bd||F===ad&&1073741823===ta&&Y()-Re<Ph?$a(a,P):jd=!0:Kh(a,c)&&(b=a.lastPingedTime,0!==b&&b<c||(a.lastPingedTime=c,V(a)))}function qj(a,b){var c=a.stateNode;null!==c&&c.delete(b);b=0;0===b&&(b=ka(),b=Va(b,a,null));a=ed(a,b);null!==a&&V(a)}function Ej(a){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var b=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(b.isDisabled||!b.supportsFiber)return!0;try{var c=
b.inject(a);bf=function(a,e){try{b.onCommitFiberRoot(c,a,void 0,64===(a.current.effectTag&64))}catch(f){}};Ne=function(a){try{b.onCommitFiberUnmount(c,a)}catch(e){}}}catch(d){}return!0}function Fj(a,b,c,d){this.tag=a;this.key=c;this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null;this.index=0;this.ref=null;this.pendingProps=b;this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null;this.mode=d;this.effectTag=0;this.lastEffect=this.firstEffect=this.nextEffect=
null;this.childExpirationTime=this.expirationTime=0;this.alternate=null}function Ge(a){a=a.prototype;return!(!a||!a.isReactComponent)}function Gj(a){if("function"===typeof a)return Ge(a)?1:0;if(void 0!==a&&null!==a){a=a.$$typeof;if(a===zd)return 11;if(a===Ad)return 14}return 2}function Sa(a,b){var c=a.alternate;null===c?(c=la(a.tag,b,a.key,a.mode),c.elementType=a.elementType,c.type=a.type,c.stateNode=a.stateNode,c.alternate=a,a.alternate=c):(c.pendingProps=b,c.effectTag=0,c.nextEffect=null,c.firstEffect=
null,c.lastEffect=null);c.childExpirationTime=a.childExpirationTime;c.expirationTime=a.expirationTime;c.child=a.child;c.memoizedProps=a.memoizedProps;c.memoizedState=a.memoizedState;c.updateQueue=a.updateQueue;b=a.dependencies;c.dependencies=null===b?null:{expirationTime:b.expirationTime,firstContext:b.firstContext,responders:b.responders};c.sibling=a.sibling;c.index=a.index;c.ref=a.ref;return c}function Oc(a,b,c,d,e,f){var g=2;d=a;if("function"===typeof a)Ge(a)&&(g=1);else if("string"===typeof a)g=
5;else a:switch(a){case Ma:return Ha(c.children,e,f,b);case Hj:g=8;e|=7;break;case Af:g=8;e|=1;break;case kc:return a=la(12,c,b,e|8),a.elementType=kc,a.type=kc,a.expirationTime=f,a;case lc:return a=la(13,c,b,e),a.type=lc,a.elementType=lc,a.expirationTime=f,a;case yd:return a=la(19,c,b,e),a.elementType=yd,a.expirationTime=f,a;default:if("object"===typeof a&&null!==a)switch(a.$$typeof){case Cf:g=10;break a;case Bf:g=9;break a;case zd:g=11;break a;case Ad:g=14;break a;case Ef:g=16;d=null;break a;case Df:g=
22;break a}throw Error(k(130,null==a?a:typeof a,""));}b=la(g,c,b,e);b.elementType=a;b.type=d;b.expirationTime=f;return b}function Ha(a,b,c,d){a=la(7,a,d,b);a.expirationTime=c;return a}function qe(a,b,c){a=la(6,a,null,b);a.expirationTime=c;return a}function re(a,b,c){b=la(4,null!==a.children?a.children:[],a.key,b);b.expirationTime=c;b.stateNode={containerInfo:a.containerInfo,pendingChildren:null,implementation:a.implementation};return b}function Ij(a,b,c){this.tag=b;this.current=null;this.containerInfo=
a;this.pingCache=this.pendingChildren=null;this.finishedExpirationTime=0;this.finishedWork=null;this.timeoutHandle=-1;this.pendingContext=this.context=null;this.hydrate=c;this.callbackNode=null;this.callbackPriority=90;this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Kh(a,b){var c=a.firstSuspendedTime;a=a.lastSuspendedTime;return 0!==c&&c>=b&&a<=b}function Ya(a,b){var c=a.firstSuspendedTime,d=a.lastSuspendedTime;
c<b&&(a.firstSuspendedTime=b);if(d>b||0===c)a.lastSuspendedTime=b;b<=a.lastPingedTime&&(a.lastPingedTime=0);b<=a.lastExpiredTime&&(a.lastExpiredTime=0)}function yh(a,b){b>a.firstPendingTime&&(a.firstPendingTime=b);var c=a.firstSuspendedTime;0!==c&&(b>=c?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:b>=a.lastSuspendedTime&&(a.lastSuspendedTime=b+1),b>a.nextKnownPendingLevel&&(a.nextKnownPendingLevel=b))}function Ue(a,b){var c=a.lastExpiredTime;if(0===c||c>b)a.lastExpiredTime=b}
function md(a,b,c,d){var e=b.current,f=ka(),g=Vb.suspense;f=Va(f,e,g);a:if(c){c=c._reactInternalFiber;b:{if(Na(c)!==c||1!==c.tag)throw Error(k(170));var h=c;do{switch(h.tag){case 3:h=h.stateNode.context;break b;case 1:if(N(h.type)){h=h.stateNode.__reactInternalMemoizedMergedChildContext;break b}}h=h.return}while(null!==h);throw Error(k(171));}if(1===c.tag){var m=c.type;if(N(m)){c=Gg(c,m,h);break a}}c=h}else c=Ca;null===b.context?b.context=c:b.pendingContext=c;b=Ea(f,g);b.payload={element:a};d=void 0===
d?null:d;null!==d&&(b.callback=d);Fa(e,b);Ja(e,f);return f}function cf(a){a=a.current;if(!a.child)return null;switch(a.child.tag){case 5:return a.child.stateNode;default:return a.child.stateNode}}function Wh(a,b){a=a.memoizedState;null!==a&&null!==a.dehydrated&&a.retryTime<b&&(a.retryTime=b)}function df(a,b){Wh(a,b);(a=a.alternate)&&Wh(a,b)}function ef(a,b,c){c=null!=c&&!0===c.hydrate;var d=new Ij(a,b,c),e=la(3,null,null,2===b?7:1===b?3:0);d.current=e;e.stateNode=d;ne(e);a[Lb]=d.current;c&&0!==b&&
xi(a,9===a.nodeType?a:a.ownerDocument);this._internalRoot=d}function bc(a){return!(!a||1!==a.nodeType&&9!==a.nodeType&&11!==a.nodeType&&(8!==a.nodeType||" react-mount-point-unstable "!==a.nodeValue))}function Jj(a,b){b||(b=a?9===a.nodeType?a.documentElement:a.firstChild:null,b=!(!b||1!==b.nodeType||!b.hasAttribute("data-reactroot")));if(!b)for(var c;c=a.lastChild;)a.removeChild(c);return new ef(a,0,b?{hydrate:!0}:void 0)}function nd(a,b,c,d,e){var f=c._reactRootContainer;if(f){var g=f._internalRoot;
if("function"===typeof e){var h=e;e=function(){var a=cf(g);h.call(a)}}md(b,g,a,e)}else{f=c._reactRootContainer=Jj(c,d);g=f._internalRoot;if("function"===typeof e){var m=e;e=function(){var a=cf(g);m.call(a)}}Rh(function(){md(b,g,a,e)})}return cf(g)}function Kj(a,b,c){var d=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:gb,key:null==d?null:""+d,children:a,containerInfo:b,implementation:c}}function Xh(a,b){var c=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;
if(!bc(b))throw Error(k(200));return Kj(a,b,null,c)}if(!ea)throw Error(k(227));var ki=function(a,b,c,d,e,f,g,h,m){var n=Array.prototype.slice.call(arguments,3);try{b.apply(c,n)}catch(C){this.onError(C)}},yb=!1,gc=null,hc=!1,pd=null,li={onError:function(a){yb=!0;gc=a}},td=null,rf=null,mf=null,ic=null,cb={},jc=[],qd={},db={},rd={},wa=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.assign,
sd=null,eb=null,fb=null,ee=function(a,b){return a(b)},eg=function(a,b,c,d,e){return a(b,c,d,e)},vd=function(){},vf=ee,Oa=!1,wd=!1,Z=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.Scheduler,Lj=Z.unstable_cancelCallback,ff=Z.unstable_now,$f=Z.unstable_scheduleCallback,Mj=Z.unstable_shouldYield,Yh=Z.unstable_requestPaint,Pd=Z.unstable_runWithPriority,Nj=Z.unstable_getCurrentPriorityLevel,Oj=Z.unstable_ImmediatePriority,Zh=Z.unstable_UserBlockingPriority,ag=Z.unstable_NormalPriority,Pj=Z.unstable_LowPriority,
Qj=Z.unstable_IdlePriority,oi=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,wf=Object.prototype.hasOwnProperty,yf={},xf={},E={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(a){E[a]=
new L(a,0,!1,a,null,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(a){var b=a[0];E[b]=new L(b,1,!1,a[1],null,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(a){E[a]=new L(a,2,!1,a.toLowerCase(),null,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(a){E[a]=new L(a,2,!1,a,null,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(a){E[a]=
new L(a,3,!1,a.toLowerCase(),null,!1)});["checked","multiple","muted","selected"].forEach(function(a){E[a]=new L(a,3,!0,a,null,!1)});["capture","download"].forEach(function(a){E[a]=new L(a,4,!1,a,null,!1)});["cols","rows","size","span"].forEach(function(a){E[a]=new L(a,6,!1,a,null,!1)});["rowSpan","start"].forEach(function(a){E[a]=new L(a,5,!1,a.toLowerCase(),null,!1)});var gf=/[\-:]([a-z])/g,hf=function(a){return a[1].toUpperCase()};"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(a){var b=
a.replace(gf,hf);E[b]=new L(b,1,!1,a,null,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/1999/xlink",!1)});["xml:base","xml:lang","xml:space"].forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/XML/1998/namespace",!1)});["tabIndex","crossOrigin"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!1)});E.xlinkHref=new L("xlinkHref",1,
!1,"xlink:href","http://www.w3.org/1999/xlink",!0);["src","href","action","formAction"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!0)});var da=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;da.hasOwnProperty("ReactCurrentDispatcher")||(da.ReactCurrentDispatcher={current:null});da.hasOwnProperty("ReactCurrentBatchConfig")||(da.ReactCurrentBatchConfig={suspense:null});var si=/^(.*)[\\\/]/,Q="function"===typeof Symbol&&Symbol.for,Pc=Q?Symbol.for("react.element"):60103,gb=Q?Symbol.for("react.portal"):
60106,Ma=Q?Symbol.for("react.fragment"):60107,Af=Q?Symbol.for("react.strict_mode"):60108,kc=Q?Symbol.for("react.profiler"):60114,Cf=Q?Symbol.for("react.provider"):60109,Bf=Q?Symbol.for("react.context"):60110,Hj=Q?Symbol.for("react.concurrent_mode"):60111,zd=Q?Symbol.for("react.forward_ref"):60112,lc=Q?Symbol.for("react.suspense"):60113,yd=Q?Symbol.for("react.suspense_list"):60120,Ad=Q?Symbol.for("react.memo"):60115,Ef=Q?Symbol.for("react.lazy"):60116,Df=Q?Symbol.for("react.block"):60121,zf="function"===
typeof Symbol&&Symbol.iterator,od,xh=function(a){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(b,c,d,e){MSApp.execUnsafeLocalFunction(function(){return a(b,c,d,e)})}:a}(function(a,b){if("http://www.w3.org/2000/svg"!==a.namespaceURI||"innerHTML"in a)a.innerHTML=b;else{od=od||document.createElement("div");od.innerHTML="<svg>"+b.valueOf().toString()+"</svg>";for(b=od.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;b.firstChild;)a.appendChild(b.firstChild)}}),Wb=function(a,
b){if(b){var c=a.firstChild;if(c&&c===a.lastChild&&3===c.nodeType){c.nodeValue=b;return}}a.textContent=b},ib={animationend:nc("Animation","AnimationEnd"),animationiteration:nc("Animation","AnimationIteration"),animationstart:nc("Animation","AnimationStart"),transitionend:nc("Transition","TransitionEnd")},Id={},Of={};wa&&(Of=document.createElement("div").style,"AnimationEvent"in window||(delete ib.animationend.animation,delete ib.animationiteration.animation,delete ib.animationstart.animation),"TransitionEvent"in
window||delete ib.transitionend.transition);var $h=oc("animationend"),ai=oc("animationiteration"),bi=oc("animationstart"),ci=oc("transitionend"),Db="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Pf=new ("function"===typeof WeakMap?WeakMap:Map),Ab=null,wi=function(a){if(a){var b=a._dispatchListeners,c=a._dispatchInstances;
if(Array.isArray(b))for(var d=0;d<b.length&&!a.isPropagationStopped();d++)lf(a,b[d],c[d]);else b&&lf(a,b,c);a._dispatchListeners=null;a._dispatchInstances=null;a.isPersistent()||a.constructor.release(a)}},qc=[],Rd=!1,fa=[],xa=null,ya=null,za=null,Eb=new Map,Fb=new Map,Jb=[],Nd="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),
yi="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" "),dg={},cg=new Map,Td=new Map,Rj=["abort","abort",$h,"animationEnd",ai,"animationIteration",bi,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata",
"loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",ci,"transitionEnd","waiting","waiting"];Sd("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),
0);Sd("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1);Sd(Rj,2);(function(a,b){for(var c=0;c<a.length;c++)Td.set(a[c],b)})("change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),0);var Hi=Zh,Gi=Pd,tc=!0,Kb={animationIterationCount:!0,
borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,
strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},Sj=["Webkit","ms","Moz","O"];Object.keys(Kb).forEach(function(a){Sj.forEach(function(b){b=b+a.charAt(0).toUpperCase()+a.substring(1);Kb[b]=Kb[a]})});var Ii=M({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0}),ng="$",og="/$",$d="$?",Zd="$!",Ze=null,$e=null,We="function"===typeof setTimeout?setTimeout:void 0,vj="function"===
typeof clearTimeout?clearTimeout:void 0,jf=Math.random().toString(36).slice(2),Aa="__reactInternalInstance$"+jf,vc="__reactEventHandlers$"+jf,Lb="__reactContainere$"+jf,Ba=null,ce=null,wc=null;M(R.prototype,{preventDefault:function(){this.defaultPrevented=!0;var a=this.nativeEvent;a&&(a.preventDefault?a.preventDefault():"unknown"!==typeof a.returnValue&&(a.returnValue=!1),this.isDefaultPrevented=xc)},stopPropagation:function(){var a=this.nativeEvent;a&&(a.stopPropagation?a.stopPropagation():"unknown"!==
typeof a.cancelBubble&&(a.cancelBubble=!0),this.isPropagationStopped=xc)},persist:function(){this.isPersistent=xc},isPersistent:yc,destructor:function(){var a=this.constructor.Interface,b;for(b in a)this[b]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null;this.isPropagationStopped=this.isDefaultPrevented=yc;this._dispatchInstances=this._dispatchListeners=null}});R.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(a){return a.timeStamp||
Date.now()},defaultPrevented:null,isTrusted:null};R.extend=function(a){function b(){return c.apply(this,arguments)}var c=this,d=function(){};d.prototype=c.prototype;d=new d;M(d,b.prototype);b.prototype=d;b.prototype.constructor=b;b.Interface=M({},c.Interface,a);b.extend=c.extend;sg(b);return b};sg(R);var Tj=R.extend({data:null}),Uj=R.extend({data:null}),Ni=[9,13,27,32],de=wa&&"CompositionEvent"in window,cc=null;wa&&"documentMode"in document&&(cc=document.documentMode);var Vj=wa&&"TextEvent"in window&&
!cc,xg=wa&&(!de||cc&&8<cc&&11>=cc),wg=String.fromCharCode(32),ua={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},
dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},vg=!1,mb=!1,Wj={eventTypes:ua,extractEvents:function(a,b,c,d,e){var f;if(de)b:{switch(a){case "compositionstart":var g=ua.compositionStart;break b;case "compositionend":g=ua.compositionEnd;break b;case "compositionupdate":g=
ua.compositionUpdate;break b}g=void 0}else mb?tg(a,c)&&(g=ua.compositionEnd):"keydown"===a&&229===c.keyCode&&(g=ua.compositionStart);g?(xg&&"ko"!==c.locale&&(mb||g!==ua.compositionStart?g===ua.compositionEnd&&mb&&(f=rg()):(Ba=d,ce="value"in Ba?Ba.value:Ba.textContent,mb=!0)),e=Tj.getPooled(g,b,c,d),f?e.data=f:(f=ug(c),null!==f&&(e.data=f)),lb(e),f=e):f=null;(a=Vj?Oi(a,c):Pi(a,c))?(b=Uj.getPooled(ua.beforeInput,b,c,d),b.data=a,lb(b)):b=null;return null===f?b:null===b?f:[f,b]}},Qi={color:!0,date:!0,
datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0},Ag={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}},Mb=null,Nb=null,kf=!1;wa&&(kf=Tf("input")&&(!document.documentMode||9<document.documentMode));var Xj={eventTypes:Ag,_isInputEventSupported:kf,extractEvents:function(a,b,c,d,e){e=b?Pa(b):window;var f=
e.nodeName&&e.nodeName.toLowerCase();if("select"===f||"input"===f&&"file"===e.type)var g=Si;else if(yg(e))if(kf)g=Wi;else{g=Ui;var h=Ti}else(f=e.nodeName)&&"input"===f.toLowerCase()&&("checkbox"===e.type||"radio"===e.type)&&(g=Vi);if(g&&(g=g(a,b)))return zg(g,c,d);h&&h(a,e,b);"blur"===a&&(a=e._wrapperState)&&a.controlled&&"number"===e.type&&Ed(e,"number",e.value)}},dc=R.extend({view:null,detail:null}),Yi={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"},di=0,ei=0,fi=!1,gi=!1,ec=dc.extend({screenX:null,
screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:fe,button:null,buttons:null,relatedTarget:function(a){return a.relatedTarget||(a.fromElement===a.srcElement?a.toElement:a.fromElement)},movementX:function(a){if("movementX"in a)return a.movementX;var b=di;di=a.screenX;return fi?"mousemove"===a.type?a.screenX-b:0:(fi=!0,0)},movementY:function(a){if("movementY"in a)return a.movementY;var b=ei;ei=a.screenY;return gi?"mousemove"===
a.type?a.screenY-b:0:(gi=!0,0)}}),hi=ec.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),fc={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout",
"pointerover"]}},Yj={eventTypes:fc,extractEvents:function(a,b,c,d,e){var f="mouseover"===a||"pointerover"===a,g="mouseout"===a||"pointerout"===a;if(f&&0===(e&32)&&(c.relatedTarget||c.fromElement)||!g&&!f)return null;f=d.window===d?d:(f=d.ownerDocument)?f.defaultView||f.parentWindow:window;if(g){if(g=b,b=(b=c.relatedTarget||c.toElement)?Bb(b):null,null!==b){var h=Na(b);if(b!==h||5!==b.tag&&6!==b.tag)b=null}}else g=null;if(g===b)return null;if("mouseout"===a||"mouseover"===a){var m=ec;var n=fc.mouseLeave;
var l=fc.mouseEnter;var k="mouse"}else if("pointerout"===a||"pointerover"===a)m=hi,n=fc.pointerLeave,l=fc.pointerEnter,k="pointer";a=null==g?f:Pa(g);f=null==b?f:Pa(b);n=m.getPooled(n,g,c,d);n.type=k+"leave";n.target=a;n.relatedTarget=f;c=m.getPooled(l,b,c,d);c.type=k+"enter";c.target=f;c.relatedTarget=a;d=g;k=b;if(d&&k)a:{m=d;l=k;g=0;for(a=m;a;a=pa(a))g++;a=0;for(b=l;b;b=pa(b))a++;for(;0<g-a;)m=pa(m),g--;for(;0<a-g;)l=pa(l),a--;for(;g--;){if(m===l||m===l.alternate)break a;m=pa(m);l=pa(l)}m=null}else m=
null;l=m;for(m=[];d&&d!==l;){g=d.alternate;if(null!==g&&g===l)break;m.push(d);d=pa(d)}for(d=[];k&&k!==l;){g=k.alternate;if(null!==g&&g===l)break;d.push(k);k=pa(k)}for(k=0;k<m.length;k++)be(m[k],"bubbled",n);for(k=d.length;0<k--;)be(d[k],"captured",c);return 0===(e&64)?[n]:[n,c]}},Qa="function"===typeof Object.is?Object.is:Zi,$i=Object.prototype.hasOwnProperty,Zj=wa&&"documentMode"in document&&11>=document.documentMode,Eg={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},
dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},nb=null,he=null,Pb=null,ge=!1,ak={eventTypes:Eg,extractEvents:function(a,b,c,d,e,f){e=f||(d.window===d?d.document:9===d.nodeType?d:d.ownerDocument);if(!(f=!e)){a:{e=Jd(e);f=rd.onSelect;for(var g=0;g<f.length;g++)if(!e.has(f[g])){e=!1;break a}e=!0}f=!e}if(f)return null;e=b?Pa(b):window;switch(a){case "focus":if(yg(e)||"true"===e.contentEditable)nb=e,he=b,Pb=null;break;case "blur":Pb=he=nb=null;
break;case "mousedown":ge=!0;break;case "contextmenu":case "mouseup":case "dragend":return ge=!1,Dg(c,d);case "selectionchange":if(Zj)break;case "keydown":case "keyup":return Dg(c,d)}return null}},bk=R.extend({animationName:null,elapsedTime:null,pseudoElement:null}),ck=R.extend({clipboardData:function(a){return"clipboardData"in a?a.clipboardData:window.clipboardData}}),dk=dc.extend({relatedTarget:null}),ek={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",
Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},fk={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",
224:"Meta"},gk=dc.extend({key:function(a){if(a.key){var b=ek[a.key]||a.key;if("Unidentified"!==b)return b}return"keypress"===a.type?(a=Ac(a),13===a?"Enter":String.fromCharCode(a)):"keydown"===a.type||"keyup"===a.type?fk[a.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:fe,charCode:function(a){return"keypress"===a.type?Ac(a):0},keyCode:function(a){return"keydown"===a.type||"keyup"===a.type?a.keyCode:0},which:function(a){return"keypress"===
a.type?Ac(a):"keydown"===a.type||"keyup"===a.type?a.keyCode:0}}),hk=ec.extend({dataTransfer:null}),ik=dc.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:fe}),jk=R.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),kk=ec.extend({deltaX:function(a){return"deltaX"in a?a.deltaX:"wheelDeltaX"in a?-a.wheelDeltaX:0},deltaY:function(a){return"deltaY"in a?a.deltaY:"wheelDeltaY"in a?-a.wheelDeltaY:"wheelDelta"in a?
-a.wheelDelta:0},deltaZ:null,deltaMode:null}),lk={eventTypes:dg,extractEvents:function(a,b,c,d,e){e=cg.get(a);if(!e)return null;switch(a){case "keypress":if(0===Ac(c))return null;case "keydown":case "keyup":a=gk;break;case "blur":case "focus":a=dk;break;case "click":if(2===c.button)return null;case "auxclick":case "dblclick":case "mousedown":case "mousemove":case "mouseup":case "mouseout":case "mouseover":case "contextmenu":a=ec;break;case "drag":case "dragend":case "dragenter":case "dragexit":case "dragleave":case "dragover":case "dragstart":case "drop":a=
hk;break;case "touchcancel":case "touchend":case "touchmove":case "touchstart":a=ik;break;case $h:case ai:case bi:a=bk;break;case ci:a=jk;break;case "scroll":a=dc;break;case "wheel":a=kk;break;case "copy":case "cut":case "paste":a=ck;break;case "gotpointercapture":case "lostpointercapture":case "pointercancel":case "pointerdown":case "pointermove":case "pointerout":case "pointerover":case "pointerup":a=hi;break;default:a=R}b=a.getPooled(e,b,c,d);lb(b);return b}};(function(a){if(ic)throw Error(k(101));
ic=Array.prototype.slice.call(a);nf()})("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" "));(function(a,b,c){td=a;rf=b;mf=c})(ae,Hb,Pa);pf({SimpleEventPlugin:lk,EnterLeaveEventPlugin:Yj,ChangeEventPlugin:Xj,SelectEventPlugin:ak,BeforeInputEventPlugin:Wj});var ie=[],ob=-1,Ca={},B={current:Ca},G={current:!1},Ra=Ca,bj=Pd,je=$f,Rg=Lj,aj=Nj,Dc=Oj,Ig=Zh,Jg=ag,Kg=Pj,Lg=Qj,Qg={},yj=Mj,Cj=void 0!==Yh?Yh:function(){},qa=null,
Ec=null,ke=!1,ii=ff(),Y=1E4>ii?ff:function(){return ff()-ii},Ic={current:null},Hc=null,qb=null,Gc=null,Tg=0,Jc=2,Ga=!1,Vb=da.ReactCurrentBatchConfig,$g=(new ea.Component).refs,Mc={isMounted:function(a){return(a=a._reactInternalFiber)?Na(a)===a:!1},enqueueSetState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;d=Va(d,a,e);e=Ea(d,e);e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueReplaceState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;
d=Va(d,a,e);e=Ea(d,e);e.tag=1;e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueForceUpdate:function(a,b){a=a._reactInternalFiber;var c=ka(),d=Vb.suspense;c=Va(c,a,d);d=Ea(c,d);d.tag=Jc;void 0!==b&&null!==b&&(d.callback=b);Fa(a,d);Ja(a,c)}},Qc=Array.isArray,wb=ah(!0),Fe=ah(!1),Sb={},ja={current:Sb},Ub={current:Sb},Tb={current:Sb},D={current:0},Sc=da.ReactCurrentDispatcher,X=da.ReactCurrentBatchConfig,Ia=0,z=null,K=null,J=null,Uc=!1,Tc={readContext:W,useCallback:S,useContext:S,
useEffect:S,useImperativeHandle:S,useLayoutEffect:S,useMemo:S,useReducer:S,useRef:S,useState:S,useDebugValue:S,useResponder:S,useDeferredValue:S,useTransition:S},dj={readContext:W,useCallback:ih,useContext:W,useEffect:eh,useImperativeHandle:function(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;return ze(4,2,gh.bind(null,b,a),c)},useLayoutEffect:function(a,b){return ze(4,2,a,b)},useMemo:function(a,b){var c=ub();b=void 0===b?null:b;a=a();c.memoizedState=[a,b];return a},useReducer:function(a,b,c){var d=
ub();b=void 0!==c?c(b):b;d.memoizedState=d.baseState=b;a=d.queue={pending:null,dispatch:null,lastRenderedReducer:a,lastRenderedState:b};a=a.dispatch=ch.bind(null,z,a);return[d.memoizedState,a]},useRef:function(a){var b=ub();a={current:a};return b.memoizedState=a},useState:xe,useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=xe(a),d=c[0],e=c[1];eh(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=
xe(!1),c=b[0];b=b[1];return[ih(Ce.bind(null,b,a),[b,a]),c]}},ej={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Vc,useRef:dh,useState:function(a){return Vc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Vc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Vc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,
b,a),[b,a]),c]}},fj={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Wc,useRef:dh,useState:function(a){return Wc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Wc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Wc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,b,a),[b,a]),c]}},ra=null,Ka=null,Wa=
!1,gj=da.ReactCurrentOwner,ia=!1,Je={dehydrated:null,retryTime:0};var jj=function(a,b,c,d){for(c=b.child;null!==c;){if(5===c.tag||6===c.tag)a.appendChild(c.stateNode);else if(4!==c.tag&&null!==c.child){c.child.return=c;c=c.child;continue}if(c===b)break;for(;null===c.sibling;){if(null===c.return||c.return===b)return;c=c.return}c.sibling.return=c.return;c=c.sibling}};var wh=function(a){};var ij=function(a,b,c,d,e){var f=a.memoizedProps;if(f!==d){var g=b.stateNode;Ta(ja.current);a=null;switch(c){case "input":f=
Cd(g,f);d=Cd(g,d);a=[];break;case "option":f=Fd(g,f);d=Fd(g,d);a=[];break;case "select":f=M({},f,{value:void 0});d=M({},d,{value:void 0});a=[];break;case "textarea":f=Gd(g,f);d=Gd(g,d);a=[];break;default:"function"!==typeof f.onClick&&"function"===typeof d.onClick&&(g.onclick=uc)}Ud(c,d);var h,m;c=null;for(h in f)if(!d.hasOwnProperty(h)&&f.hasOwnProperty(h)&&null!=f[h])if("style"===h)for(m in g=f[h],g)g.hasOwnProperty(m)&&(c||(c={}),c[m]="");else"dangerouslySetInnerHTML"!==h&&"children"!==h&&"suppressContentEditableWarning"!==
h&&"suppressHydrationWarning"!==h&&"autoFocus"!==h&&(db.hasOwnProperty(h)?a||(a=[]):(a=a||[]).push(h,null));for(h in d){var k=d[h];g=null!=f?f[h]:void 0;if(d.hasOwnProperty(h)&&k!==g&&(null!=k||null!=g))if("style"===h)if(g){for(m in g)!g.hasOwnProperty(m)||k&&k.hasOwnProperty(m)||(c||(c={}),c[m]="");for(m in k)k.hasOwnProperty(m)&&g[m]!==k[m]&&(c||(c={}),c[m]=k[m])}else c||(a||(a=[]),a.push(h,c)),c=k;else"dangerouslySetInnerHTML"===h?(k=k?k.__html:void 0,g=g?g.__html:void 0,null!=k&&g!==k&&(a=a||
[]).push(h,k)):"children"===h?g===k||"string"!==typeof k&&"number"!==typeof k||(a=a||[]).push(h,""+k):"suppressContentEditableWarning"!==h&&"suppressHydrationWarning"!==h&&(db.hasOwnProperty(h)?(null!=k&&oa(e,h),a||g===k||(a=[])):(a=a||[]).push(h,k))}c&&(a=a||[]).push("style",c);e=a;if(b.updateQueue=e)b.effectTag|=4}};var kj=function(a,b,c,d){c!==d&&(b.effectTag|=4)};var pj="function"===typeof WeakSet?WeakSet:Set,wj="function"===typeof WeakMap?WeakMap:Map,sj=Math.ceil,gd=da.ReactCurrentDispatcher,
Uh=da.ReactCurrentOwner,H=0,Ye=8,ca=16,ma=32,Xa=0,hd=1,Oh=2,ad=3,bd=4,Xe=5,p=H,U=null,t=null,P=0,F=Xa,id=null,ta=1073741823,Yb=1073741823,kd=null,Xb=0,jd=!1,Re=0,Ph=500,l=null,cd=!1,Se=null,La=null,ld=!1,Zb=null,$b=90,bb=null,ac=0,af=null,dd=0,Ja=function(a,b){if(50<ac)throw ac=0,af=null,Error(k(185));a=ed(a,b);if(null!==a){var c=Cc();1073741823===b?(p&Ye)!==H&&(p&(ca|ma))===H?Te(a):(V(a),p===H&&ha()):V(a);(p&4)===H||98!==c&&99!==c||(null===bb?bb=new Map([[a,b]]):(c=bb.get(a),(void 0===c||c>b)&&bb.set(a,
b)))}};var zj=function(a,b,c){var d=b.expirationTime;if(null!==a){var e=b.pendingProps;if(a.memoizedProps!==e||G.current)ia=!0;else{if(d<c){ia=!1;switch(b.tag){case 3:sh(b);Ee();break;case 5:bh(b);if(b.mode&4&&1!==c&&e.hidden)return b.expirationTime=b.childExpirationTime=1,null;break;case 1:N(b.type)&&Bc(b);break;case 4:se(b,b.stateNode.containerInfo);break;case 10:d=b.memoizedProps.value;e=b.type._context;y(Ic,e._currentValue);e._currentValue=d;break;case 13:if(null!==b.memoizedState){d=b.child.childExpirationTime;
if(0!==d&&d>=c)return th(a,b,c);y(D,D.current&1);b=sa(a,b,c);return null!==b?b.sibling:null}y(D,D.current&1);break;case 19:d=b.childExpirationTime>=c;if(0!==(a.effectTag&64)){if(d)return vh(a,b,c);b.effectTag|=64}e=b.memoizedState;null!==e&&(e.rendering=null,e.tail=null);y(D,D.current);if(!d)return null}return sa(a,b,c)}ia=!1}}else ia=!1;b.expirationTime=0;switch(b.tag){case 2:d=b.type;null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;e=pb(b,B.current);rb(b,c);e=we(null,
b,d,a,e,c);b.effectTag|=1;if("object"===typeof e&&null!==e&&"function"===typeof e.render&&void 0===e.$$typeof){b.tag=1;b.memoizedState=null;b.updateQueue=null;if(N(d)){var f=!0;Bc(b)}else f=!1;b.memoizedState=null!==e.state&&void 0!==e.state?e.state:null;ne(b);var g=d.getDerivedStateFromProps;"function"===typeof g&&Lc(b,d,g,a);e.updater=Mc;b.stateNode=e;e._reactInternalFiber=b;pe(b,d,a,c);b=Ie(null,b,d,!0,f,c)}else b.tag=0,T(null,b,e,c),b=b.child;return b;case 16:a:{e=b.elementType;null!==a&&(a.alternate=
null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;ri(e);if(1!==e._status)throw e._result;e=e._result;b.type=e;f=b.tag=Gj(e);a=aa(e,a);switch(f){case 0:b=He(null,b,e,a,c);break a;case 1:b=rh(null,b,e,a,c);break a;case 11:b=nh(null,b,e,a,c);break a;case 14:b=oh(null,b,e,aa(e.type,a),d,c);break a}throw Error(k(306,e,""));}return b;case 0:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),He(a,b,d,e,c);case 1:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),rh(a,b,d,e,c);
case 3:sh(b);d=b.updateQueue;if(null===a||null===d)throw Error(k(282));d=b.pendingProps;e=b.memoizedState;e=null!==e?e.element:null;oe(a,b);Qb(b,d,null,c);d=b.memoizedState.element;if(d===e)Ee(),b=sa(a,b,c);else{if(e=b.stateNode.hydrate)Ka=kb(b.stateNode.containerInfo.firstChild),ra=b,e=Wa=!0;if(e)for(c=Fe(b,null,d,c),b.child=c;c;)c.effectTag=c.effectTag&-3|1024,c=c.sibling;else T(a,b,d,c),Ee();b=b.child}return b;case 5:return bh(b),null===a&&De(b),d=b.type,e=b.pendingProps,f=null!==a?a.memoizedProps:
null,g=e.children,Yd(d,e)?g=null:null!==f&&Yd(d,f)&&(b.effectTag|=16),qh(a,b),b.mode&4&&1!==c&&e.hidden?(b.expirationTime=b.childExpirationTime=1,b=null):(T(a,b,g,c),b=b.child),b;case 6:return null===a&&De(b),null;case 13:return th(a,b,c);case 4:return se(b,b.stateNode.containerInfo),d=b.pendingProps,null===a?b.child=wb(b,null,d,c):T(a,b,d,c),b.child;case 11:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),nh(a,b,d,e,c);case 7:return T(a,b,b.pendingProps,c),b.child;case 8:return T(a,
b,b.pendingProps.children,c),b.child;case 12:return T(a,b,b.pendingProps.children,c),b.child;case 10:a:{d=b.type._context;e=b.pendingProps;g=b.memoizedProps;f=e.value;var h=b.type._context;y(Ic,h._currentValue);h._currentValue=f;if(null!==g)if(h=g.value,f=Qa(h,f)?0:("function"===typeof d._calculateChangedBits?d._calculateChangedBits(h,f):1073741823)|0,0===f){if(g.children===e.children&&!G.current){b=sa(a,b,c);break a}}else for(h=b.child,null!==h&&(h.return=b);null!==h;){var m=h.dependencies;if(null!==
m){g=h.child;for(var l=m.firstContext;null!==l;){if(l.context===d&&0!==(l.observedBits&f)){1===h.tag&&(l=Ea(c,null),l.tag=Jc,Fa(h,l));h.expirationTime<c&&(h.expirationTime=c);l=h.alternate;null!==l&&l.expirationTime<c&&(l.expirationTime=c);Sg(h.return,c);m.expirationTime<c&&(m.expirationTime=c);break}l=l.next}}else g=10===h.tag?h.type===b.type?null:h.child:h.child;if(null!==g)g.return=h;else for(g=h;null!==g;){if(g===b){g=null;break}h=g.sibling;if(null!==h){h.return=g.return;g=h;break}g=g.return}h=
g}T(a,b,e.children,c);b=b.child}return b;case 9:return e=b.type,f=b.pendingProps,d=f.children,rb(b,c),e=W(e,f.unstable_observedBits),d=d(e),b.effectTag|=1,T(a,b,d,c),b.child;case 14:return e=b.type,f=aa(e,b.pendingProps),f=aa(e.type,f),oh(a,b,e,f,d,c);case 15:return ph(a,b,b.type,b.pendingProps,d,c);case 17:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),b.tag=1,N(d)?(a=!0,Bc(b)):a=!1,rb(b,c),Yg(b,d,e),pe(b,d,e,c),Ie(null,
b,d,!0,a,c);case 19:return vh(a,b,c)}throw Error(k(156,b.tag));};var bf=null,Ne=null,la=function(a,b,c,d){return new Fj(a,b,c,d)};ef.prototype.render=function(a){md(a,this._internalRoot,null,null)};ef.prototype.unmount=function(){var a=this._internalRoot,b=a.containerInfo;md(null,a,null,function(){b[Lb]=null})};var Di=function(a){if(13===a.tag){var b=Fc(ka(),150,100);Ja(a,b);df(a,b)}};var Yf=function(a){13===a.tag&&(Ja(a,3),df(a,3))};var Bi=function(a){if(13===a.tag){var b=ka();b=Va(b,a,null);Ja(a,
b);df(a,b)}};sd=function(a,b,c){switch(b){case "input":Dd(a,c);b=c.name;if("radio"===c.type&&null!=b){for(c=a;c.parentNode;)c=c.parentNode;c=c.querySelectorAll("input[name="+JSON.stringify(""+b)+'][type="radio"]');for(b=0;b<c.length;b++){var d=c[b];if(d!==a&&d.form===a.form){var e=ae(d);if(!e)throw Error(k(90));Gf(d);Dd(d,e)}}}break;case "textarea":Lf(a,c);break;case "select":b=c.value,null!=b&&hb(a,!!c.multiple,b,!1)}};(function(a,b,c,d){ee=a;eg=b;vd=c;vf=d})(Qh,function(a,b,c,d,e){var f=p;p|=4;
try{return Da(98,a.bind(null,b,c,d,e))}finally{p=f,p===H&&ha()}},function(){(p&(1|ca|ma))===H&&(uj(),xb())},function(a,b){var c=p;p|=2;try{return a(b)}finally{p=c,p===H&&ha()}});var mk={Events:[Hb,Pa,ae,pf,qd,lb,function(a){Kd(a,Ki)},sf,tf,sc,pc,xb,{current:!1}]};(function(a){var b=a.findFiberByHostInstance;return Ej(M({},a,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:da.ReactCurrentDispatcher,findHostInstanceByFiber:function(a){a=Sf(a);
return null===a?null:a.stateNode},findFiberByHostInstance:function(a){return b?b(a):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))})({findFiberByHostInstance:Bb,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"});I.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=mk;I.createPortal=Xh;I.findDOMNode=function(a){if(null==a)return null;if(1===a.nodeType)return a;var b=a._reactInternalFiber;if(void 0===
b){if("function"===typeof a.render)throw Error(k(188));throw Error(k(268,Object.keys(a)));}a=Sf(b);a=null===a?null:a.stateNode;return a};I.flushSync=function(a,b){if((p&(ca|ma))!==H)throw Error(k(187));var c=p;p|=1;try{return Da(99,a.bind(null,b))}finally{p=c,ha()}};I.hydrate=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!0,c)};I.render=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!1,c)};I.unmountComponentAtNode=function(a){if(!bc(a))throw Error(k(40));return a._reactRootContainer?
(Rh(function(){nd(null,null,a,!1,function(){a._reactRootContainer=null;a[Lb]=null})}),!0):!1};I.unstable_batchedUpdates=Qh;I.unstable_createPortal=function(a,b){return Xh(a,b,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)};I.unstable_renderSubtreeIntoContainer=function(a,b,c,d){if(!bc(c))throw Error(k(200));if(null==a||void 0===a._reactInternalFiber)throw Error(k(38));return nd(a,b,c,!1,d)};I.version="16.13.1"});
</script>
    <script>const e = React.createElement;

function pathToString(path) {
  if (path[0] === '/') {
    return '/' + path.slice(1).join('/');
  } else {
    return path.join('/');
  }
}

function findCommonPath(files) {
  if (!files || !files.length) {
    return [];
  }

  function isPrefix(arr, prefix) {
    if (arr.length < prefix.length) {
      return false;
    }
    for (let i = prefix.length - 1; i >= 0; --i) {
      if (arr[i] !== prefix[i]) {
        return false;
      }
    }
    return true;
  }

  let commonPath = files[0].path.slice(0, -1);
  while (commonPath.length) {
    if (files.every(file => isPrefix(file.path, commonPath))) {
      break;
    }
    commonPath.pop();
  }
  return commonPath;
}

function findFolders(files) {
  if (!files || !files.length) {
    return [];
  }

  let folders = files.filter(file => file.path.length > 1).map(file => file.path[0]);
  folders = [...new Set(folders)]; // unique
  folders.sort();

  folders = folders.map(folder => {
    let filesInFolder = files
      .filter(file => file.path[0] === folder)
      .map(file => ({
        ...file,
        path: file.path.slice(1),
        parent: [...file.parent, file.path[0]],
      }));

    const children = findFolders(filesInFolder); // recursion

    return {
      is_folder: true,
      path: [folder],
      parent: files[0].parent,
      children,
      covered: children.reduce((sum, file) => sum + file.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.coverable, 0),
      prevRun: {
        covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
        coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
      },
    };
  });

  return [...folders, ...files.filter(file => file.path.length === 1)];
}

class App extends React.Component {
  constructor(...args) {
    super(...args);

    this.state = {
      current: [],
    };
  }

  componentDidMount() {
    this.updateStateFromLocation();
    window.addEventListener('hashchange', () => this.updateStateFromLocation(), false);
  }

  updateStateFromLocation() {
    if (window.location.hash.length > 1) {
      const current = window.location.hash.slice(1).split('/');
      this.setState({current});
    } else {
      this.setState({current: []});
    }
  }

  getCurrentPath() {
    let file = this.props.root;
    let path = [file];
    for (let p of this.state.current) {
      file = file.children.find(file => file.path[0] === p);
      if (!file) {
        return path;
      }
      path.push(file);
    }
    return path;
  }

  render() {
    const path = this.getCurrentPath();
    const file = path[path.length - 1];

    let w = null;
    if (file.is_folder) {
      w = e(FilesList, {
        folder: file,
        onSelectFile: this.selectFile.bind(this),
        onBack: path.length > 1 ? this.back.bind(this) : null,
      });
    } else {
      w = e(DisplayFile, {
        file,
        onBack: this.back.bind(this),
      });
    }

    return e('div', {className: 'app'}, w);
  }

  selectFile(file) {
    this.setState(
      ({current}) => {
        return {current: [...current, file.path[0]]};
      },
      () => this.updateHash(),
    );
  }

  back(file) {
    this.setState(
      ({current}) => {
        return {current: current.slice(0, current.length - 1)};
      },
      () => this.updateHash(),
    );
  }

  updateHash() {
    if (!this.state.current || !this.state.current.length) {
      window.location = '#';
    } else {
      window.location = '#' + this.state.current.join('/');
    }
  }
}

function FilesList({folder, onSelectFile, onBack}) {
  let files = folder.children;
  return e(
    'div',
    {className: 'display-folder'},
    e(FileHeader, {file: folder, onBack}),
    e(
      'table',
      {className: 'files-list'},
      e('thead', {className: 'files-list__head'}, e('tr', null, e('th', null, 'Path'), e('th', null, 'Coverage'))),
      e(
        'tbody',
        {className: 'files-list__body'},
        files.map(file => e(File, {file, onClick: onSelectFile})),
      ),
    ),
  );
}

function File({file, onClick}) {
  const coverage = file.coverable ? (file.covered / file.coverable) * 100 : -1;
  const coverageDelta =
    file.prevRun && (file.covered / file.coverable) * 100 - (file.prevRun.covered / file.prevRun.coverable) * 100;

  return e(
    'tr',
    {
      className:
        'files-list__file' +
        (coverage >= 0 && coverage < 50 ? ' files-list__file_low' : '') +
        (coverage >= 50 && coverage < 80 ? ' files-list__file_medium' : '') +
        (coverage >= 80 ? ' files-list__file_high' : '') +
        (file.is_folder ? ' files-list__file_folder' : ''),
      onClick: () => onClick(file),
    },
    e('td', null, e('a', null, pathToString(file.path))),
    e(
      'td',
      null,
      file.covered + ' / ' + file.coverable + (coverage >= 0 ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e(
        'span',
        {title: 'Change from the previous run'},
        coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : '',
      ),
    ),
  );
}

function DisplayFile({file, onBack}) {
  return e('div', {className: 'display-file'}, e(FileHeader, {file, onBack}), e(FileContent, {file}));
}

function FileHeader({file, onBack}) {
  const coverage = (file.covered / file.coverable) * 100;
  const coverageDelta = file.prevRun && coverage - (file.prevRun.covered / file.prevRun.coverable) * 100;

  return e(
    'div',
    {className: 'file-header'},
    onBack ? e('a', {className: 'file-header__back', onClick: onBack}, 'Back') : null,
    e('div', {className: 'file-header__name'}, pathToString([...file.parent, ...file.path])),
    e(
      'div',
      {className: 'file-header__stat'},
      'Covered: ' + file.covered + ' of ' + file.coverable + (file.coverable ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e(
        'span',
        {title: 'Change from the previous run'},
        coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : '',
      ),
      e('input', {id: 'theme-toggle', type: 'checkbox', hidden: true}),
      e('label', {for: 'theme-toggle', id: 'theme-toggle-label'}, ''),
    ),
  );
}

function FileContent({file}) {
  return e(
    'pre',
    {className: 'file-content'},
    file.content.split(/\r?\n/).map((line, index) => {
      const trace = file.traces.find(trace => trace.line === index + 1);
      const covered = trace && trace.stats.Line;
      const uncovered = trace && !trace.stats.Line;
      return e(
        'code',
        {
          className: 'code-line' + (covered ? ' code-line_covered' : '') + (uncovered ? ' code-line_uncovered' : ''),
          title: trace ? JSON.stringify(trace.stats, null, 2) : null,
        },
        line,
      );
    }),
  );
}

(function () {
  const commonPath = findCommonPath(data.files);
  const prevFilesMap = new Map();

  previousData &&
    previousData.files.forEach(file => {
      const path = file.path.slice(commonPath.length).join('/');
      prevFilesMap.set(path, file);
    });

  const files = data.files.map(file => {
    const path = file.path.slice(commonPath.length);
    const {covered = 0, coverable = 0} = prevFilesMap.get(path.join('/')) || {};
    return {
      ...file,
      path,
      parent: commonPath,
      prevRun: {covered, coverable},
    };
  });

  const children = findFolders(files);

  const root = {
    is_folder: true,
    children,
    path: commonPath,
    parent: [],
    covered: children.reduce((sum, file) => sum + file.covered, 0),
    coverable: children.reduce((sum, file) => sum + file.coverable, 0),
    prevRun: {
      covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
    },
  };

  ReactDOM.render(e(App, {root, prevFilesMap}), document.getElementById('root'));

  const toggle = document.getElementById('theme-toggle');
  const label = document.getElementById('theme-toggle-label');
  label.textContent = '';

  toggle.addEventListener('change', () => {
    if (toggle.checked) {
      document.documentElement.setAttribute('data-theme', 'dark');
      label.textContent = '';
    } else {
      document.documentElement.removeAttribute('data-theme');
      label.textContent = '';
    }
  });
})();
</script>
</body>
</html>